üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.3.4. Batching et Bulk Operations

## Introduction

Lorsque vous devez manipuler des milliers ou des millions de lignes dans PostgreSQL, la fa√ßon dont vous proc√©dez fait **toute la diff√©rence**. Ins√©rer 100,000 lignes une par une peut prendre plusieurs heures, alors que les op√©rations en masse (bulk operations) peuvent accomplir la m√™me t√¢che en quelques secondes.

**Objectif de ce chapitre :** Ma√Ætriser les techniques de traitement en masse pour manipuler efficacement de grands volumes de donn√©es avec PostgreSQL.

---

## 1. Le Probl√®me : Op√©rations Une par Une

### 1.1. D√©finition du Probl√®me

**Op√©ration unitaire :** Ex√©cuter une requ√™te SQL pour chaque ligne individuellement.

**Exemple :** Importer 10,000 utilisateurs depuis un fichier CSV.

### 1.2. L'Approche Na√Øve (Probl√©matique)

```python
# Python avec Django ORM
import csv

with open('users.csv') as f:
    reader = csv.DictReader(f)
    for row in reader:
        User.objects.create(
            name=row['name'],
            email=row['email'],
            age=int(row['age'])
        )
# Pour 10,000 lignes : 10,000 INSERT !
```

**SQL g√©n√©r√© (10,000 fois) :**
```sql
INSERT INTO users (name, email, age) VALUES ('Alice', 'alice@example.com', 25);
INSERT INTO users (name, email, age) VALUES ('Bob', 'bob@example.com', 30);
INSERT INTO users (name, email, age) VALUES ('Charlie', 'charlie@example.com', 35);
-- ... 9,997 fois de plus !
```

### 1.3. Pourquoi C'est Probl√©matique ?

**Overhead par requ√™te :**
1. **Parsing SQL** : PostgreSQL doit analyser chaque requ√™te
2. **Planification** : Cr√©er un plan d'ex√©cution
3. **Commit** : √âcrire dans le WAL (Write-Ahead Log)
4. **Lock/Unlock** : Acquisition et lib√©ration de verrous
5. **Latence r√©seau** : Aller-retour application ‚Üî base

**Impact sur les performances :**
```
1 INSERT    = ~1ms (parsing + exec + commit)
10,000 fois = 10,000ms = 10 secondes minimum
```

**En r√©alit√©, souvent bien pire :**
- Locks cumul√©s
- Fragmentation du WAL
- Contention sur les index

**Temps r√©el observ√© : 30-60 secondes** pour 10,000 lignes ! üò±

### 1.4. Le Co√ªt Cach√© : Les Index

Chaque INSERT d√©clenche la mise √† jour de **tous les index** de la table.

```sql
-- Table avec 3 index
CREATE TABLE users (
    id SERIAL PRIMARY KEY,                    -- Index 1
    email VARCHAR(100) UNIQUE,                -- Index 2
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_users_created_at ON users(created_at);  -- Index 3
```

**1 INSERT = 3 mises √† jour d'index !**
**10,000 INSERT = 30,000 mises √† jour d'index !**

---

## 2. La Solution : Bulk Operations

### 2.1. D√©finition

Les **bulk operations** (op√©rations en masse) regroupent plusieurs op√©rations en une seule requ√™te ou en un lot optimis√©.

**Principe :** Au lieu de 10,000 requ√™tes ‚Üí 1 requ√™te (ou quelques requ√™tes).

### 2.2. Types d'Op√©rations en Masse

1. **Bulk INSERT** : Ins√©rer plusieurs lignes d'un coup
2. **Bulk UPDATE** : Mettre √† jour plusieurs lignes avec des valeurs diff√©rentes
3. **Bulk DELETE** : Supprimer plusieurs lignes selon des crit√®res
4. **Batching** : D√©couper en lots pour g√©rer de tr√®s gros volumes

### 2.3. Avantages

- ‚úÖ **Performance multipli√©e par 10 √† 100√ó**
- ‚úÖ **Moins de pression sur le WAL**
- ‚úÖ **Moins de locks/unlocks**
- ‚úÖ **Moins de fragmentation**
- ‚úÖ **Meilleure utilisation du cache**

---

## 3. Bulk INSERT : Ins√©rer en Masse

### 3.1. Approche 1 : INSERT avec VALUES Multiples

**SQL standard :**
```sql
-- Au lieu de 3 INSERT s√©par√©s
INSERT INTO users (name, email, age) VALUES ('Alice', 'alice@example.com', 25);
INSERT INTO users (name, email, age) VALUES ('Bob', 'bob@example.com', 30);
INSERT INTO users (name, email, age) VALUES ('Charlie', 'charlie@example.com', 35);

-- Faire 1 seul INSERT avec plusieurs VALUES
INSERT INTO users (name, email, age) VALUES
    ('Alice', 'alice@example.com', 25),
    ('Bob', 'bob@example.com', 30),
    ('Charlie', 'charlie@example.com', 35);
```

**Avantages :**
- 1 parsing, 1 plan, 1 commit
- Beaucoup plus rapide

**Performance :**
- 3 INSERT s√©par√©s : ~3ms
- 1 INSERT group√© : ~0.5ms
- **Gain : 6√ó**

### 3.2. Approche 2 : ORM Bulk Create

#### Python - Django
```python
# ‚ùå Lent : 1 INSERT par ligne
for row in data:
    User.objects.create(name=row['name'], email=row['email'])

# ‚úÖ Rapide : bulk_create
users = [
    User(name=row['name'], email=row['email'])
    for row in data
]
User.objects.bulk_create(users)
```

**SQL g√©n√©r√© par bulk_create :**
```sql
INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com'),
    ('Charlie', 'charlie@example.com'),
    -- ... toutes les lignes d'un coup
```

**Options importantes :**
```python
User.objects.bulk_create(
    users,
    batch_size=1000,        # D√©couper en lots de 1000
    ignore_conflicts=True   # Ignorer les doublons (PG 9.5+)
)
```

#### Python - SQLAlchemy
```python
from sqlalchemy.orm import Session

# ‚ùå Lent
for row in data:
    session.add(User(name=row['name'], email=row['email']))
session.commit()

# ‚úÖ Rapide
session.bulk_insert_mappings(
    User,
    [{'name': row['name'], 'email': row['email']} for row in data]
)
session.commit()
```

#### JavaScript - Prisma
```javascript
// ‚ùå Lent
for (const row of data) {
  await prisma.user.create({
    data: { name: row.name, email: row.email }
  });
}

// ‚úÖ Rapide
await prisma.user.createMany({
  data: data.map(row => ({
    name: row.name,
    email: row.email
  }))
});
```

#### Java - Hibernate
```java
// ‚ùå Lent
for (User user : users) {
    session.save(user);
}

// ‚úÖ Rapide avec batching
Session session = sessionFactory.openSession();
Transaction tx = session.beginTransaction();

for (int i = 0; i < users.size(); i++) {
    session.save(users.get(i));

    if (i % 50 == 0) {  // Batch de 50
        session.flush();
        session.clear();
    }
}

tx.commit();
```

**Configuration Hibernate :**
```properties
hibernate.jdbc.batch_size=50
hibernate.order_inserts=true
hibernate.order_updates=true
```

### 3.3. Approche 3 : COPY (La Plus Rapide)

**COPY** est la commande native PostgreSQL pour l'import massif. C'est **la m√©thode la plus rapide**.

#### Syntaxe de base
```sql
COPY users(name, email, age)
FROM '/tmp/users.csv'
WITH (FORMAT csv, HEADER true);
```

**Variante : COPY depuis STDIN**
```python
# Python avec psycopg3
import csv
import psycopg

conn = psycopg.connect("dbname=mydb")
cursor = conn.cursor()

with open('users.csv', 'r') as f:
    cursor.copy(
        "COPY users(name, email, age) FROM STDIN WITH CSV HEADER",
        f
    )

conn.commit()
```

#### Performance de COPY

**Benchmark pour 1,000,000 lignes :**

| M√©thode | Temps | Vitesse |
|---------|-------|---------|
| INSERT une par une | ~60 min | 278 lignes/s |
| INSERT group√© (1000) | ~3 min | 5,555 lignes/s |
| bulk_create (Django) | ~2 min | 8,333 lignes/s |
| **COPY** | **~20 sec** | **50,000 lignes/s** |

**COPY est 180√ó plus rapide que INSERT unitaire !**

#### Pourquoi COPY est si rapide ?

1. **Format binaire optimis√©** (moins de parsing)
2. **Bypass des triggers** (optionnel)
3. **Buffering massif** en m√©moire
4. **Moins de locks** (verrou unique)
5. **Optimisation des index** (reconstruction en masse)

### 3.4. Comparaison des Approches

| Approche | Vitesse | Simplicit√© | Contr√¥le | Cas d'usage |
|----------|---------|------------|----------|-------------|
| INSERT unitaire | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | < 100 lignes |
| INSERT group√© | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 100-10k lignes |
| ORM bulk_create | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 1k-100k lignes |
| **COPY** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê | > 10k lignes |

---

## 4. Bulk UPDATE : Mettre √† Jour en Masse

### 4.1. Le Probl√®me

Vous devez mettre √† jour 10,000 lignes avec des **valeurs diff√©rentes**.

**Approche na√Øve :**
```python
# ‚ùå 10,000 UPDATE
for user_id, new_email in updates:
    User.objects.filter(id=user_id).update(email=new_email)
```

### 4.2. Solution 1 : UPDATE avec CASE

**SQL :**
```sql
UPDATE users
SET email = CASE
    WHEN id = 1 THEN 'alice_new@example.com'
    WHEN id = 2 THEN 'bob_new@example.com'
    WHEN id = 3 THEN 'charlie_new@example.com'
    ELSE email
END
WHERE id IN (1, 2, 3);
```

**G√©n√©rer depuis Python :**
```python
from django.db import connection

def bulk_update_emails(updates):
    """
    updates = [(user_id, new_email), ...]
    """
    if not updates:
        return

    case_sql = "CASE\n"
    ids = []

    for user_id, new_email in updates:
        case_sql += f"  WHEN id = %s THEN %s\n"
        ids.append(user_id)
        ids.append(new_email)

    case_sql += "  ELSE email\nEND"

    user_ids = [u[0] for u in updates]

    sql = f"""
        UPDATE users
        SET email = {case_sql}
        WHERE id IN ({','.join(['%s'] * len(user_ids))})
    """

    with connection.cursor() as cursor:
        cursor.execute(sql, ids + user_ids)
```

### 4.3. Solution 2 : UPDATE FROM (PostgreSQL)

**Plus propre et plus performant :**

```sql
-- Cr√©er une table temporaire
CREATE TEMP TABLE temp_updates (
    id INTEGER,
    new_email VARCHAR(100)
);

-- Ins√©rer les nouvelles valeurs
INSERT INTO temp_updates VALUES
    (1, 'alice_new@example.com'),
    (2, 'bob_new@example.com'),
    (3, 'charlie_new@example.com');

-- UPDATE avec JOIN
UPDATE users
SET email = temp_updates.new_email
FROM temp_updates
WHERE users.id = temp_updates.id;

-- Nettoyer
DROP TABLE temp_updates;
```

**Avec Python (psycopg3) :**
```python
def bulk_update_emails(cursor, updates):
    # Cr√©er table temporaire
    cursor.execute("""
        CREATE TEMP TABLE temp_updates (
            id INTEGER,
            new_email VARCHAR(100)
        ) ON COMMIT DROP
    """)

    # Ins√©rer avec COPY (rapide !)
    with cursor.copy("COPY temp_updates(id, new_email) FROM STDIN") as copy:
        for user_id, new_email in updates:
            copy.write_row([user_id, new_email])

    # UPDATE avec JOIN
    cursor.execute("""
        UPDATE users
        SET email = temp_updates.new_email
        FROM temp_updates
        WHERE users.id = temp_updates.id
    """)
```

**Performance :**
- 10,000 UPDATE unitaires : ~30 secondes
- 1 UPDATE avec CASE : ~2 secondes
- 1 UPDATE FROM temp table : ~0.5 secondes

### 4.4. Solution 3 : ORM bulk_update (Django 2.2+)

```python
# Django 2.2+
users = User.objects.filter(id__in=[1, 2, 3])

# Modifier en m√©moire
for user in users:
    user.email = f"new_{user.email}"

# Bulk update
User.objects.bulk_update(users, ['email'])
```

**SQL g√©n√©r√© (similaire √† UPDATE avec CASE) :**
```sql
UPDATE users
SET email = CASE
    WHEN id = 1 THEN 'new_alice@example.com'
    WHEN id = 2 THEN 'new_bob@example.com'
    WHEN id = 3 THEN 'new_charlie@example.com'
END
WHERE id IN (1, 2, 3);
```

**Options :**
```python
User.objects.bulk_update(
    users,
    ['email', 'last_modified'],  # Champs √† mettre √† jour
    batch_size=1000               # D√©couper en lots
)
```

---

## 5. Bulk DELETE : Supprimer en Masse

### 5.1. DELETE Simple

**Si tous les crit√®res sont identiques :**
```sql
-- ‚úÖ Efficient : 1 seule requ√™te
DELETE FROM users WHERE created_at < '2020-01-01';
```

**ORM √©quivalent :**
```python
# Django
User.objects.filter(created_at__lt='2020-01-01').delete()

# SQLAlchemy
session.query(User).filter(User.created_at < '2020-01-01').delete()
```

### 5.2. DELETE avec Liste d'IDs

**SQL :**
```sql
DELETE FROM users WHERE id IN (1, 2, 3, 4, 5, ..., 10000);
```

**Attention :** PostgreSQL a une limite sur la longueur des requ√™tes.

**Solution : D√©couper en lots**
```python
def bulk_delete_by_ids(user_ids, batch_size=1000):
    for i in range(0, len(user_ids), batch_size):
        batch = user_ids[i:i + batch_size]
        User.objects.filter(id__in=batch).delete()
```

### 5.3. DELETE avec Table Temporaire

Pour des crit√®res complexes :

```sql
-- Cr√©er une table temporaire avec les IDs √† supprimer
CREATE TEMP TABLE to_delete (id INTEGER);

INSERT INTO to_delete
SELECT id FROM users
WHERE some_complex_condition;

-- Supprimer
DELETE FROM users
WHERE id IN (SELECT id FROM to_delete);
```

### 5.4. TRUNCATE vs DELETE

**TRUNCATE** : Vider compl√®tement une table (ultra rapide).

```sql
-- ‚ùå Lent pour 10M lignes
DELETE FROM users;

-- ‚úÖ Instantan√©
TRUNCATE TABLE users;
```

**Diff√©rences :**

| Crit√®re | DELETE | TRUNCATE |
|---------|--------|----------|
| Vitesse | Lente (scan complet) | Instantan√©e |
| Conditions WHERE | ‚úÖ Oui | ‚ùå Non |
| Triggers | ‚úÖ Ex√©cut√©s | ‚ùå Ignor√©s |
| Rollback | ‚úÖ Possible | ‚ö†Ô∏è Possible mais moins s√ªr |
| R√©initialise AUTO_INCREMENT | ‚ùå Non | ‚úÖ Oui |

**Quand utiliser TRUNCATE :**
- Vider compl√®tement une table
- Pas besoin de conditions
- Performance critique

---

## 6. Batching : G√©rer de Gros Volumes

### 6.1. Qu'est-ce que le Batching ?

**Batching** : D√©couper une op√©ration massive en **lots (batches)** plus petits.

**Pourquoi :**
1. **√âviter la saturation m√©moire** (application + PostgreSQL)
2. **Permettre le commit progressif** (rollback partiel possible)
3. **R√©duire la dur√©e des locks** (pas de verrouillage trop long)
4. **G√©rer les timeouts** (requ√™tes tr√®s longues)

### 6.2. Exemple : Import de 10 Millions de Lignes

**Sans batching (probl√©matique) :**
```python
# ‚ùå Tout en m√©moire : OOM (Out Of Memory)
users = [User(name=row['name'], email=row['email']) for row in all_10M_rows]
User.objects.bulk_create(users)  # üí• Crash !
```

**Avec batching :**
```python
BATCH_SIZE = 10000

def import_users_batched(csv_file):
    batch = []

    with open(csv_file) as f:
        reader = csv.DictReader(f)

        for row in reader:
            batch.append(User(name=row['name'], email=row['email']))

            if len(batch) >= BATCH_SIZE:
                User.objects.bulk_create(batch)
                batch = []  # Vider le batch

        # Dernier batch (reste)
        if batch:
            User.objects.bulk_create(batch)
```

**Avantages :**
- M√©moire constante (~10k lignes max)
- Commit tous les 10k (r√©cup√©rable en cas d'erreur)
- Pas de timeout

### 6.3. Batching avec Progr√®s

**Afficher la progression :**
```python
import csv
from tqdm import tqdm  # Barre de progression

def import_users_with_progress(csv_file):
    BATCH_SIZE = 10000
    batch = []

    # Compter les lignes d'abord
    with open(csv_file) as f:
        total_lines = sum(1 for _ in f) - 1  # -1 pour le header

    with open(csv_file) as f:
        reader = csv.DictReader(f)

        for row in tqdm(reader, total=total_lines, desc="Importing users"):
            batch.append(User(name=row['name'], email=row['email']))

            if len(batch) >= BATCH_SIZE:
                User.objects.bulk_create(batch)
                batch = []

        if batch:
            User.objects.bulk_create(batch)

# Affiche : Importing users: 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 4523891/10000000 [02:15<02:44, 33432.89it/s]
```

### 6.4. Batching avec Gestion d'Erreurs

**Principe :** Si un batch √©choue, continuer avec les suivants.

```python
def import_users_resilient(csv_file):
    BATCH_SIZE = 10000
    batch = []
    errors = []

    with open(csv_file) as f:
        reader = csv.DictReader(f)

        for i, row in enumerate(reader):
            try:
                batch.append(User(name=row['name'], email=row['email']))

                if len(batch) >= BATCH_SIZE:
                    try:
                        User.objects.bulk_create(batch)
                    except Exception as e:
                        errors.append({
                            'batch_start': i - BATCH_SIZE,
                            'batch_end': i,
                            'error': str(e)
                        })
                    finally:
                        batch = []

            except Exception as e:
                errors.append({
                    'line': i,
                    'row': row,
                    'error': str(e)
                })

        # Dernier batch
        if batch:
            try:
                User.objects.bulk_create(batch)
            except Exception as e:
                errors.append({
                    'batch_start': i - len(batch),
                    'batch_end': i,
                    'error': str(e)
                })

    return errors
```

---

## 7. Transactions et Atomicit√©

### 7.1. Transaction Globale

**Tout ou rien :**
```python
from django.db import transaction

@transaction.atomic
def import_users(csv_file):
    # Si une erreur survient, TOUT est rollback
    users = []
    with open(csv_file) as f:
        for row in csv.DictReader(f):
            users.append(User(name=row['name'], email=row['email']))

    User.objects.bulk_create(users)
```

**Probl√®me :** Si l'import √©choue √† 99%, tout est perdu.

### 7.2. Transaction par Batch

**Meilleur compromis :**
```python
def import_users_with_batch_transaction(csv_file):
    BATCH_SIZE = 10000
    batch = []

    with open(csv_file) as f:
        reader = csv.DictReader(f)

        for row in reader:
            batch.append(User(name=row['name'], email=row['email']))

            if len(batch) >= BATCH_SIZE:
                with transaction.atomic():
                    User.objects.bulk_create(batch)
                batch = []

        if batch:
            with transaction.atomic():
                User.objects.bulk_create(batch)
```

**Avantage :** Si batch #42 √©choue, les 41 premiers sont conserv√©s.

### 7.3. Savepoints pour Rollback Partiel

```python
def import_users_with_savepoint(csv_file):
    with transaction.atomic():
        batch = []

        with open(csv_file) as f:
            for row in csv.DictReader(f):
                batch.append(User(name=row['name'], email=row['email']))

                if len(batch) >= 10000:
                    sid = transaction.savepoint()  # Point de sauvegarde

                    try:
                        User.objects.bulk_create(batch)
                        transaction.savepoint_commit(sid)
                    except Exception:
                        transaction.savepoint_rollback(sid)  # Rollback partiel

                    batch = []
```

---

## 8. Performance et Optimisations

### 8.1. D√©sactiver les Contraintes Temporairement

**Pour import initial massif SEULEMENT :**

```sql
-- D√©sactiver les triggers
ALTER TABLE users DISABLE TRIGGER ALL;

-- Import massif
COPY users FROM '/tmp/users.csv' WITH CSV;

-- R√©activer
ALTER TABLE users ENABLE TRIGGER ALL;
```

**‚ö†Ô∏è ATTENTION :** Les contraintes ne sont plus v√©rifi√©es !

### 8.2. D√©sactiver les Index Temporairement

**Pour tables sans donn√©es existantes :**

```sql
-- Supprimer les index
DROP INDEX idx_users_email;
DROP INDEX idx_users_created_at;

-- Import massif
COPY users FROM '/tmp/users.csv' WITH CSV;

-- Recr√©er les index (plus rapide en une fois)
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_created_at ON users(created_at);
```

**Performance :** 3-5√ó plus rapide pour import initial.

### 8.3. Augmenter maintenance_work_mem

**Temporairement pour l'import :**

```sql
-- Augmenter la m√©moire pour les op√©rations de maintenance
SET maintenance_work_mem = '2GB';

-- Import
COPY users FROM '/tmp/users.csv' WITH CSV;

-- R√©initialiser
RESET maintenance_work_mem;
```

### 8.4. UNLOGGED Tables (Temporaire)

**Pour imports temporaires :**

```sql
-- Cr√©er une table UNLOGGED (pas de WAL)
CREATE UNLOGGED TABLE users_temp (LIKE users INCLUDING ALL);

-- Import ultra-rapide
COPY users_temp FROM '/tmp/users.csv' WITH CSV;

-- Convertir en table normale
ALTER TABLE users_temp SET LOGGED;
```

**Performance :** 2-3√ó plus rapide.

**‚ö†Ô∏è RISQUE :** Donn√©es perdues en cas de crash pendant l'import.

---

## 9. Gestion des Erreurs et Validation

### 9.1. Validation Avant Import

```python
def validate_csv(csv_file):
    errors = []

    with open(csv_file) as f:
        reader = csv.DictReader(f)

        for i, row in enumerate(reader, start=2):  # start=2 car ligne 1 = header
            # V√©rifier les champs obligatoires
            if not row.get('name'):
                errors.append(f"Ligne {i}: 'name' manquant")

            # V√©rifier le format email
            if not re.match(r'^[\w\.-]+@[\w\.-]+\.\w+$', row.get('email', '')):
                errors.append(f"Ligne {i}: email invalide '{row.get('email')}'")

            # V√©rifier la plage de valeurs
            try:
                age = int(row.get('age', 0))
                if age < 0 or age > 150:
                    errors.append(f"Ligne {i}: √¢ge invalide {age}")
            except ValueError:
                errors.append(f"Ligne {i}: √¢ge doit √™tre un nombre")

    return errors

# Utilisation
errors = validate_csv('users.csv')
if errors:
    print(f"‚ùå {len(errors)} erreurs trouv√©es :")
    for error in errors[:10]:  # Afficher les 10 premi√®res
        print(f"  - {error}")
else:
    print("‚úÖ Validation OK, import possible")
    import_users('users.csv')
```

### 9.2. ignore_conflicts : Ignorer les Doublons

```python
# Django : ignorer les doublons (ON CONFLICT DO NOTHING)
User.objects.bulk_create(users, ignore_conflicts=True)
```

**SQL g√©n√©r√© :**
```sql
INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com')
ON CONFLICT DO NOTHING;
```

### 9.3. update_conflicts : Upsert en Masse

```python
# Django 4.1+ : upsert (INSERT ou UPDATE)
User.objects.bulk_create(
    users,
    update_conflicts=True,
    update_fields=['name'],
    unique_fields=['email']
)
```

**SQL g√©n√©r√© :**
```sql
INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com')
ON CONFLICT (email) DO UPDATE SET name = EXCLUDED.name;
```

---

## 10. Cas Pratiques par Sc√©nario

### 10.1. Import CSV de 100,000 Lignes

**Solution optimale : COPY**

```python
import psycopg

def import_csv_fast(csv_file, table_name):
    conn = psycopg.connect("dbname=mydb")
    cursor = conn.cursor()

    with open(csv_file, 'r') as f:
        cursor.copy(
            f"COPY {table_name} FROM STDIN WITH CSV HEADER",
            f
        )

    conn.commit()
    conn.close()

# Temps : ~2 secondes pour 100k lignes
```

### 10.2. Synchronisation depuis API Externe

**Contexte :** R√©cup√©rer 50,000 produits depuis une API et les mettre √† jour.

```python
import requests

def sync_products_from_api():
    BATCH_SIZE = 1000

    # R√©cup√©rer depuis l'API
    response = requests.get('https://api.example.com/products')
    products_data = response.json()

    # Pr√©parer les objets
    products = [
        Product(
            external_id=p['id'],
            name=p['name'],
            price=p['price']
        )
        for p in products_data
    ]

    # Bulk upsert par batch
    for i in range(0, len(products), BATCH_SIZE):
        batch = products[i:i + BATCH_SIZE]

        Product.objects.bulk_create(
            batch,
            update_conflicts=True,
            update_fields=['name', 'price'],
            unique_fields=['external_id']
        )

    print(f"‚úÖ {len(products)} produits synchronis√©s")
```

### 10.3. Calcul de Statistiques sur Millions de Lignes

**Contexte :** Calculer les statistiques mensuelles sur 10M de transactions.

```python
def calculate_monthly_stats():
    # ‚ùå Charger tout en m√©moire : OOM
    # transactions = Transaction.objects.all()

    # ‚úÖ It√©rer par batch
    BATCH_SIZE = 10000
    offset = 0
    stats = {}

    while True:
        batch = Transaction.objects.all()[offset:offset + BATCH_SIZE]

        if not batch:
            break

        for transaction in batch:
            month = transaction.date.strftime('%Y-%m')
            stats.setdefault(month, {'total': 0, 'count': 0})
            stats[month]['total'] += transaction.amount
            stats[month]['count'] += 1

        offset += BATCH_SIZE

    return stats
```

**Mieux : Faire le calcul en SQL**
```python
from django.db.models import Sum, Count
from django.db.models.functions import TruncMonth

stats = Transaction.objects.annotate(
    month=TruncMonth('date')
).values('month').annotate(
    total=Sum('amount'),
    count=Count('id')
).order_by('month')
```

### 10.4. Migration de Donn√©es entre Tables

**Contexte :** Copier 1M lignes d'une table vers une autre avec transformation.

```sql
-- Solution optimale : INSERT INTO ... SELECT
INSERT INTO new_users (name, email, status)
SELECT
    UPPER(name),                          -- Transformation
    email,
    CASE
        WHEN is_active THEN 'active'
        ELSE 'inactive'
    END
FROM old_users
WHERE created_at >= '2024-01-01';

-- Ultra rapide : pas de passage par l'application
```

---

## 11. Checklist et Bonnes Pratiques

### 11.1. Checklist de Performance

Avant de lancer une op√©ration massive :

- [ ] **Volume estim√©** : Combien de lignes ?
- [ ] **M√©thode choisie** : INSERT group√©, bulk_create, ou COPY ?
- [ ] **Batching configur√©** : Taille de batch adapt√©e (1000-10000)
- [ ] **Transactions** : Par batch ou globale ?
- [ ] **Validation** : Donn√©es valid√©es avant import ?
- [ ] **Gestion erreurs** : Strat√©gie de r√©cup√©ration ?
- [ ] **Index** : D√©sactiver temporairement si table vide ?
- [ ] **Monitoring** : Surveiller l'utilisation m√©moire et CPU ?
- [ ] **Backup** : Sauvegarde avant modification massive ?
- [ ] **Tests** : Test√© sur sous-ensemble d'abord ?

### 11.2. Tailles de Batch Recommand√©es

| Volume Total | Batch Size | Raison |
|--------------|------------|--------|
| < 1,000 | Pas de batching | Overhead inutile |
| 1,000 - 10,000 | 1,000 | √âquilibre performance/m√©moire |
| 10,000 - 100,000 | 5,000 | Bonne performance |
| 100,000 - 1M | 10,000 | Optimal pour gros volumes |
| > 1M | 10,000 - 50,000 | Selon m√©moire disponible |

### 11.3. Guide de Choix de M√©thode

```
Vous devez ins√©rer combien de lignes ?
‚îÇ
‚îú‚îÄ < 100
‚îÇ  ‚îî‚îÄ‚Üí INSERT unitaire OK (simplicit√©)
‚îÇ
‚îú‚îÄ 100 - 1,000
‚îÇ  ‚îî‚îÄ‚Üí ORM bulk_create (bon compromis)
‚îÇ
‚îú‚îÄ 1,000 - 10,000
‚îÇ  ‚îî‚îÄ‚Üí ORM bulk_create avec batching
‚îÇ
‚îú‚îÄ 10,000 - 100,000
‚îÇ  ‚îú‚îÄ Donn√©es d√©j√† en CSV/fichier ?
‚îÇ  ‚îÇ  ‚îî‚îÄ‚Üí COPY (optimal)
‚îÇ  ‚îî‚îÄ Donn√©es depuis API/traitement ?
‚îÇ     ‚îî‚îÄ‚Üí bulk_create avec batching
‚îÇ
‚îî‚îÄ > 100,000
   ‚îî‚îÄ‚Üí COPY + batching + d√©sactiver index temporairement
```

### 11.4. Anti-Patterns √† √âviter

#### ‚ùå Anti-Pattern 1 : Pas de Batching

```python
# ‚ùå Charger 1M lignes en m√©moire
users = [User(...) for i in range(1_000_000)]
User.objects.bulk_create(users)  # OOM !
```

#### ‚ùå Anti-Pattern 2 : Batch Trop Petit

```python
# ‚ùå Batch de 10 : trop d'overhead
for i in range(0, len(users), 10):
    User.objects.bulk_create(users[i:i+10])
```

#### ‚ùå Anti-Pattern 3 : Ignorer les Erreurs

```python
# ‚ùå Aucune gestion d'erreur
try:
    User.objects.bulk_create(users)
except Exception:
    pass  # Donn√©es perdues !
```

#### ‚ùå Anti-Pattern 4 : Transaction Globale Massive

```python
# ‚ùå Si √©chec √† 99%, tout est perdu
@transaction.atomic
def import_10M_users():
    # ... 10M lignes
    pass
```

---

## 12. Monitoring et Debugging

### 12.1. Surveiller l'Utilisation M√©moire

```python
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    print(f"M√©moire utilis√©e : {mem_mb:.2f} MB")

# Pendant l'import
BATCH_SIZE = 10000
for i in range(0, len(users), BATCH_SIZE):
    User.objects.bulk_create(users[i:i+BATCH_SIZE])
    monitor_memory()
```

### 12.2. Mesurer le Temps d'Ex√©cution

```python
import time

def benchmark_import(method, data):
    start = time.time()
    method(data)
    elapsed = time.time() - start

    rows_per_sec = len(data) / elapsed
    print(f"‚è±Ô∏è  Temps : {elapsed:.2f}s")
    print(f"üìä Vitesse : {rows_per_sec:.0f} lignes/s")

# Comparer les m√©thodes
benchmark_import(insert_unitaire, data)      # 278 lignes/s
benchmark_import(bulk_create, data)          # 8,333 lignes/s
benchmark_import(copy_from_file, csv_file)   # 50,000 lignes/s
```

### 12.3. V√©rifier l'Int√©grit√© Apr√®s Import

```python
def verify_import(expected_count):
    actual_count = User.objects.count()

    if actual_count == expected_count:
        print(f"‚úÖ Import OK : {actual_count} lignes")
    else:
        print(f"‚ö†Ô∏è  Attention : {actual_count}/{expected_count} lignes import√©es")
        print(f"   Manque : {expected_count - actual_count} lignes")

# Apr√®s import
verify_import(100000)
```

---

## 13. R√©sum√© et Synth√®se

### 13.1. Points Cl√©s √† Retenir

1. **Les op√©rations unitaires sont 100√ó plus lentes** que les bulk operations
2. **COPY est la m√©thode la plus rapide** pour l'import massif (50,000 lignes/s)
3. **Toujours utiliser le batching** pour > 1,000 lignes
4. **Taille de batch optimale : 1,000 √† 10,000** selon le volume
5. **Transaction par batch** pour permettre la r√©cup√©ration partielle
6. **Valider avant d'importer** pour √©viter les erreurs en cours de route
7. **D√©sactiver les index temporairement** pour import initial massif

### 13.2. Tableau de Performance

**Benchmark : 100,000 lignes**

| M√©thode | Temps | Lignes/s | M√©moire | Complexit√© |
|---------|-------|----------|---------|------------|
| INSERT unitaire | 600s | 167 | Faible | Simple |
| INSERT group√© (x100) | 60s | 1,667 | Moyenne | Simple |
| ORM bulk_create | 12s | 8,333 | Moyenne | Simple |
| ORM bulk_create + batch | 15s | 6,667 | Faible | Moyenne |
| **COPY** | **2s** | **50,000** | Faible | Moyenne |

### 13.3. R√®gles d'Or

#### 1. Volume < 1,000 : Simplicit√©
Pas besoin d'optimisation, INSERT classique suffit.

#### 2. Volume 1,000-10,000 : ORM Bulk
Utilisez `bulk_create` ou √©quivalent.

#### 3. Volume > 10,000 : COPY
Privil√©giez COPY pour la performance maximale.

#### 4. Toujours Batcher
Au-del√† de 1,000 lignes, d√©coupez en lots.

#### 5. Valider d'Abord
Ne jamais importer sans validation pr√©alable.

---

## 14. Pour Aller Plus Loin

### 14.1. Ressources Recommand√©es

**Documentation PostgreSQL :**
- [COPY Command](https://www.postgresql.org/docs/current/sql-copy.html)
- [INSERT Performance](https://www.postgresql.org/docs/current/populate.html)

**Articles :**
- "Bulk Insert Performance" - Cybertec
- "PostgreSQL Bulk Loading" - 2ndQuadrant

**Outils :**
- **pgloader** : Migration de donn√©es avanc√©e
- **pgbench** : Benchmarking PostgreSQL
- **Apache NiFi** : ETL avec PostgreSQL

### 14.2. Sujets Connexes

Maintenant que vous ma√Ætrisez les bulk operations :
- **13. Indexation et Optimisation** : Optimiser les requ√™tes SELECT
- **16.11. Sauvegardes** : Exporter et restaurer de gros volumes
- **11.4. Partitionnement** : G√©rer des tables de plusieurs milliards de lignes

### 14.3. Exercice Mental

**Situation :** Vous devez synchroniser quotidiennement 500,000 produits depuis un API externe vers PostgreSQL. Certains produits existent d√©j√†, d'autres sont nouveaux.

**Questions :**
1. Quelle strat√©gie utiliseriez-vous ?
2. Comment g√©rer les upserts (insertion ou mise √† jour) ?
3. Comment assurer la r√©silience en cas d'erreur ?
4. Comment surveiller les performances ?

**R√©ponse sugg√©r√©e :**
```python
def sync_products_daily():
    # 1. R√©cup√©rer depuis l'API
    products_data = fetch_from_api()  # 500k produits

    # 2. Pr√©parer les objets
    products = [Product(...) for p in products_data]

    # 3. Upsert par batch de 5,000
    BATCH_SIZE = 5000
    errors = []

    for i in range(0, len(products), BATCH_SIZE):
        batch = products[i:i + BATCH_SIZE]

        try:
            with transaction.atomic():
                Product.objects.bulk_create(
                    batch,
                    update_conflicts=True,
                    update_fields=['name', 'price', 'stock'],
                    unique_fields=['external_id']
                )
        except Exception as e:
            errors.append({'batch': i, 'error': str(e)})
            log_error(e)

    # 4. Monitoring
    log_metrics({
        'total': len(products),
        'errors': len(errors),
        'duration': elapsed_time
    })

    return errors
```

---

## Conclusion

Les **bulk operations** et le **batching** sont des techniques essentielles pour travailler efficacement avec PostgreSQL √† grande √©chelle. La diff√©rence entre un import qui prend 10 heures et un qui prend 10 minutes r√©side souvent dans ces optimisations.

**Retenez :**
- **COPY** est votre meilleur ami pour les gros volumes
- **bulk_create/bulk_update** sont parfaits pour le code applicatif
- **Le batching** est obligatoire au-del√† de 1,000 lignes
- **La validation** avant import √©vite les catastrophes
- **Le monitoring** permet de d√©tecter les probl√®mes rapidement

Avec ces techniques, vous √™tes maintenant capable de manipuler des millions de lignes efficacement et en toute confiance.

---


‚è≠Ô∏è [Principes de conception d'APIs avec PostgreSQL](/20-drivers-connexion-applicative/04-conception-apis.md)
