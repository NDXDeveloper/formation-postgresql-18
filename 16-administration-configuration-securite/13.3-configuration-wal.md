üîù Retour au [Sommaire](/SOMMAIRE.md)

# 16.13.3. Configuration WAL (Write-Ahead Log)

## Introduction au WAL

Le **WAL (Write-Ahead Log)** est l'un des m√©canismes les plus fondamentaux de PostgreSQL. C'est le syst√®me qui garantit la **durabilit√©** et la **fiabilit√©** de vos donn√©es, m√™me en cas de panne soudaine du serveur.

Dans cette section, nous allons explorer trois param√®tres critiques qui contr√¥lent le comportement du WAL :
- `wal_level` : Le niveau de d√©tail des informations enregistr√©es
- `max_wal_size` : La taille maximale du WAL avant d√©clenchement d'un checkpoint
- `checkpoint_timeout` : L'intervalle de temps entre les checkpoints

> **üéØ Pourquoi c'est important** : Une mauvaise configuration du WAL peut soit compromettre la s√©curit√© de vos donn√©es, soit causer de graves probl√®mes de performance. Comprendre ces param√®tres est essentiel pour tout administrateur PostgreSQL.

---

## Qu'est-ce que le WAL ? (Concept Fondamental)

### Le Principe du Journal de Bord

Imaginez que vous √™tes un comptable et que vous g√©rez un registre financier. Avant de modifier votre grand livre principal (qui est lourd et complexe √† maintenir), vous notez **d'abord** toutes les transactions dans un petit carnet de notes s√©quentiel. Ce carnet, c'est le WAL.

> **üí° Analogie** : Le WAL est comme un journal de bord o√π PostgreSQL note **tout ce qui se passe** avant de modifier r√©ellement les fichiers de donn√©es. C'est la technique du "write-ahead" (√©crire en avance).

### Pourquoi √âcrire en Avance ?

**Probl√®me sans WAL** :
```
1. Application demande : INSERT INTO users VALUES (...)
2. PostgreSQL modifie directement le fichier de donn√©es
3. üí• PANNE √âLECTRIQUE avant que l'√©criture soit compl√®te
4. ‚ùå Fichier de donn√©es corrompu, donn√©es perdues
```

**Solution avec WAL** :
```
1. Application demande : INSERT INTO users VALUES (...)
2. PostgreSQL √©crit d'abord dans le WAL (fichier s√©quentiel, rapide)
3. PostgreSQL confirme : "Transaction valid√©e ‚úÖ"
4. Plus tard, PostgreSQL met √† jour le fichier de donn√©es
5. üí• PANNE √âLECTRIQUE
6. Au red√©marrage : PostgreSQL rejoue le WAL ‚Üí ‚úÖ Donn√©es r√©cup√©r√©es !
```

### Les Avantages du WAL

1. **Durabilit√© (ACID - Durability)** : Les transactions valid√©es sont garanties persistantes
2. **Performance** : √âcriture s√©quentielle (WAL) est plus rapide qu'al√©atoire (fichiers de donn√©es)
3. **R√©plication** : Le WAL peut √™tre envoy√© √† d'autres serveurs pour r√©plication
4. **Point-In-Time Recovery (PITR)** : Restauration √† n'importe quel moment dans le pass√©
5. **Crash Recovery** : R√©cup√©ration automatique apr√®s une panne

### Anatomie Physique du WAL

Le WAL est compos√© de **segments** (fichiers) stock√©s dans le r√©pertoire `pg_wal/` :

```
/var/lib/postgresql/18/main/pg_wal/
‚îú‚îÄ‚îÄ 000000010000000000000001  (16 MB)
‚îú‚îÄ‚îÄ 000000010000000000000002  (16 MB)
‚îú‚îÄ‚îÄ 000000010000000000000003  (16 MB)
‚îî‚îÄ‚îÄ ...
```

**Caract√©ristiques** :
- Chaque segment fait **16 MB** (par d√©faut, configurable √† la compilation)
- Les segments sont nomm√©s s√©quentiellement
- PostgreSQL √©crit toujours de mani√®re s√©quentielle (tr√®s efficace sur HDD et SSD)
- Les anciens segments sont soit recycl√©s, soit archiv√©s (selon configuration)

---

## 1. wal_level : Le Niveau de D√©tail du Journal

### Qu'est-ce que c'est ?

`wal_level` d√©finit **la quantit√© d'informations** que PostgreSQL va enregistrer dans le WAL. Plus le niveau est √©lev√©, plus d'informations sont stock√©es, ce qui permet plus de fonctionnalit√©s mais g√©n√®re plus de volume de donn√©es.

### Les Valeurs Possibles

| Valeur | Description | Volume WAL | Cas d'Usage |
|--------|-------------|------------|-------------|
| `minimal` | Minimum vital pour crash recovery | ‚¨áÔ∏è Tr√®s faible | ‚ö†Ô∏è D√©conseill√© (limit√©) |
| `replica` | Permet la r√©plication physique | üìä Normal | ‚úÖ Serveur avec replicas physiques |
| `logical` | Permet la r√©plication logique | üìà √âlev√© | ‚úÖ R√©plication logique, CDC |

### D√©tails de Chaque Niveau

#### minimal (D√©pr√©ci√© et Limit√©)

```conf
wal_level = minimal  # ‚ö†Ô∏è NE PAS UTILISER EN PRODUCTION
```

**Enregistre** :
- Les modifications minimales pour le crash recovery
- Juste assez pour r√©cup√©rer apr√®s une panne

**Ne permet PAS** :
- ‚ùå R√©plication (ni physique ni logique)
- ‚ùå Point-In-Time Recovery (PITR)
- ‚ùå Standby servers (hot standby)
- ‚ùå Archivage WAL

**Quand l'utiliser** :
- Presque jamais en production
- Peut-√™tre pour des tests locaux sans importance

> **‚ö†Ô∏è Avertissement** : `minimal` est un pi√®ge ! Il semble attrayant (moins de WAL), mais vous perdez toute capacit√© de r√©plication et de sauvegarde avanc√©e. **√âvitez-le absolument en production.**

#### replica (Recommand√© par D√©faut)

```conf
wal_level = replica  # ‚úÖ D√âFAUT dans PostgreSQL moderne
```

**Enregistre** :
- Toutes les informations n√©cessaires pour la r√©plication physique
- M√©tadonn√©es sur les pages modifi√©es
- √âtat des verrous pour hot standby

**Permet** :
- ‚úÖ R√©plication physique (streaming replication)
- ‚úÖ Standby servers avec hot standby (lecture possible)
- ‚úÖ Point-In-Time Recovery (PITR)
- ‚úÖ Archivage WAL pour backups
- ‚úÖ pg_basebackup

**Volume WAL** : Normal, raisonnable pour la plupart des cas

**Quand l'utiliser** :
- **Par d√©faut pour 90% des installations**
- Serveurs avec r√©plication physique
- Serveurs n√©cessitant PITR
- Production standard

#### logical (Pour R√©plication Logique)

```conf
wal_level = logical  # Pour r√©plication logique et CDC
```

**Enregistre** :
- Tout ce que `replica` enregistre, PLUS :
- Informations sur les lignes modifi√©es (ancien/nouveau √©tat)
- D√©tails permettant de reconstruire les requ√™tes SQL logiquement

**Permet** :
- ‚úÖ Tout ce que `replica` permet, PLUS :
- ‚úÖ R√©plication logique (publications/subscriptions)
- ‚úÖ Change Data Capture (CDC)
- ‚úÖ Logical Decoding pour streaming d'√©v√©nements
- ‚úÖ Outils comme Debezium, pglogical

**Volume WAL** : ~10-30% plus √©lev√© que `replica`

**Quand l'utiliser** :
- R√©plication s√©lective (certaines tables seulement)
- Migration entre versions majeures avec z√©ro downtime
- Change Data Capture pour microservices
- Event sourcing / architecture √©v√©nementielle
- Int√©gration avec syst√®mes externes (Kafka, etc.)

### Comparaison Visuelle du Volume WAL

```
M√™me charge de travail (1000 transactions) :

minimal:  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  (~20 MB)
replica:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  (~60 MB)  ‚Üê RECOMMAND√â
logical:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  (~80 MB)
```

### Comment Choisir ?

**Arbre de D√©cision** :
```
Avez-vous besoin de r√©plication logique ou CDC ?
‚îú‚îÄ OUI ‚Üí wal_level = logical
‚îî‚îÄ NON ‚Üí Avez-vous besoin de r√©plication physique ou PITR ?
    ‚îú‚îÄ OUI ‚Üí wal_level = replica ‚úÖ (D√©faut recommand√©)
    ‚îî‚îÄ NON ‚Üí wal_level = replica quand m√™me ! (pour la flexibilit√© future)
```

> **üéØ Recommandation** : Utilisez **`wal_level = replica`** par d√©faut. Passez √† `logical` seulement si vous avez un besoin sp√©cifique de r√©plication logique.

### Changement de wal_level

```sql
-- V√©rifier la valeur actuelle
SHOW wal_level;

-- Modifier (n√©cessite un RED√âMARRAGE)
ALTER SYSTEM SET wal_level = 'logical';

-- Appliquer
SELECT pg_reload_conf();  -- ‚ùå NE SUFFIT PAS !

-- Il FAUT red√©marrer PostgreSQL
```

```bash
sudo systemctl restart postgresql
```

**‚ö†Ô∏è Important** : Changer `wal_level` n√©cessite un **red√©marrage complet** de PostgreSQL.

---

## 2. max_wal_size : La Taille Maximale du WAL

### Qu'est-ce que c'est ?

`max_wal_size` d√©finit **la quantit√© totale de WAL** que PostgreSQL va accumuler avant de d√©clencher un **checkpoint** pour lib√©rer de l'espace.

### Comprendre les Checkpoints

Un **checkpoint** est une op√©ration o√π PostgreSQL :
1. √âcrit toutes les pages modifi√©es en m√©moire vers les fichiers de donn√©es sur disque
2. Marque un point de r√©f√©rence dans le WAL
3. Permet de recycler les anciens segments WAL

> **üí° Analogie** : Imaginez que vous prenez des notes dans un carnet (WAL). De temps en temps, vous recopiez proprement ces notes dans un classeur permanent (fichiers de donn√©es), puis vous pouvez arracher les pages du carnet et les recycler. C'est un checkpoint !

### Le Dilemme des Checkpoints

**Checkpoints fr√©quents (max_wal_size faible)** :
- ‚úÖ Moins d'espace disque utilis√© pour le WAL
- ‚úÖ Recovery rapide apr√®s crash (moins de WAL √† rejouer)
- ‚ùå Performance d√©grad√©e (I/O intensifs fr√©quents)
- ‚ùå Pics de latence visibles par les applications

**Checkpoints espac√©s (max_wal_size √©lev√©)** :
- ‚úÖ Meilleures performances (moins d'I/O)
- ‚úÖ Latence plus stable
- ‚ùå Plus d'espace disque pour le WAL
- ‚ùå Recovery plus lent apr√®s crash

### Valeur par D√©faut

```conf
max_wal_size = 1GB  # D√©faut dans PostgreSQL moderne
```

**Historique** :
- PostgreSQL 9.4 et ant√©rieurs : Param√®tres complexes (`checkpoint_segments`)
- PostgreSQL 9.5+ : Simplifi√© avec `max_wal_size`

### Valeurs Recommand√©es

| Type de Charge | √âcritures/sec | Recommandation | Explication |
|----------------|---------------|----------------|-------------|
| Faible (Lecture intensive) | < 100 | 1-2 GB | D√©faut suffisant |
| Mod√©r√© (OLTP l√©ger) | 100-1000 | 2-4 GB | Checkpoints plus espac√©s |
| √âlev√© (OLTP intensif) | 1000-10000 | 4-8 GB | Absorber les pics d'√©criture |
| Tr√®s √©lev√© (ETL, Bulk Load) | > 10000 | 8-16 GB | Minimiser les checkpoints |
| Data Warehouse | Variable | 4-8 GB | Compromis |

### Calcul Pratique

**Formule de base** :
```
max_wal_size = (Volume √©criture par minute) √ó (Minutes entre checkpoints souhait√©s)
```

**Exemple** :
- Votre base g√©n√®re 500 MB de WAL par minute
- Vous voulez des checkpoints toutes les 10 minutes maximum
- `max_wal_size = 500 MB √ó 10 = 5 GB`

### Impact sur les Performances

**Benchmark typique (OLTP, 100 connexions)** :

| max_wal_size | Checkpoints/heure | TPS moyen | Latence P99 |
|--------------|-------------------|-----------|-------------|
| 1 GB | 24 | 5,200 | 150 ms |
| 4 GB | 6 | 5,800 | 95 ms |
| 8 GB | 3 | 6,100 | 80 ms |
| 16 GB | 1-2 | 6,200 | 75 ms |

**Observation** : Passer de 1 GB √† 8 GB r√©duit la latence P99 de **47%** !

### Configuration

```conf
# Dans postgresql.conf
max_wal_size = 4GB
```

**Avantage** : Peut √™tre modifi√© **sans red√©marrage** (reload suffit)

```sql
ALTER SYSTEM SET max_wal_size = '4GB';
SELECT pg_reload_conf();
```

### Surveillance

```sql
-- Voir les checkpoints r√©cents
SELECT * FROM pg_stat_bgwriter;

-- Nombre de checkpoints d√©clench√©s par max_wal_size vs temps
SELECT
    checkpoints_timed,    -- D√©clench√©s par checkpoint_timeout
    checkpoints_req,      -- D√©clench√©s par max_wal_size (trop de WAL)
    ROUND(100.0 * checkpoints_req / (checkpoints_timed + checkpoints_req), 2) AS pct_req
FROM pg_stat_bgwriter;
```

**Interpr√©tation** :
- `checkpoints_req` √©lev√© ‚Üí `max_wal_size` est trop petit, augmentez-le
- `checkpoints_timed` √©lev√© ‚Üí Les checkpoints sont principalement dus au temps (normal)

**Objectif** : Avoir `checkpoints_req` repr√©sentant **moins de 10%** du total.

---

## 3. checkpoint_timeout : L'Intervalle de Temps

### Qu'est-ce que c'est ?

`checkpoint_timeout` d√©finit **l'intervalle de temps maximum** entre deux checkpoints. M√™me si `max_wal_size` n'est pas atteint, un checkpoint sera d√©clench√© apr√®s cette dur√©e.

### Pourquoi un Timeout ?

Sans timeout, si votre base a peu d'√©critures, vous pourriez :
- Accumuler des semaines de WAL sans checkpoint
- Avoir un recovery extr√™mement long apr√®s crash
- Risquer de manquer d'espace disque

Le timeout garantit un **checkpoint r√©gulier** pour limiter la fen√™tre de recovery.

### Valeur par D√©faut

```conf
checkpoint_timeout = 5min  # D√©faut PostgreSQL
```

**Signification** : Au moins un checkpoint toutes les 5 minutes, m√™me si le WAL n'est pas plein.

### Valeurs Recommand√©es

| Contexte | Recommandation | Explication |
|----------|----------------|-------------|
| OLTP production | 10-15 minutes | Bon compromis performance/recovery |
| Data Warehouse | 15-30 minutes | Charges moins fr√©quentes |
| Dev/Test | 5 minutes | D√©faut OK |
| Haute disponibilit√© | 5-10 minutes | Recovery rapide prioritaire |
| Bulk load / ETL | 30 minutes | Minimiser les interruptions |

### Le Compromis Performance vs Recovery

**checkpoint_timeout court (5 minutes)** :
- ‚úÖ Recovery rapide (max 5 min de WAL √† rejouer)
- ‚úÖ Moins de WAL √† conserver
- ‚ùå Checkpoints plus fr√©quents ‚Üí plus d'I/O

**checkpoint_timeout long (30 minutes)** :
- ‚úÖ Moins d'interruptions
- ‚úÖ Meilleures performances d'√©criture
- ‚ùå Recovery lent (jusqu'√† 30 min de WAL √† rejouer)
- ‚ùå Plus d'espace disque pour le WAL

### Relation avec max_wal_size

Ces deux param√®tres travaillent **ensemble** :

```
Un checkpoint se d√©clenche quand :
    (WAL accumul√© >= max_wal_size) OU (Temps √©coul√© >= checkpoint_timeout)
```

**Exemple avec des chiffres** :
```conf
max_wal_size = 4GB
checkpoint_timeout = 15min
```

**Sc√©nario 1 : Charge l√©g√®re**
- G√©n√©ration : 100 MB WAL/minute
- Apr√®s 15 minutes : 1.5 GB de WAL
- ‚è∞ Checkpoint d√©clench√© par **timeout** (15 min atteints avant 4 GB)

**Sc√©nario 2 : Charge intense**
- G√©n√©ration : 400 MB WAL/minute
- Apr√®s 10 minutes : 4 GB de WAL
- üìä Checkpoint d√©clench√© par **max_wal_size** (4 GB atteints avant 15 min)

### Configuration

```conf
# Dans postgresql.conf
checkpoint_timeout = 15min
```

**Avantage** : Peut √™tre modifi√© **sans red√©marrage**

```sql
ALTER SYSTEM SET checkpoint_timeout = '15min';
SELECT pg_reload_conf();
```

### Surveillance et Ajustement

#### Voir les Statistiques de Checkpoint

```sql
-- Vue d'ensemble des checkpoints
SELECT
    checkpoints_timed,
    checkpoints_req,
    checkpoint_write_time,  -- Temps total d'√©criture (ms)
    checkpoint_sync_time,   -- Temps de synchronisation (ms)
    buffers_checkpoint,     -- Nombre de buffers √©crits
    buffers_clean,          -- Buffers √©crits par bgwriter
    maxwritten_clean,       -- Bgwriter stopp√© (trop √©crire)
    buffers_backend,        -- Buffers √©crits par backends (‚ö†Ô∏è mauvais)
    stats_reset
FROM pg_stat_bgwriter;
```

**Indicateurs cl√©s** :
- `checkpoints_timed` : D√©clench√©s par timeout (normal)
- `checkpoints_req` : D√©clench√©s par max_wal_size (si trop √©lev√©, augmenter max_wal_size)
- `buffers_backend` : Si > 0, probl√®me de performance (shared_buffers ou bgwriter)

#### Logs de Checkpoint

```conf
# Dans postgresql.conf
log_checkpoints = on
```

**Dans les logs** :
```
LOG: checkpoint starting: time
LOG: checkpoint complete: wrote 15234 buffers (23.4%);
     0 WAL file(s) added, 0 removed, 3 recycled;
     write=2.847 s, sync=0.123 s, total=3.012 s;
     sync files=142, longest=0.045 s, average=0.001 s;
     distance=4096 kB, estimate=4096 kB
```

**Analyse** :
- `wrote X buffers` : Quantit√© de donn√©es √©crites
- `write=X.X s` : Temps d'√©criture
- `distance=X kB` : WAL g√©n√©r√© depuis dernier checkpoint

---

## Configuration Combin√©e : Le Trio Gagnant

### Tableau des Configurations Recommand√©es

#### Serveur OLTP Production (16 GB RAM, SSD)

```conf
# WAL Configuration - OLTP optimis√©
wal_level = replica                    # R√©plication physique activ√©e
max_wal_size = 4GB                     # Absorber les pics d'√©criture
checkpoint_timeout = 10min             # Checkpoints espac√©s

# Param√®tres compl√©mentaires
wal_compression = on                   # Compresser le WAL (PostgreSQL 14+)
wal_buffers = 16MB                     # Buffer WAL en m√©moire
checkpoint_completion_target = 0.9     # √âtaler les checkpoints sur 90% du timeout
```

**Explication** :
- `wal_level = replica` : Permet r√©plication + PITR
- `max_wal_size = 4GB` : Checkpoints toutes les 10-15 min en charge normale
- `checkpoint_timeout = 10min` : S√©curit√©, recovery < 10 min
- `checkpoint_completion_target = 0.9` : √âtale l'I/O des checkpoints sur 9 minutes (90% de 10 min)

#### Serveur Data Warehouse (64 GB RAM, RAID SSD)

```conf
# WAL Configuration - Analytique
wal_level = replica                    # R√©plication si besoin
max_wal_size = 8GB                     # Gros volumes, checkpoints espac√©s
checkpoint_timeout = 20min             # Recovery non critique

# Optimisations
wal_compression = on
wal_buffers = 32MB                     # Plus de buffer
checkpoint_completion_target = 0.9
```

**Explication** :
- `max_wal_size = 8GB` : Absorber les gros ETL sans checkpoints intempestifs
- `checkpoint_timeout = 20min` : Les requ√™tes sont longues, les checkpoints peuvent l'√™tre aussi

#### Serveur avec R√©plication Logique (32 GB RAM)

```conf
# WAL Configuration - R√©plication logique + CDC
wal_level = logical                    # ‚ö†Ô∏è Niveau logical requis
max_wal_size = 6GB                     # Plus de WAL g√©n√©r√© avec logical
checkpoint_timeout = 15min             # Compromis

# Sp√©cifique r√©plication logique
max_replication_slots = 10             # Nombre de subscribers
max_wal_senders = 10                   # Processus de r√©plication
wal_sender_timeout = 60s               # Timeout connexion replica
```

**Explication** :
- `wal_level = logical` : N√©cessaire pour publications/subscriptions
- `max_wal_size = 6GB` : Compensate le volume WAL accru (~30% plus)

#### Serveur D√©veloppement (8 GB RAM, SSD)

```conf
# WAL Configuration - D√©veloppement
wal_level = replica                    # Par d√©faut, flexible
max_wal_size = 2GB                     # Moins de donn√©es, moins de WAL
checkpoint_timeout = 5min              # D√©faut OK

# Logs pour apprentissage
log_checkpoints = on
```

### Param√®tres Compl√©mentaires Importants

#### checkpoint_completion_target

```conf
checkpoint_completion_target = 0.9  # Valeur entre 0.0 et 1.0
```

**R√¥le** : √âtale l'√©criture des buffers sales (dirty buffers) sur une fraction du `checkpoint_timeout`.

**Exemple** :
- `checkpoint_timeout = 10min`
- `checkpoint_completion_target = 0.9`
- R√©sultat : L'√©criture se fait progressivement sur **9 minutes** au lieu d'un burst instantan√©

**Impact** :
- Valeur basse (0.5) ‚Üí I/O en burst, pics de latence
- Valeur haute (0.9) ‚Üí I/O liss√©, latence stable ‚úÖ

**Recommandation** : **0.9** dans 99% des cas.

#### wal_buffers

```conf
wal_buffers = -1  # Auto (par d√©faut, = 1/32 de shared_buffers, entre 64kB et 16MB)
# Ou manuel :
wal_buffers = 16MB
```

**R√¥le** : Quantit√© de m√©moire pour bufferiser le WAL avant √©criture disque.

**Recommandations** :
- `-1` (auto) suffit dans la plupart des cas
- Si charge d'√©criture TR√àS intensive : 16-32 MB

#### wal_compression

```conf
wal_compression = on  # PostgreSQL 14+
```

**R√¥le** : Compresse les pages compl√®tes (full-page writes) dans le WAL.

**Impact** :
- ‚úÖ R√©duction du volume WAL de 20-40%
- ‚úÖ Moins d'I/O disque
- ‚ùå L√©ger overhead CPU (~1-2%)

**Recommandation** : **Activez-le** sauf si CPU est le goulot d'√©tranglement.

#### wal_sync_method

```conf
wal_sync_method = fdatasync  # D√©faut sur Linux, optimal
```

**Valeurs possibles** :
- `fdatasync` (Linux) ‚úÖ
- `fsync` (compatible mais plus lent)
- `open_datasync` (certains Unix)
- `open_sync` (compatible)

**Recommandation** : Gardez `fdatasync` sur Linux.

---

## Monitoring et Diagnostics WAL

### 1. Vue Globale des Statistiques WAL

```sql
-- Statistiques compl√®tes du WAL
SELECT
    pg_walfile_name(pg_current_wal_lsn()) as current_wal_file,
    pg_current_wal_lsn() as current_lsn,
    pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0') / 1024 / 1024 AS wal_generated_mb,
    (SELECT setting FROM pg_settings WHERE name = 'max_wal_size') as max_wal_size,
    (SELECT setting FROM pg_settings WHERE name = 'checkpoint_timeout') as checkpoint_timeout;
```

### 2. Taux de G√©n√©ration WAL

```sql
-- Cr√©er une baseline
CREATE TABLE wal_baseline AS
SELECT
    now() as measured_at,
    pg_current_wal_lsn() as lsn,
    pg_stat_get_wal_buffers_full() as buffers_full
FROM pg_stat_wal;

-- 5 minutes plus tard...
WITH current_state AS (
    SELECT
        now() as measured_at,
        pg_current_wal_lsn() as lsn
)
SELECT
    EXTRACT(EPOCH FROM (c.measured_at - b.measured_at)) / 60 AS minutes_elapsed,
    pg_wal_lsn_diff(c.lsn, b.lsn) / 1024 / 1024 AS wal_mb_generated,
    (pg_wal_lsn_diff(c.lsn, b.lsn) / 1024 / 1024) /
        (EXTRACT(EPOCH FROM (c.measured_at - b.measured_at)) / 60) AS wal_mb_per_minute
FROM wal_baseline b, current_state c;
```

### 3. Analyser les Checkpoints

```sql
-- Ratio checkpoints timed vs requested
WITH checkpoint_stats AS (
    SELECT
        checkpoints_timed,
        checkpoints_req,
        ROUND(100.0 * checkpoints_req /
            NULLIF(checkpoints_timed + checkpoints_req, 0), 2) AS pct_requested,
        checkpoint_write_time,
        checkpoint_sync_time
    FROM pg_stat_bgwriter
)
SELECT
    *,
    CASE
        WHEN pct_requested > 50 THEN '‚ö†Ô∏è Augmenter max_wal_size'
        WHEN pct_requested > 20 THEN '‚ö° Consid√©rer augmentation max_wal_size'
        ELSE '‚úÖ Configuration OK'
    END as recommendation
FROM checkpoint_stats;
```

**Interpr√©tation** :
- `pct_requested < 10%` : ‚úÖ Excellent
- `pct_requested 10-20%` : üü° Acceptable
- `pct_requested > 20%` : üü† Augmenter max_wal_size
- `pct_requested > 50%` : üî¥ Probl√®me, augmenter max_wal_size imm√©diatement

### 4. V√©rifier l'Espace Disque WAL

```bash
# Taille totale du r√©pertoire pg_wal
du -sh /var/lib/postgresql/18/main/pg_wal/

# Nombre de fichiers WAL
ls /var/lib/postgresql/18/main/pg_wal/ | wc -l
```

```sql
-- Via SQL (PostgreSQL 18+)
SELECT
    COUNT(*) as wal_files,
    pg_size_pretty(SUM(size)) as total_size,
    pg_size_pretty(AVG(size)) as avg_file_size
FROM pg_ls_waldir();
```

### 5. Dashboard de Monitoring Complet

```sql
-- Vue d'ensemble pour monitoring quotidien
WITH wal_stats AS (
    SELECT
        pg_current_wal_lsn() as current_lsn,
        pg_walfile_name(pg_current_wal_lsn()) as current_file
),
checkpoint_stats AS (
    SELECT
        checkpoints_timed,
        checkpoints_req,
        ROUND(100.0 * checkpoints_req /
            (checkpoints_timed + checkpoints_req), 2) AS pct_req,
        pg_size_pretty(buffers_checkpoint * 8192) as data_written,
        checkpoint_write_time / 1000.0 as write_time_sec,
        checkpoint_sync_time / 1000.0 as sync_time_sec
    FROM pg_stat_bgwriter
),
config AS (
    SELECT
        (SELECT setting FROM pg_settings WHERE name = 'wal_level') as wal_level,
        (SELECT setting FROM pg_settings WHERE name = 'max_wal_size') as max_wal_size,
        (SELECT setting FROM pg_settings WHERE name = 'checkpoint_timeout') as checkpoint_timeout
)
SELECT
    'WAL Configuration' as metric_group,
    json_build_object(
        'wal_level', c.wal_level,
        'max_wal_size', c.max_wal_size,
        'checkpoint_timeout', c.checkpoint_timeout,
        'current_wal_file', w.current_file
    ) as configuration,
    json_build_object(
        'checkpoints_timed', cs.checkpoints_timed,
        'checkpoints_requested', cs.checkpoints_req,
        'pct_requested', cs.pct_req || '%',
        'data_written', cs.data_written,
        'avg_write_time', ROUND(cs.write_time_sec, 2) || 's',
        'avg_sync_time', ROUND(cs.sync_time_sec, 2) || 's'
    ) as statistics
FROM wal_stats w, checkpoint_stats cs, config c;
```

---

## Troubleshooting : Probl√®mes Courants

### Probl√®me 1 : Checkpoints Trop Fr√©quents

**Sympt√¥me** :
```sql
SELECT
    checkpoints_req / (checkpoints_timed + checkpoints_req) * 100 AS pct_req
FROM pg_stat_bgwriter;
-- R√©sultat : > 50%
```

**Dans les logs** :
```
LOG: checkpoint starting: xlog
LOG: checkpoint complete: ... distance=4096 kB, estimate=4096 kB
```

**Cause** : `max_wal_size` est trop petit pour la charge d'√©criture.

**Solution** :
```conf
# Augmenter progressivement
max_wal_size = 8GB  # Double la valeur actuelle
```

```sql
ALTER SYSTEM SET max_wal_size = '8GB';
SELECT pg_reload_conf();
```

### Probl√®me 2 : Pics de Latence P√©riodiques

**Sympt√¥me** : Toutes les X minutes, latence applicative explose.

**Cause** : Checkpoints provoquent des I/O bursts.

**Solution** :
```conf
# √âtaler les √©critures
checkpoint_completion_target = 0.9  # Au lieu de 0.5

# Espacer les checkpoints
checkpoint_timeout = 15min  # Au lieu de 5min
max_wal_size = 4GB  # Augmenter
```

### Probl√®me 3 : Espace Disque WAL √âpuis√©

**Sympt√¥me** :
```
ERROR: could not write to file "pg_wal/xlogtemp.12345": No space left on device
PANIC: could not write to WAL file
```

**Causes possibles** :

1. **Slots de r√©plication bloqu√©s**
```sql
-- Identifier les slots qui retiennent le WAL
SELECT
    slot_name,
    slot_type,
    active,
    pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) / 1024 / 1024 AS mb_behind
FROM pg_replication_slots
ORDER BY mb_behind DESC;
```

**Solution** :
```sql
-- Supprimer le slot bloqu√© (‚ö†Ô∏è Perte r√©plication)
SELECT pg_drop_replication_slot('slot_name');
```

2. **archive_command √©choue**
```sql
-- V√©rifier l'archivage
SELECT * FROM pg_stat_archiver;
```

**Solution** : Corriger le script d'archivage ou d√©sactiver temporairement.

3. **max_wal_size trop √©lev√© + activit√© intense**

**Solution** : Augmenter l'espace disque ou r√©duire temporairement max_wal_size.

### Probl√®me 4 : Recovery Tr√®s Lent Apr√®s Crash

**Sympt√¥me** : PostgreSQL met 30+ minutes √† d√©marrer apr√®s une panne.

**Cause** : Trop de WAL √† rejouer (`checkpoint_timeout` trop √©lev√©, `max_wal_size` trop grand).

**Solution** :
```conf
# R√©duire la fen√™tre de recovery
checkpoint_timeout = 5min   # Au lieu de 30min
max_wal_size = 2GB          # Au lieu de 16GB
```

**Trade-off** : Performances l√©g√®rement r√©duites en √©change d'une recovery rapide.

### Probl√®me 5 : R√©plication en Retard (Lag)

**Sympt√¥me** :
```sql
SELECT
    client_addr,
    state,
    pg_wal_lsn_diff(sent_lsn, write_lsn) / 1024 / 1024 AS write_lag_mb,
    pg_wal_lsn_diff(write_lsn, flush_lsn) / 1024 / 1024 AS flush_lag_mb,
    pg_wal_lsn_diff(flush_lsn, replay_lsn) / 1024 / 1024 AS replay_lag_mb
FROM pg_stat_replication;
-- Lag > 1 GB
```

**Causes** :

1. **R√©seau lent entre primary et replica**
2. **Replica sous-dimensionn√© (I/O, CPU)**
3. **WAL g√©n√©r√© trop rapidement**

**Solutions** :
```conf
# Sur le primary
wal_compression = on  # R√©duire le volume WAL
wal_keep_size = 4GB   # Garder plus de WAL pour les replicas lents

# Sur le replica
max_wal_senders = 5   # Plus de processus de r√©plication
```

---

## Optimisations Avanc√©es

### 1. WAL Compression (PostgreSQL 14+)

```conf
wal_compression = on
wal_compression_level = 1  # PostgreSQL 15+ (1-9, d√©faut = -1 = auto)
```

**B√©n√©fices** :
- R√©duction volume WAL : 20-40%
- Moins d'I/O disque
- R√©plication plus rapide (moins de donn√©es r√©seau)

**Co√ªt** :
- 1-2% CPU suppl√©mentaire
- N√©gligeable sur serveurs modernes

**Recommandation** : ‚úÖ Activez-le par d√©faut.

### 2. Full Page Writes (FPW) - Attention !

```conf
full_page_writes = on  # ‚ö†Ô∏è NE JAMAIS D√âSACTIVER EN PRODUCTION
```

**R√¥le** : Apr√®s chaque checkpoint, la premi√®re modification d'une page est enregistr√©e **enti√®rement** dans le WAL (pas juste le delta).

**Pourquoi** : Protection contre la corruption en cas de partial page write.

**‚ö†Ô∏è DANGER** : D√©sactiver `full_page_writes = off` :
- ‚úÖ R√©duit le volume WAL de 50-70%
- ‚ùå Risque de corruption de donn√©es apr√®s crash
- ‚ùå Uniquement acceptable si filesystem garantit atomic writes (rare)

**Recommandation** : **TOUJOURS laisser √† `on`** sauf configuration tr√®s sp√©cifique avec ZFS ou similaire.

### 3. WAL sur Partition S√©par√©e

**Best Practice Production** : Placer `pg_wal` sur un disque/partition s√©par√©(e).

**Avantages** :
- I/O WAL s√©quentiel non perturb√© par I/O tables (al√©atoire)
- Optimisation possible : HDD pour donn√©es, SSD pour WAL
- Isolation des pannes

**Configuration** :
```bash
# Cr√©er un lien symbolique vers une partition SSD d√©di√©e
mv /var/lib/postgresql/18/main/pg_wal /mnt/wal-ssd/pg_wal
ln -s /mnt/wal-ssd/pg_wal /var/lib/postgresql/18/main/pg_wal
```

### 4. Tuning Avanc√© pour SSD NVMe

```conf
# Configuration ultra-performante pour SSD NVMe rapide
wal_level = replica
max_wal_size = 8GB
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9

# Sp√©cifique SSD NVMe
wal_sync_method = fdatasync
wal_buffers = 32MB
wal_writer_delay = 10ms       # D√©faut 200ms, r√©duire pour SSD
wal_writer_flush_after = 1MB  # D√©faut 1MB, OK pour SSD

# Nouveau PostgreSQL 18 : I/O asynchrone
io_method = 'async'
```

---

## Checklist de Configuration WAL

### ‚úÖ Configuration de Base (Minimum)

```conf
# Obligatoire
wal_level = replica              # R√©plication + PITR
max_wal_size = 4GB               # Ajust√© √† la charge
checkpoint_timeout = 10min       # Compromis recovery/performance
checkpoint_completion_target = 0.9  # Lisser les I/O

# Monitoring
log_checkpoints = on             # Tracer les checkpoints
```

### ‚úÖ Configuration Production Standard

```conf
# WAL Core
wal_level = replica
max_wal_size = 4GB
checkpoint_timeout = 10min
checkpoint_completion_target = 0.9

# Optimisations
wal_compression = on             # PostgreSQL 14+
wal_buffers = 16MB
wal_sync_method = fdatasync      # Linux

# R√©plication
max_wal_senders = 5
wal_keep_size = 2GB              # Garder du WAL pour replicas

# Monitoring
log_checkpoints = on
```

### ‚úÖ Configuration Haute Performance (I/O Intensif)

```conf
# WAL Core
wal_level = replica
max_wal_size = 8GB               # Grande fen√™tre
checkpoint_timeout = 15min       # Checkpoints espac√©s
checkpoint_completion_target = 0.9

# Optimisations
wal_compression = on
wal_buffers = 32MB               # Plus de buffer
wal_writer_delay = 10ms          # Flush rapide (SSD)
wal_sync_method = fdatasync

# R√©plication
max_wal_senders = 10
wal_keep_size = 4GB

# Monitoring
log_checkpoints = on
```

### ‚úÖ Configuration R√©plication Logique

```conf
# WAL Core
wal_level = logical              # ‚ö†Ô∏è Requis pour logical replication
max_wal_size = 6GB               # 30% plus pour compenser le volume
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9

# R√©plication logique
max_replication_slots = 10
max_wal_senders = 10
wal_sender_timeout = 60s

# Optimisations
wal_compression = on
wal_buffers = 16MB

# Monitoring
log_checkpoints = on
```

---

## R√©sum√© et Points Cl√©s

### üéØ Les 3 Param√®tres Essentiels

1. **wal_level** : Le niveau d'information
   - `replica` pour 90% des cas (r√©plication physique + PITR)
   - `logical` seulement si r√©plication logique ou CDC n√©cessaire
   - ‚ö†Ô∏è N√©cessite un **red√©marrage**

2. **max_wal_size** : La taille du "tampon" WAL
   - 1 GB (d√©faut) ‚Üí 4-8 GB (production)
   - Plus grand = checkpoints plus espac√©s = meilleures performances
   - Attention √† l'espace disque et temps de recovery
   - ‚úÖ Modifiable **sans red√©marrage**

3. **checkpoint_timeout** : L'intervalle de temps
   - 5 min (d√©faut) ‚Üí 10-15 min (production)
   - Plus grand = moins d'interruptions
   - Attention au temps de recovery
   - ‚úÖ Modifiable **sans red√©marrage**

### üìä R√®gles d'Or

1. **√âquilibre Performance vs Recovery** :
   - Production OLTP : `max_wal_size = 4GB`, `checkpoint_timeout = 10min`
   - Data Warehouse : `max_wal_size = 8GB`, `checkpoint_timeout = 20min`

2. **Monitoring Critique** :
   - `checkpoints_req < 10%` du total ‚Üí ‚úÖ Bon
   - `checkpoints_req > 20%` du total ‚Üí üî¥ Augmenter max_wal_size

3. **Toujours Activer** :
   - `wal_compression = on` (PostgreSQL 14+)
   - `log_checkpoints = on`
   - `checkpoint_completion_target = 0.9`

4. **Ne Jamais** :
   - D√©sactiver `full_page_writes` en production
   - Mettre `wal_level = minimal` en production
   - Ignorer les alertes d'espace disque WAL

### üîç Surveillance Continue

```sql
-- Requ√™te quotidienne de sanity check
SELECT
    'Checkpoints' as metric,
    checkpoints_timed as timed,
    checkpoints_req as requested,
    ROUND(100.0 * checkpoints_req / (checkpoints_timed + checkpoints_req), 2) || '%' as pct_req,
    CASE
        WHEN checkpoints_req::float / (checkpoints_timed + checkpoints_req) > 0.2
        THEN '‚ö†Ô∏è Augmenter max_wal_size'
        ELSE '‚úÖ OK'
    END as status
FROM pg_stat_bgwriter;
```

---

## Pour Aller Plus Loin

### Documentation Officielle
- [Write-Ahead Logging (WAL)](https://www.postgresql.org/docs/18/wal-intro.html)
- [WAL Configuration](https://www.postgresql.org/docs/18/runtime-config-wal.html)
- [Reliability and the Write-Ahead Log](https://www.postgresql.org/docs/18/wal-reliability.html)

### Articles Approfondis
- "Understanding PostgreSQL WAL" - 2ndQuadrant
- "Tuning Checkpoints for Performance" - Percona Blog
- "PostgreSQL WAL Deep Dive" - CrunchyData

### Outils
- **pg_stat_bgwriter** : Vue syst√®me pour checkpoints
- **pg_stat_archiver** : Surveillance archivage WAL
- **pgBadger** : Analyse logs incluant checkpoints
- **Prometheus + postgres_exporter** : M√©triques WAL en temps r√©el

### Lectures Recommand√©es
- "PostgreSQL: Up and Running" - Chapitre WAL et R√©plication
- "The Internals of PostgreSQL" - Section 9 (WAL)

---

## Conclusion

La configuration du WAL est un **√©quilibre d√©licat** entre :
- **Performance** (checkpoints espac√©s, I/O liss√©)
- **S√©curit√©** (durabilit√© des transactions)
- **Recovery** (temps de r√©cup√©ration apr√®s crash)
- **Espace disque** (volume WAL conserv√©)

Les trois param√®tres `wal_level`, `max_wal_size`, et `checkpoint_timeout` forment le **trio fondamental** de cette configuration. Bien les comprendre et les ajuster selon votre charge de travail peut am√©liorer significativement les performances et la fiabilit√© de votre syst√®me PostgreSQL.

**Prochaine √©tape** : Dans la section 16.13.4, nous explorerons la configuration de l'autovacuum, un autre m√©canisme critique de PostgreSQL qui travaille en symbiose avec le WAL pour maintenir la sant√© de votre base de donn√©es !

---


‚è≠Ô∏è [Configuration autovacuum](/16-administration-configuration-securite/13.4-configuration-autovacuum.md)
