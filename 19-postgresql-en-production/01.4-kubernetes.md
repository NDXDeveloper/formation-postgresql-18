ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 19.1.4. Kubernetes (StatefulSets, Operators)

## Introduction Ã  Kubernetes

### Qu'est-ce que Kubernetes ?

**Kubernetes** (souvent abrÃ©gÃ© **K8s**) est une plateforme open source d'orchestration de conteneurs crÃ©Ã©e par Google en 2014. C'est devenu le standard de facto pour gÃ©rer des applications conteneurisÃ©es Ã  grande Ã©chelle.

**Analogie simple :**

Imaginez un chef d'orchestre (Kubernetes) dirigeant un orchestre (vos applications) :
- Le chef d'orchestre ne joue pas lui-mÃªme, il **coordonne** les musiciens
- Il s'assure que chaque musicien (conteneur) joue au bon moment
- Si un musicien est absent, il le **remplace automatiquement**
- Il ajuste le volume (ressources) selon les besoins
- Il maintient l'harmonie (Ã©tat dÃ©sirÃ©) de l'ensemble

**En termes techniques :**

Kubernetes = SystÃ¨me qui automatise :
- Le **dÃ©ploiement** d'applications conteneurisÃ©es
- La **mise Ã  l'Ã©chelle** (scaling) automatique
- La **gestion** du cycle de vie
- La **rÃ©paration automatique** (self-healing)
- L'**Ã©quilibrage de charge** (load balancing)
- Le **stockage** persistant

### Pourquoi "K8s" ?

**K8s** = Kubernetes
- K = premiÃ¨re lettre
- 8 = nombre de lettres entre K et s (ubernete)
- s = derniÃ¨re lettre

C'est un numeronym, comme i18n (internationalization) ou l10n (localization).

### Vocabulaire de Base Kubernetes

| Terme | DÃ©finition | Analogie |
|-------|-----------|----------|
| **Cluster** | Ensemble de machines (nodes) gÃ©rÃ©es par K8s | Un datacenter complet |
| **Node** | Machine (VM ou physique) dans le cluster | Un serveur individuel |
| **Pod** | Plus petite unitÃ© dÃ©ployable (1+ conteneurs) | Une boÃ®te contenant applications |
| **Deployment** | GÃ¨re des pods sans Ã©tat (stateless) | Recette pour apps web |
| **StatefulSet** | GÃ¨re des pods avec Ã©tat (stateful) | Recette pour bases de donnÃ©es |
| **Service** | Point d'accÃ¨s rÃ©seau stable aux pods | Adresse postale fixe |
| **PersistentVolume (PV)** | Espace de stockage dans le cluster | Disque dur physique |
| **PersistentVolumeClaim (PVC)** | Demande de stockage par un pod | RÃ©servation d'espace disque |
| **Namespace** | Isolation logique de ressources | Appartement dans immeuble |
| **Operator** | Extension K8s avec logique mÃ©tier | Administrateur DBA automatisÃ© |

### Architecture Kubernetes : Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KUBERNETES CLUSTER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚             CONTROL PLANE (Master)                 â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚  â”‚ API      â”‚  â”‚Scheduler â”‚  â”‚Controllerâ”‚          â”‚    â”‚
â”‚  â”‚  â”‚ Server   â”‚  â”‚          â”‚  â”‚ Manager  â”‚          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚    â”‚
â”‚  â”‚  â”‚  etcd    â”‚  â† Ã‰tat du cluster                   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Worker 1   â”‚  â”‚   Worker 2   â”‚  â”‚   Worker 3   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚ â”‚  Pod     â”‚ â”‚  â”‚ â”‚  Pod     â”‚ â”‚  â”‚ â”‚  Pod     â”‚ â”‚      â”‚
â”‚  â”‚ â”‚ [PG-0]   â”‚ â”‚  â”‚ â”‚ [PG-1]   â”‚ â”‚  â”‚ â”‚ [PG-2]   â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚ â”‚  Kubelet â”‚ â”‚  â”‚ â”‚  Kubelet â”‚ â”‚  â”‚ â”‚  Kubelet â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composants :**

**Control Plane (Cerveau) :**
- **API Server** : Point d'entrÃ©e pour toutes commandes K8s
- **Scheduler** : DÃ©cide oÃ¹ placer les pods
- **Controller Manager** : Maintient l'Ã©tat dÃ©sirÃ©
- **etcd** : Base de donnÃ©es distribuÃ©e (Ã©tat du cluster)

**Worker Nodes (Muscles) :**
- **Kubelet** : Agent qui exÃ©cute les pods
- **Container Runtime** : Docker, containerd, CRI-O
- **kube-proxy** : GÃ¨re le rÃ©seau

---

## PostgreSQL sur Kubernetes : DÃ©fis et Solutions

### Pourquoi PostgreSQL sur K8s est Complexe ?

Les bases de donnÃ©es sont **stateful** (avec Ã©tat), ce qui est contraire Ã  la philosophie initiale de Kubernetes (conÃ§u pour stateless).

**DÃ©fis spÃ©cifiques :**

```
DÃ©fi 1 : Persistance des DonnÃ©es
â”œâ”€ ProblÃ¨me : Pods Ã©phÃ©mÃ¨res (peuvent Ãªtre dÃ©truits/recrÃ©Ã©s)
â””â”€ Solution : PersistentVolumes (PV/PVC)

DÃ©fi 2 : IdentitÃ© Stable
â”œâ”€ ProblÃ¨me : Pods ont noms alÃ©atoires (web-7f8b9c-x2k4l)
â””â”€ Solution : StatefulSets (noms prÃ©dictibles : postgres-0, postgres-1)

DÃ©fi 3 : Ordre de DÃ©marrage
â”œâ”€ ProblÃ¨me : Primary doit dÃ©marrer avant Standbys
â””â”€ Solution : StatefulSets (dÃ©marrage sÃ©quentiel)

DÃ©fi 4 : RÃ©seau Stable
â”œâ”€ ProblÃ¨me : IP des pods change Ã  chaque redÃ©marrage
â””â”€ Solution : Headless Services (DNS stable)

DÃ©fi 5 : RÃ©plication et HA
â”œâ”€ ProblÃ¨me : Gestion complexe Primary/Standby, failover
â””â”€ Solution : Operators (Zalando, CloudNativePG)

DÃ©fi 6 : Backups et Restauration
â”œâ”€ ProblÃ¨me : Pas de mÃ©canisme natif K8s
â””â”€ Solution : Operators avec backup intÃ©grÃ©

DÃ©fi 7 : Mises Ã  Jour
â”œâ”€ ProblÃ¨me : Upgrade PostgreSQL sans perte donnÃ©es
â””â”€ Solution : Operators avec gestion versions
```

### Quand Utiliser PostgreSQL sur Kubernetes ?

**âœ… Cas d'Usage IdÃ©aux :**

1. **Applications Cloud-Native**
   - Architecture microservices
   - DÃ©ploiements frÃ©quents
   - Infrastructure as Code (GitOps)

2. **Multi-Environnements**
   - Dev, Staging, Prod sur mÃªme cluster
   - Isolation par namespaces
   - Utilisation optimisÃ©e ressources

3. **Scaling Dynamique**
   - Charge variable
   - Auto-scaling horizontal (read replicas)
   - Utilisation efficace du cluster

4. **Multi-Cloud / Hybrid Cloud**
   - PortabilitÃ© entre clouds (AWS, GCP, Azure)
   - Infrastructure unifiÃ©e
   - Ã‰viter vendor lock-in

5. **Ã‰quipe DevOps Mature**
   - Expertise Kubernetes
   - Automatisation poussÃ©e
   - Culture GitOps

**âŒ Cas oÃ¹ Ã‰viter K8s :**

1. **Ã‰quipe sans Expertise K8s**
   - Courbe d'apprentissage trÃ¨s raide
   - ComplexitÃ© opÃ©rationnelle Ã©levÃ©e

2. **Base de DonnÃ©es Unique Critique**
   - VM ou bare metal plus simple
   - Moins de couches d'abstraction
   - Troubleshooting plus direct

3. **Performance Absolue Requise**
   - Latence sub-milliseconde
   - Overhead K8s inacceptable
   - Bare metal prÃ©fÃ©rable

4. **RÃ©glementations Strictes**
   - Certifications spÃ©cifiques
   - Audits complexes avec K8s

---

## StatefulSets : GÃ©rer PostgreSQL avec Ã‰tat

### Qu'est-ce qu'un StatefulSet ?

Un **StatefulSet** est un contrÃ´leur Kubernetes conÃ§u pour gÃ©rer des applications avec Ã©tat (stateful) comme les bases de donnÃ©es.

**DiffÃ©rence avec Deployment :**

```
Deployment (Stateless - Apps Web) :
â”œâ”€ Pods interchangeables
â”œâ”€ Noms alÃ©atoires (web-7f8b9c-x2k4l)
â”œâ”€ IP changeante
â”œâ”€ Pas d'ordre dÃ©marrage
â””â”€ Exemple : Frontend, API

StatefulSet (Stateful - Bases de DonnÃ©es) :
â”œâ”€ Pods avec identitÃ© unique
â”œâ”€ Noms ordonnÃ©s (postgres-0, postgres-1, postgres-2)
â”œâ”€ DNS stable (postgres-0.postgres-headless.default.svc.cluster.local)
â”œâ”€ DÃ©marrage/arrÃªt sÃ©quentiel
â”œâ”€ PersistentVolume par pod
â””â”€ Exemple : PostgreSQL, MongoDB, Kafka
```

### CaractÃ©ristiques d'un StatefulSet

**1. IdentitÃ© Stable :**

```yaml
# StatefulSet avec 3 rÃ©plicas
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  replicas: 3
  serviceName: postgres-headless  # Service headless requis

# CrÃ©e :
# â”œâ”€ postgres-0  (Primary)
# â”œâ”€ postgres-1  (Standby)
# â””â”€ postgres-2  (Standby)

# Noms toujours identiques aprÃ¨s redÃ©marrage
```

**2. DÃ©marrage SÃ©quentiel :**

```
DÃ©marrage :
1. postgres-0 dÃ©marre et devient Ready
2. postgres-1 dÃ©marre (attend postgres-0 Ready)
3. postgres-2 dÃ©marre (attend postgres-1 Ready)

ArrÃªt (ordre inverse) :
1. postgres-2 s'arrÃªte
2. postgres-1 s'arrÃªte
3. postgres-0 s'arrÃªte
```

**3. Stockage Persistant DÃ©diÃ© :**

```
Chaque pod a son PersistentVolume :
â”œâ”€ postgres-0 â†’ pvc-postgres-0 (100 GB)
â”œâ”€ postgres-1 â†’ pvc-postgres-1 (100 GB)
â””â”€ postgres-2 â†’ pvc-postgres-2 (100 GB)

Survit au redÃ©marrage des pods !
```

**4. DNS PrÃ©visible :**

```
DNS Pattern :
<pod-name>.<service-name>.<namespace>.svc.cluster.local

Exemples :
â”œâ”€ postgres-0.postgres-headless.default.svc.cluster.local
â”œâ”€ postgres-1.postgres-headless.default.svc.cluster.local
â””â”€ postgres-2.postgres-headless.default.svc.cluster.local

Applications peuvent se connecter de maniÃ¨re stable
```

### Exemple StatefulSet PostgreSQL Basique

**Attention :** Ceci est un exemple SIMPLE sans HA. Pour production, utiliser un Operator (voir plus loin).

```yaml
# postgres-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-headless
  labels:
    app: postgres
spec:
  ports:
  - port: 5432
    name: postgres
  clusterIP: None  # Headless service
  selector:
    app: postgres

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres-headless
  replicas: 1  # 1 seul pod (pas de HA dans cet exemple)
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:18
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 5
          periodSeconds: 5
  volumeClaimTemplates:  # Template PVC pour chaque pod
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd  # Adapter Ã  votre cluster
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
type: Opaque
data:
  password: U3VwZXJTZWNyZXQxMjM=  # Base64 de "SuperSecret123"
```

**DÃ©ploiement :**

```bash
# CrÃ©er le StatefulSet
kubectl apply -f postgres-statefulset.yaml

# VÃ©rifier
kubectl get statefulset
kubectl get pods
kubectl get pvc

# Se connecter
kubectl exec -it postgres-0 -- psql -U postgres
```

**Limitations de cet exemple :**
- âš ï¸ Pas de haute disponibilitÃ© (1 seul pod)
- âš ï¸ Pas de rÃ©plication automatique
- âš ï¸ Pas de failover automatique
- âš ï¸ Pas de backups automatisÃ©s
- âš ï¸ Gestion manuelle complexe

**Solution :** Utiliser un **Operator** !

---

## Operators : Automatisation de PostgreSQL sur K8s

### Qu'est-ce qu'un Operator ?

Un **Operator** est une extension Kubernetes qui encode la connaissance opÃ©rationnelle d'un humain expert (DBA) dans un logiciel.

**Analogie :**

```
Sans Operator :
Vous = DBA qui doit :
â”œâ”€ Configurer rÃ©plication manuellement
â”œâ”€ Surveiller Primary/Standby
â”œâ”€ DÃ©tecter pannes
â”œâ”€ Faire failover manuellement
â”œâ”€ Lancer backups Ã  la main
â””â”€ GÃ©rer mises Ã  jour

Avec Operator :
Operator = DBA Robot qui :
â”œâ”€ Configure rÃ©plication automatiquement
â”œâ”€ Surveille 24/7
â”œâ”€ DÃ©tecte pannes instantanÃ©ment
â”œâ”€ Fait failover en 10-30 secondes
â”œâ”€ Lance backups planifiÃ©s
â””â”€ GÃ¨re mises Ã  jour sans perte donnÃ©es
```

**Techniquement :**

Un Operator = **Custom Resource Definition (CRD)** + **Controller**

```
CRD (Custom Resource Definition) :
â””â”€ Ã‰tend l'API Kubernetes avec nouveaux types de ressources
   Exemple : PostgreSQL, PostgresCluster, Backup

Controller :
â””â”€ Logique mÃ©tier qui surveille CRDs et maintient Ã©tat dÃ©sirÃ©
   Exemple : Si Primary down â†’ Promouvoir Standby
```

### Pattern Operator

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Utilisateur                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ kubectl apply -f postgres-cluster.yaml
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kubernetes API Server                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ Enregistre Custom Resource
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Operator Controller (Pod)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Watch : PostgresCluster CRD                    â”‚  â”‚
â”‚  â”‚  Reconcile Loop :                               â”‚  â”‚
â”‚  â”‚  1. Lire Ã©tat actuel                            â”‚  â”‚
â”‚  â”‚  2. Comparer avec Ã©tat dÃ©sirÃ©                   â”‚  â”‚
â”‚  â”‚  3. Actions pour atteindre Ã©tat dÃ©sirÃ©          â”‚  â”‚
â”‚  â”‚     - CrÃ©er StatefulSet                         â”‚  â”‚
â”‚  â”‚     - Configurer rÃ©plication                    â”‚  â”‚
â”‚  â”‚     - Faire backups                             â”‚  â”‚
â”‚  â”‚     - GÃ©rer failover                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ CrÃ©e et gÃ¨re
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Resources Kubernetes (Pods, Services, etc.)  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ PG-0     â”‚  â”‚ PG-1     â”‚  â”‚ PG-2     â”‚            â”‚
â”‚  â”‚ Primary  â”‚  â”‚ Standby  â”‚  â”‚ Standby  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Operators PostgreSQL Populaires

| Operator | CrÃ©ateur | MaturitÃ© | ComplexitÃ© | Recommandation |
|----------|----------|----------|------------|----------------|
| **CloudNativePG** | EDB | â­â­â­â­â­ | Moyenne | âœ… **RecommandÃ© Production** |
| **Zalando** | Zalando (Allemagne) | â­â­â­â­â­ | Ã‰levÃ©e | âœ… Excellent (mature) |
| **Crunchy** | Crunchy Data | â­â­â­â­â­ | Moyenne-Ã‰levÃ©e | âœ… Enterprise |
| **Stolon** | Sorint.lab | â­â­â­ | Moyenne | âš ï¸ Moins actif |
| **KubeDB** | AppsCode | â­â­â­â­ | Faible-Moyenne | âœ… Multi-DB |

**Notre focus :**
1. **CloudNativePG** (recommandÃ© dÃ©buter)
2. **Zalando Postgres Operator** (trÃ¨s mature)

---

## CloudNativePG : L'Operator Moderne

### PrÃ©sentation CloudNativePG

**CloudNativePG** (anciennement Cloud Native PostgreSQL) est dÃ©veloppÃ© par **EnterpriseDB (EDB)**, leader PostgreSQL.

**Points Forts :**
- âœ… **Moderne** : ConÃ§u pour Kubernetes natif
- âœ… **Simple** : API claire et intuitive
- âœ… **Complet** : HA, Backups, Monitoring
- âœ… **Performant** : RÃ©plication streaming native
- âœ… **Open Source** : Apache 2.0
- âœ… **Bien documentÃ©** : Documentation excellente
- âœ… **Actif** : DÃ©veloppement trÃ¨s actif (2024-2025)

**FonctionnalitÃ©s :**

```
CloudNativePG offre :
â”œâ”€ DÃ©ploiement PostgreSQL automatique (1 commande)
â”œâ”€ Haute disponibilitÃ© (Primary + Standbys)
â”œâ”€ Auto-failover (10-30 secondes)
â”œâ”€ RÃ©plication synchrone ou asynchrone
â”œâ”€ Backups automatisÃ©s (pgBackRest intÃ©grÃ©)
â”œâ”€ Point-in-Time Recovery (PITR)
â”œâ”€ Connection pooling (PgBouncer intÃ©grÃ©)
â”œâ”€ Monitoring (mÃ©triques Prometheus)
â”œâ”€ TLS automatique
â”œâ”€ Rolling updates sans downtime
â””â”€ Support PostgreSQL 12 Ã  18
```

### Installation CloudNativePG

**PrÃ©requis :**
- Cluster Kubernetes 1.25+
- kubectl configurÃ©
- Helm 3+ (optionnel, mais recommandÃ©)

**Installation via Manifest :**

```bash
# Installer l'operator (une seule fois par cluster)
kubectl apply -f \
  https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.22/releases/cnpg-1.22.0.yaml

# VÃ©rifier installation
kubectl get deployment -n cnpg-system cnpg-controller-manager

# Logs operator
kubectl logs -n cnpg-system \
  deployment/cnpg-controller-manager -f
```

**Installation via Helm :**

```bash
# Ajouter repo Helm
helm repo add cnpg https://cloudnative-pg.github.io/charts
helm repo update

# Installer
helm install cnpg \
  --namespace cnpg-system \
  --create-namespace \
  cnpg/cloudnative-pg

# VÃ©rifier
helm list -n cnpg-system
kubectl get pods -n cnpg-system
```

### CrÃ©er un Cluster PostgreSQL Simple

**cluster-simple.yaml :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
  namespace: default
spec:
  # Nombre d'instances (1 Primary + N Standbys)
  instances: 3

  # Image PostgreSQL
  imageName: ghcr.io/cloudnative-pg/postgresql:18

  # Stockage
  storage:
    storageClass: fast-ssd  # Adapter Ã  votre cluster
    size: 100Gi

  # Bootstrap (initialisation)
  bootstrap:
    initdb:
      database: app
      owner: app
      secret:
        name: app-secret

  # PostgreSQL Configuration
  postgresql:
    parameters:
      shared_buffers: "2GB"
      max_connections: "200"
      work_mem: "64MB"
      maintenance_work_mem: "1GB"
      # PostgreSQL 18 spÃ©cifique
      wal_compression: "zstd"
      io_method: "async"

  # Ressources
  resources:
    requests:
      memory: "4Gi"
      cpu: "2"
    limits:
      memory: "8Gi"
      cpu: "4"

  # Monitoring
  monitoring:
    enablePodMonitor: true
```

**Secret pour base de donnÃ©es :**

```yaml
# app-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  namespace: default
type: kubernetes.io/basic-auth
data:
  username: YXBw        # base64("app")
  password: U2VjcmV0MTIz  # base64("Secret123")
```

**DÃ©ploiement :**

```bash
# CrÃ©er secret
kubectl apply -f app-secret.yaml

# CrÃ©er cluster PostgreSQL
kubectl apply -f cluster-simple.yaml

# Suivre dÃ©ploiement
kubectl get cluster -w

# DÃ©tails cluster
kubectl describe cluster postgres-cluster

# Pods crÃ©Ã©s
kubectl get pods -l cnpg.io/cluster=postgres-cluster

# Exemple de sortie :
# NAME                  READY   STATUS    RESTARTS   AGE
# postgres-cluster-1    1/1     Running   0          2m  â† Primary
# postgres-cluster-2    1/1     Running   0          1m  â† Standby
# postgres-cluster-3    1/1     Running   0          30s â† Standby
```

**Ce qui a Ã©tÃ© crÃ©Ã© automatiquement :**

```
CloudNativePG a crÃ©Ã© :
â”œâ”€ StatefulSet (3 pods : postgres-cluster-1, -2, -3)
â”œâ”€ Services :
â”‚   â”œâ”€ postgres-cluster-rw  (Read/Write â†’ Primary)
â”‚   â”œâ”€ postgres-cluster-ro  (Read-Only â†’ Standbys)
â”‚   â””â”€ postgres-cluster-r   (Read â†’ Tous)
â”œâ”€ PersistentVolumeClaims (1 par pod)
â”œâ”€ Secrets (certificats TLS, mots de passe)
â”œâ”€ ConfigMaps (configuration PostgreSQL)
â””â”€ RÃ©plication streaming configurÃ©e automatiquement
```

### Se Connecter au Cluster

**Depuis un autre pod dans K8s :**

```bash
# Service Read/Write (Primary)
psql postgresql://app:Secret123@postgres-cluster-rw:5432/app

# Service Read-Only (Standbys)
psql postgresql://app:Secret123@postgres-cluster-ro:5432/app
```

**Depuis l'extÃ©rieur (kubectl port-forward) :**

```bash
# Forward port local â†’ Service K8s
kubectl port-forward \
  service/postgres-cluster-rw 5432:5432

# Dans autre terminal
psql postgresql://app:Secret123@localhost:5432/app
```

**Via LoadBalancer (si cloud provider) :**

```yaml
# loadbalancer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-external
spec:
  type: LoadBalancer
  selector:
    cnpg.io/cluster: postgres-cluster
    role: primary
  ports:
  - port: 5432
    targetPort: 5432
```

### Backups AutomatisÃ©s

**Configuration backup S3-compatible :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:18

  storage:
    size: 100Gi

  # Configuration Backup
  backup:
    # RÃ©tention backups
    retentionPolicy: "30d"

    # Backup vers S3 (ou compatible : MinIO, etc.)
    barmanObjectStore:
      destinationPath: s3://my-bucket/postgres-backups/
      endpointURL: https://s3.amazonaws.com  # ou MinIO URL

      # Credentials S3
      s3Credentials:
        accessKeyId:
          name: s3-credentials
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: s3-credentials
          key: SECRET_ACCESS_KEY

      # Compression
      wal:
        compression: gzip

      # Encryption
      data:
        encryption: AES256

  # Schedule backups (CronJob)
  backup:
    volumeSnapshot:
      online: true
    target: primary
    method: barmanObjectStore
```

**CrÃ©er backup manuel :**

```yaml
# backup-now.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Backup
metadata:
  name: backup-manual-20250101
spec:
  cluster:
    name: postgres-cluster
  method: barmanObjectStore
```

```bash
# Lancer backup
kubectl apply -f backup-now.yaml

# Lister backups
kubectl get backups

# DÃ©tails backup
kubectl describe backup backup-manual-20250101
```

**Scheduled Backups (automatique) :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: postgres-daily-backup
spec:
  # Cron schedule (tous les jours Ã  2h)
  schedule: "0 2 * * *"

  # Politique suspension (si maintenance)
  suspend: false

  # Cluster cible
  cluster:
    name: postgres-cluster

  # MÃ©thode backup
  method: barmanObjectStore
```

**Restauration depuis Backup (PITR) :**

```yaml
# cluster-restore.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-restored
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:18

  # Bootstrap depuis backup
  bootstrap:
    recovery:
      source: postgres-cluster  # Cluster source

      # Point-in-Time (optionnel)
      recoveryTarget:
        targetTime: "2025-01-01 14:30:00.00000+00"

      # Ou backup spÃ©cifique
      backup:
        name: backup-manual-20250101

  # RÃ©fÃ©rence au backup source
  externalClusters:
  - name: postgres-cluster
    barmanObjectStore:
      destinationPath: s3://my-bucket/postgres-backups/
      s3Credentials:
        accessKeyId:
          name: s3-credentials
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
```

### Haute DisponibilitÃ© et Failover

**Fonctionnement du Failover :**

```
Ã‰tat Normal :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Replication    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  postgres-1    â”‚   Streaming       â”‚  postgres-2    â”‚
â”‚  (Primary)     â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•>    â”‚  (Standby)     â”‚
â”‚  Read/Write    â”‚                   â”‚  Read-Only     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Service RW
    (postgres-cluster-rw)

Panne Primary :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  postgres-1    â”‚                   â”‚  postgres-2    â”‚
â”‚  (DOWN) âŒ     â”‚                   â”‚  (Standby)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
CloudNativePG dÃ©tecte panne (10-15s)        â”‚
                                             â†“
                                     Promotion automatique

AprÃ¨s Failover (10-30s) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  postgres-1    â”‚   Replication     â”‚  postgres-2    â”‚
â”‚  (Standby)     â”‚ <â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚  (Primary) âœ…  â”‚
â”‚  Read-Only     â”‚                   â”‚  Read/Write    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â†‘
                                         Service RW
                                    (bascule automatiquement)
```

**Configuration RÃ©plication Synchrone :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
spec:
  instances: 3

  # RÃ©plication synchrone
  postgresql:
    synchronous:
      # Nombre de standbys synchrones
      number: 1

      # MÃ©thode : 'any' ou 'first'
      method: any

  # Quorum : au moins 1 standby sync doit confirmer
  minSyncReplicas: 1
  maxSyncReplicas: 2
```

**Explication :**
- **Synchrone** : Transaction committÃ©e seulement aprÃ¨s rÃ©plication sur standby(s)
- **Avantage** : ZÃ©ro perte de donnÃ©es (RPO = 0)
- **InconvÃ©nient** : Latence lÃ©gÃ¨rement supÃ©rieure

**Test de Failover Manuel :**

```bash
# Identifier le Primary
kubectl get cluster postgres-cluster -o jsonpath='{.status.currentPrimary}'
# Output : postgres-cluster-1

# Simuler panne : supprimer le pod Primary
kubectl delete pod postgres-cluster-1

# Observer failover automatique (10-30s)
kubectl get cluster postgres-cluster -w

# VÃ©rifier nouveau Primary
kubectl get cluster postgres-cluster -o jsonpath='{.status.currentPrimary}'
# Output : postgres-cluster-2 (ou 3)

# L'ancien Primary redÃ©marre automatiquement en Standby
```

### Monitoring et ObservabilitÃ©

**CloudNativePG expose mÃ©triques Prometheus :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
spec:
  instances: 3

  # Activer monitoring
  monitoring:
    enablePodMonitor: true

    # Custom queries
    customQueries:
    - name: "active_connections"
      query: |
        SELECT count(*) as active_connections
        FROM pg_stat_activity
        WHERE state = 'active'
      metrics:
      - active_connections:
          usage: "GAUGE"
          description: "Number of active connections"
```

**MÃ©triques disponibles :**

```
MÃ©triques exposÃ©es (port 9187) :
â”œâ”€ cnpg_pg_replication_lag_bytes
â”œâ”€ cnpg_pg_wal_archive_status
â”œâ”€ cnpg_pg_stat_database_*
â”œâ”€ cnpg_pg_stat_replication_*
â”œâ”€ cnpg_backends_waiting_total
â”œâ”€ cnpg_pg_postmaster_start_time
â””â”€ Toutes mÃ©triques pg_stat_* standard
```

**Dashboard Grafana (prÃ©-configurÃ©) :**

```bash
# Importer dashboard CloudNativePG officiel
# Grafana ID : 17068
# URL : https://grafana.com/grafana/dashboards/17068
```

**Alerting (Prometheus) :**

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: postgres-alerts
spec:
  groups:
  - name: postgresql
    interval: 30s
    rules:
    - alert: PostgreSQLDown
      expr: up{job="postgres-cluster"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL instance down"

    - alert: PostgreSQLReplicationLag
      expr: cnpg_pg_replication_lag_bytes > 100000000  # 100 MB
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Replication lag > 100 MB"

    - alert: PostgreSQLTooManyConnections
      expr: |
        cnpg_backends_total /
        cnpg_pg_settings_max_connections > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL connections > 80%"
```

---

## Zalando Postgres Operator

### PrÃ©sentation Zalando Operator

**Zalando Postgres Operator** est l'un des operators PostgreSQL les plus matures, crÃ©Ã© par Zalando (e-commerce allemand) en 2016.

**Points Forts :**
- âœ… **TrÃ¨s mature** : 8+ ans de production
- âœ… **Battle-tested** : UtilisÃ© par Zalando Ã  grande Ã©chelle
- âœ… **Complet** : HA, Pooling, Monitoring
- âœ… **Patroni intÃ©grÃ©** : Gestion HA robuste
- âœ… **UI Web** : Interface graphique (optionnelle)
- âœ… **Flexible** : Nombreuses options configuration

**DiffÃ©rences avec CloudNativePG :**

| CritÃ¨re | CloudNativePG | Zalando |
|---------|--------------|---------|
| **SimplicitÃ©** | â­â­â­â­â­ Plus simple | â­â­â­â­ Plus complexe |
| **MaturitÃ©** | â­â­â­â­ (rÃ©cent mais actif) | â­â­â­â­â­ (trÃ¨s mature) |
| **HA Backend** | RÃ©plication native | Patroni |
| **Backups** | IntÃ©grÃ© (pgBackRest) | WAL-E/WAL-G (externe) |
| **Connection Pooling** | PgBouncer intÃ©grÃ© | PgBouncer intÃ©grÃ© |
| **UI** | CLI seulement | UI Web disponible |
| **Courbe apprentissage** | Faible | Moyenne |

**Recommandation :**
- **DÃ©butants / SimplicitÃ©** : CloudNativePG
- **Production Ã  grande Ã©chelle / Patroni dÃ©jÃ  utilisÃ©** : Zalando

### Installation Zalando Operator

```bash
# Cloner repo
git clone https://github.com/zalando/postgres-operator.git
cd postgres-operator

# Installer via Helm
helm install postgres-operator \
  ./charts/postgres-operator \
  --namespace postgres-operator \
  --create-namespace

# VÃ©rifier
kubectl get pods -n postgres-operator
```

**Ou via manifest :**

```bash
# Installer CRDs
kubectl apply -f manifests/configmap.yaml
kubectl apply -f manifests/operator-service-account-rbac.yaml
kubectl apply -f manifests/postgres-operator.yaml

# VÃ©rifier
kubectl get deployment -n postgres-operator
```

### CrÃ©er un Cluster PostgreSQL avec Zalando

**minimal-postgres-manifest.yaml :**

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-minimal-cluster
  namespace: default
spec:
  # Version PostgreSQL
  dockerImage: ghcr.io/zalando/spilo-18:3.2-p1

  # Team ID (namespace logique)
  teamId: "myteam"

  # Nombre d'instances
  numberOfInstances: 3

  # Utilisateurs
  users:
    app_user:  # Base de donnÃ©es crÃ©Ã©e automatiquement
    - superuser
    - createdb

  # Bases de donnÃ©es
  databases:
    app_db: app_user  # db: owner

  # Volumes
  volume:
    size: 100Gi
    storageClass: fast-ssd

  # Ressources
  resources:
    requests:
      cpu: "2"
      memory: 4Gi
    limits:
      cpu: "4"
      memory: 8Gi

  # PostgreSQL Configuration
  postgresql:
    version: "18"
    parameters:
      shared_buffers: "2GB"
      max_connections: "200"
      work_mem: "64MB"
      # PostgreSQL 18
      wal_compression: "zstd"

  # Connection Pooler (PgBouncer)
  enableConnectionPooler: true
  connectionPooler:
    numberOfInstances: 2
    mode: "transaction"
    schema: "pooler"
    user: "pooler"
    resources:
      requests:
        cpu: "500m"
        memory: "256Mi"
      limits:
        cpu: "1"
        memory: "512Mi"
```

**DÃ©ploiement :**

```bash
# CrÃ©er cluster
kubectl apply -f minimal-postgres-manifest.yaml

# Suivre crÃ©ation
kubectl get postgresql acid-minimal-cluster -w

# Pods crÃ©Ã©s
kubectl get pods -l cluster-name=acid-minimal-cluster

# Exemple sortie :
# NAME                           READY   STATUS    AGE
# acid-minimal-cluster-0         1/1     Running   2m  â† Primary
# acid-minimal-cluster-1         1/1     Running   1m  â† Standby
# acid-minimal-cluster-2         1/1     Running   30s â† Standby
# acid-minimal-cluster-pooler-0  1/1     Running   1m  â† PgBouncer
# acid-minimal-cluster-pooler-1  1/1     Running   1m  â† PgBouncer
```

**Services crÃ©Ã©s :**

```bash
kubectl get svc -l cluster-name=acid-minimal-cluster

# Output :
# NAME                          TYPE        CLUSTER-IP      PORT(S)
# acid-minimal-cluster          ClusterIP   10.96.100.50    5432  â† Primary
# acid-minimal-cluster-repl     ClusterIP   10.96.100.51    5432  â† Replicas
# acid-minimal-cluster-config   ClusterIP   None            ...   â† Headless
# acid-minimal-cluster-pooler   ClusterIP   10.96.100.52    5432  â† PgBouncer
```

### FonctionnalitÃ©s AvancÃ©es Zalando

#### 1. Logical Backups (Backups Logiques)

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-cluster
spec:
  # ... autres configs ...

  # Backups logiques automatiques
  enableLogicalBackup: true
  logicalBackupSchedule: "30 2 * * *"  # 2h30 tous les jours
```

**Stockage backups (ConfigMap operator) :**

```yaml
# Dans ConfigMap postgres-operator
logical_backup_s3_bucket: "my-postgres-backups"
logical_backup_s3_region: "eu-west-1"
logical_backup_s3_endpoint: ""
logical_backup_s3_sse: "AES256"
```

#### 2. Custom Users et Bases

```yaml
spec:
  users:
    # Utilisateur avec privilÃ¨ges spÃ©cifiques
    app_readonly:
      - login
      - inherit

    app_admin:
      - superuser
      - createdb
      - createrole

  databases:
    production: app_admin
    staging: app_admin
    analytics: app_readonly

  # Permissions custom aprÃ¨s crÃ©ation
  preparedDatabases:
    production:
      defaultUsers: true
      extensions:
        pg_stat_statements: public
        pgcrypto: public
      schemas:
        app: app_admin
        audit: app_admin
```

#### 3. Connection Pooling AvancÃ©

```yaml
spec:
  # PgBouncer configuration
  enableConnectionPooler: true
  connectionPooler:
    numberOfInstances: 3  # HA pooler

    # Mode pooling
    mode: "transaction"  # ou "session" ou "statement"

    # Ressources
    resources:
      requests:
        cpu: "1"
        memory: "512Mi"
      limits:
        cpu: "2"
        memory: "1Gi"

    # Configuration PgBouncer custom
    maxDBConnections: 60  # Max connexions par DB
    defaultPoolSize: 25   # Pool size par user/DB
```

**Connexion via pooler :**

```bash
# Via service pooler
psql postgresql://app_user:password@acid-minimal-cluster-pooler:5432/app_db
```

#### 4. Clones et Standby Clusters

**Clone depuis cluster existant :**

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-clone
spec:
  # Clone depuis cluster source
  clone:
    cluster: "acid-minimal-cluster"
    # Ou depuis backup S3
    # s3_wal_path: "s3://bucket/path"

  numberOfInstances: 2
  # ... autres configs ...
```

**Standby Cluster (DR) :**

```yaml
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-standby-cluster
spec:
  # Standby depuis cluster distant
  standby:
    s3_wal_path: "s3://my-backups/acid-minimal-cluster"

  numberOfInstances: 2
  # Configuration identique au primary
```

#### 5. Major Version Upgrades

```yaml
# Upgrade PostgreSQL 17 â†’ 18
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-cluster
spec:
  dockerImage: ghcr.io/zalando/spilo-18:3.2-p1  # Nouvelle image
  postgresql:
    version: "18"  # Nouvelle version

  # StratÃ©gie upgrade
  maintenanceWindows:
  - Mon:01:00-06:00
  - Sat:00:00-06:00
```

**Processus upgrade (automatique) :**
1. Clone du cluster avec nouvelle version
2. Tests automatiques
3. Basculement pendant fenÃªtre maintenance
4. Rollback automatique si Ã©chec

### UI Web Zalando (Optionnelle)

```bash
# Installer UI
helm install postgres-operator-ui \
  ./charts/postgres-operator-ui \
  --namespace postgres-operator

# Port-forward
kubectl port-forward -n postgres-operator \
  svc/postgres-operator-ui 8081:80

# AccÃ¨s : http://localhost:8081
```

**FonctionnalitÃ©s UI :**
- Visualisation clusters
- Monitoring en temps rÃ©el
- Logs des opÃ©rations
- Gestion utilisateurs/bases
- Pas d'Ã©dition YAML (lecture seule)

---

## Comparaison CloudNativePG vs Zalando

### Tableau Comparatif DÃ©taillÃ©

| FonctionnalitÃ© | CloudNativePG | Zalando | Gagnant |
|----------------|--------------|---------|---------|
| **SimplicitÃ©** | â­â­â­â­â­ API simple | â­â­â­â­ Plus complexe | CloudNativePG |
| **MaturitÃ©** | â­â­â­â­ 3 ans | â­â­â­â­â­ 8+ ans | Zalando |
| **Documentation** | â­â­â­â­â­ Excellente | â­â­â­â­ Bonne | CloudNativePG |
| **Backend HA** | Native streaming | Patroni | Ã‰galitÃ© |
| **Backups** | pgBackRest intÃ©grÃ© | WAL-E/G externe | CloudNativePG |
| **PITR** | âœ… IntÃ©grÃ© | âœ… Via WAL-G | Ã‰galitÃ© |
| **Connection Pooling** | âœ… PgBouncer | âœ… PgBouncer | Ã‰galitÃ© |
| **Monitoring** | âœ… Prometheus | âœ… Prometheus | Ã‰galitÃ© |
| **Rolling Updates** | âœ… Sans downtime | âœ… Sans downtime | Ã‰galitÃ© |
| **UI Web** | âŒ CLI seulement | âœ… UI disponible | Zalando |
| **Logical Backups** | âš ï¸ Manuel | âœ… Automatique | Zalando |
| **Clone Clusters** | âœ… Via backup | âœ… Natif | Zalando |
| **Standby Clusters** | âœ… | âœ… | Ã‰galitÃ© |
| **TLS Auto** | âœ… | âœ… | Ã‰galitÃ© |
| **Multi-tenant** | âœ… Via namespaces | âœ… Via teamId | Ã‰galitÃ© |
| **CommunautÃ©** | â­â­â­â­ Active | â­â­â­â­â­ TrÃ¨s active | Zalando |
| **Support commercial** | EDB | âŒ Community | CloudNativePG |

### Recommandations par Cas d'Usage

**Choisir CloudNativePG si :**

- âœ… DÃ©buter avec PostgreSQL sur K8s
- âœ… API simple et moderne prÃ©fÃ©rÃ©e
- âœ… Backups intÃ©grÃ©s souhaitÃ©s (S3)
- âœ… Support commercial EDB nÃ©cessaire
- âœ… Documentation exhaustive apprÃ©ciÃ©e
- âœ… Ã‰quipe DevOps petite/moyenne

**Choisir Zalando si :**

- âœ… Production Ã  trÃ¨s grande Ã©chelle (100+ clusters)
- âœ… DÃ©jÃ  familier avec Patroni
- âœ… UI Web souhaitÃ©e
- âœ… Logical backups automatiques requis
- âœ… Clonage frÃ©quent de clusters
- âœ… Ã‰quipe DevOps expÃ©rimentÃ©e K8s

**Notre recommandation gÃ©nÃ©rale :**

```
DÃ©butants / PME :
â””â”€ CloudNativePG (simplicitÃ©, documentation)

Grandes entreprises / Scale :
â””â”€ Zalando (maturitÃ©, fonctionnalitÃ©s avancÃ©es)

Besoin support commercial :
â””â”€ CloudNativePG (EDB) ou Crunchy Operator
```

---

## Stockage sur Kubernetes

### StorageClass : DÃ©finir le Type de Stockage

Un **StorageClass** dÃ©finit le "type" de stockage disponible dans le cluster.

**Exemple StorageClass :**

```yaml
# fast-ssd-storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # ou gce-pd, azure-disk, etc.
parameters:
  type: gp3        # AWS : gp3 SSD
  iops: "16000"    # 16 000 IOPS
  throughput: "1000"  # 1000 MB/s
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer  # CrÃ©ation retardÃ©e
allowVolumeExpansion: true  # Resize possible
reclaimPolicy: Retain  # ou Delete
```

**Provisioners courants :**

| Cloud / Storage | Provisioner | Type |
|-----------------|-------------|------|
| **AWS EBS** | `ebs.csi.aws.com` | gp3, io2, etc. |
| **Google PD** | `pd.csi.storage.gke.io` | pd-ssd, pd-balanced |
| **Azure Disk** | `disk.csi.azure.com` | Premium_LRS |
| **Ceph RBD** | `rbd.csi.ceph.com` | RBD |
| **NFS** | `nfs.csi.k8s.io` | NFS |
| **Local Path** | `rancher.io/local-path` | Disque local node |

**Recommandations PostgreSQL :**

```yaml
# Production : SSD haute performance
fast-ssd:
  type: gp3 (AWS) ou pd-ssd (GCP)
  iops: 10000+
  throughput: 500+ MB/s

# DÃ©veloppement : Standard
standard:
  type: gp2 (AWS) ou pd-standard (GCP)
```

### PersistentVolume (PV) et PersistentVolumeClaim (PVC)

**SchÃ©ma conceptuel :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  StorageClass                       â”‚
â”‚  "fast-ssd" : AWS EBS gp3, 10000 IOPS               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Provisionner dynamiquement
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PersistentVolume (PV)                    â”‚
â”‚  Volume physique : /dev/xvdf (100 GB)               â”‚
â”‚  CrÃ©Ã© automatiquement par StorageClass              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Binding (lien)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PersistentVolumeClaim (PVC)                   â”‚
â”‚  Demande de stockage par PostgreSQL Pod             â”‚
â”‚  "Je veux 100 GB de fast-ssd"                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Monte dans
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PostgreSQL Pod                        â”‚
â”‚  Volume montÃ© : /var/lib/postgresql/data            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemple PVC manuel (avec StatefulSet) :**

```yaml
# Les StatefulSets crÃ©ent automatiquement PVC via volumeClaimTemplates
# Mais on peut aussi crÃ©er PVC manuellement :

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data
spec:
  accessModes:
  - ReadWriteOnce  # RWO : 1 pod Ã  la fois
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi
```

**Access Modes :**

| Mode | AbrÃ©viation | Description | Usage PostgreSQL |
|------|-------------|-------------|------------------|
| **ReadWriteOnce** | RWO | 1 pod en RW | âœ… RecommandÃ© |
| **ReadOnlyMany** | ROX | N pods en RO | âŒ Pas utile |
| **ReadWriteMany** | RWX | N pods en RW | âŒ Dangereux (corruption) |

**âš ï¸ Important :** PostgreSQL nÃ©cessite **ReadWriteOnce** (RWO). Jamais ReadWriteMany !

### Snapshots de Volumes

```yaml
# volumesnapshot.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: postgres-snapshot-20250101
spec:
  volumeSnapshotClassName: csi-snapclass
  source:
    persistentVolumeClaimName: postgres-data-postgres-cluster-1
```

**Restauration depuis snapshot :**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data-restored
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: fast-ssd
  dataSource:
    name: postgres-snapshot-20250101
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  resources:
    requests:
      storage: 100Gi
```

---

## Best Practices PostgreSQL sur Kubernetes

### 1. Ressources (Requests/Limits)

```yaml
spec:
  containers:
  - name: postgres
    resources:
      # Requests : Minimum garanti
      requests:
        memory: "4Gi"
        cpu: "2"

      # Limits : Maximum autorisÃ©
      limits:
        memory: "8Gi"
        cpu: "4"
```

**RÃ¨gles :**

```
Memory :
â”œâ”€ requests = MÃ©moire minimum (shared_buffers + overhead)
â”œâ”€ limits = 2Ã— requests (marge pour work_mem)
â””â”€ Toujours dÃ©finir les deux (Ã©viter OOM)

CPU :
â”œâ”€ requests = Usage moyen
â”œâ”€ limits = 2-3Ã— requests (pics de charge)
â””â”€ Ne pas trop limiter (throttling)
```

**Exemple calcul :**

```yaml
# PostgreSQL avec shared_buffers=4GB
requests:
  memory: "6Gi"  # 4GB + 2GB overhead OS/PostgreSQL
  cpu: "2"       # 2 cores minimum

limits:
  memory: "12Gi"  # 2Ã— requests (pics work_mem)
  cpu: "4"        # 2Ã— requests (pics requÃªtes)
```

### 2. Affinity et Anti-Affinity

**Distribuer pods sur nodes diffÃ©rents :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
spec:
  instances: 3

  # Anti-affinity : pas 2 pods sur mÃªme node
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            cnpg.io/cluster: postgres-cluster
        topologyKey: kubernetes.io/hostname
```

**Avantages :**
- TolÃ©rance panne node (Primary et Standby sur nodes diffÃ©rents)
- Meilleure distribution ressources
- HA renforcÃ©e

**Node Affinity (prÃ©fÃ©rer nodes SSD) :**

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: storage-type
          operator: In
          values:
          - ssd
          - nvme
```

### 3. Taints et Tolerations

**DÃ©dier nodes Ã  PostgreSQL :**

```bash
# Taint un node (uniquement PostgreSQL)
kubectl taint nodes node-1 workload=database:NoSchedule

# PostgreSQL tolÃ¨re ce taint
```

```yaml
spec:
  tolerations:
  - key: "workload"
    operator: "Equal"
    value: "database"
    effect: "NoSchedule"
```

### 4. Priority Classes

```yaml
# priorityclass.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-database
value: 1000000  # Plus Ã©levÃ© = plus prioritaire
globalDefault: false
description: "High priority for databases"
```

```yaml
# Utiliser dans Cluster
spec:
  priorityClassName: high-priority-database
```

**Avantage :** En cas de ressources limitÃ©es, PostgreSQL prÃ©empte autres pods.

### 5. Network Policies

**Isoler PostgreSQL :**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-network-policy
spec:
  podSelector:
    matchLabels:
      cnpg.io/cluster: postgres-cluster

  policyTypes:
  - Ingress
  - Egress

  # Ingress : Qui peut se connecter Ã  PostgreSQL
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: app-namespace
    - podSelector:
        matchLabels:
          app: myapp
    ports:
    - protocol: TCP
      port: 5432

  # Egress : OÃ¹ PostgreSQL peut se connecter
  egress:
  - to:
    - podSelector:
        matchLabels:
          cnpg.io/cluster: postgres-cluster
    ports:
    - protocol: TCP
      port: 5432
  - to:  # S3 pour backups
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

### 6. Pod Disruption Budgets (PDB)

**Garantir disponibilitÃ© pendant maintenances :**

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: postgres-pdb
spec:
  minAvailable: 2  # Minimum 2 pods disponibles
  selector:
    matchLabels:
      cnpg.io/cluster: postgres-cluster
```

**Effet :**
- EmpÃªche drain/Ã©viction simultanÃ©e de trop de pods
- Garantit 2 pods minimum pendant mises Ã  jour cluster
- Assure continuitÃ© service

### 7. Secrets Management

**âŒ Mauvaise pratique :**

```yaml
env:
- name: POSTGRES_PASSWORD
  value: "SuperSecret123"  # En clair dans YAML !
```

**âœ… Bonne pratique : Kubernetes Secrets**

```yaml
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
type: Opaque
data:
  password: U3VwZXJTZWNyZXQxMjM=  # base64
```

```yaml
# Utiliser dans pod
env:
- name: POSTGRES_PASSWORD
  valueFrom:
    secretKeyRef:
      name: postgres-secret
      key: password
```

**âœ… Meilleure pratique : External Secrets (Vault, AWS Secrets Manager)**

```yaml
# externalsecret.yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: postgres-secret
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: postgres-secret
    creationPolicy: Owner
  data:
  - secretKey: password
    remoteRef:
      key: database/postgres
      property: password
```

---

## Monitoring et ObservabilitÃ© sur K8s

### Stack Monitoring ComplÃ¨te

**Architecture recommandÃ©e :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL Cluster                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ PG-0     â”‚  â”‚ PG-1     â”‚  â”‚ PG-2     â”‚          â”‚
â”‚  â”‚ :9187    â”‚  â”‚ :9187    â”‚  â”‚ :9187    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚             â”‚             â”‚                â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                     â”‚ Scrape mÃ©triques             â”‚
â”‚                     â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚          Prometheus                      â”‚      â”‚
â”‚  â”‚  - Collecte mÃ©triques                    â”‚      â”‚
â”‚  â”‚  - Stockage time-series                  â”‚      â”‚
â”‚  â”‚  - Alerting rules                        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                      â”‚                             â”‚
â”‚                      â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚          Grafana                         â”‚      â”‚
â”‚  â”‚  - Dashboards                            â”‚      â”‚
â”‚  â”‚  - Visualisations                        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚        AlertManager                      â”‚      â”‚
â”‚  â”‚  - Routage alertes                       â”‚      â”‚
â”‚  â”‚  - Notifications (Email, Slack, etc.)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation Stack Prometheus

**Via kube-prometheus-stack (Helm) :**

```bash
# Ajouter repo
helm repo add prometheus-community \
  https://prometheus-community.github.io/helm-charts
helm repo update

# Installer stack complÃ¨te
helm install kube-prometheus-stack \
  prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
```

**Composants installÃ©s :**
- Prometheus Operator
- Prometheus Server
- AlertManager
- Grafana
- Node Exporter
- Kube State Metrics

### ServiceMonitor CloudNativePG

**CloudNativePG expose automatiquement mÃ©triques si activÃ© :**

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
spec:
  instances: 3
  monitoring:
    enablePodMonitor: true  # Active PodMonitor
```

**PodMonitor crÃ©Ã© automatiquement :**

```yaml
# VÃ©rifiÃ© automatiquement par operator
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: postgres-cluster
spec:
  selector:
    matchLabels:
      cnpg.io/cluster: postgres-cluster
  podMetricsEndpoints:
  - port: metrics
    path: /metrics
```

### Dashboards Grafana

**Importer dashboards PostgreSQL :**

```bash
# AccÃ©der Grafana
kubectl port-forward -n monitoring \
  svc/kube-prometheus-stack-grafana 3000:80

# Credentials par dÃ©faut
# User: admin
# Password: prom-operator (ou rÃ©cupÃ©rer dans secret)

# Dashboards recommandÃ©s (Grafana.com) :
# - CloudNativePG : 17068
# - PostgreSQL Database : 9628
# - PgBouncer : 13209
```

**Dashboard custom :**

```yaml
# configmap-dashboard.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  postgres-dashboard.json: |
    {
      "dashboard": {
        "title": "PostgreSQL Custom",
        "panels": [...]
      }
    }
```

### Alertes Critiques

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: postgresql-alerts
  namespace: monitoring
spec:
  groups:
  - name: postgresql.rules
    interval: 30s
    rules:
    # Alerte : Instance Down
    - alert: PostgreSQLInstanceDown
      expr: up{job=~".*postgres.*"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL instance {{ $labels.pod }} is down"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is down for more than 1 minute"

    # Alerte : Replication Lag
    - alert: PostgreSQLReplicationLag
      expr: |
        (pg_replication_lag{application_name!=""} / 1024 / 1024) > 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL replication lag > 100 MB"
        description: "Replication lag on {{ $labels.pod }} is {{ $value | humanize }} MB"

    # Alerte : Connexions Ã©levÃ©es
    - alert: PostgreSQLTooManyConnections
      expr: |
        sum(pg_stat_activity_count) by (pod)
        /
        max(pg_settings_max_connections) by (pod)
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL connections > 80%"
        description: "Pod {{ $labels.pod }} has {{ $value | humanizePercentage }} connections used"

    # Alerte : Deadlocks
    - alert: PostgreSQLDeadlocks
      expr: rate(pg_stat_database_deadlocks[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL deadlocks detected"
        description: "Database {{ $labels.datname }} has {{ $value }} deadlocks/s"

    # Alerte : Disk Space
    - alert: PostgreSQLDiskSpaceLow
      expr: |
        (1 - (node_filesystem_avail_bytes{mountpoint="/var/lib/postgresql/data"}
        / node_filesystem_size_bytes{mountpoint="/var/lib/postgresql/data"})) > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL disk space > 85%"
```

---

## Troubleshooting Kubernetes

### ProblÃ¨me 1 : Pods en CrashLoopBackOff

**SymptÃ´me :**

```bash
kubectl get pods
# NAME                  READY   STATUS             RESTARTS
# postgres-cluster-1    0/1     CrashLoopBackOff   5
```

**Diagnostic :**

```bash
# Logs du pod
kubectl logs postgres-cluster-1

# Logs prÃ©cÃ©dent crash
kubectl logs postgres-cluster-1 --previous

# Ã‰vÃ©nements
kubectl describe pod postgres-cluster-1

# Erreurs communes :
# - Permission denied /var/lib/postgresql/data
# - Insufficient memory (OOMKilled)
# - Invalid configuration
```

**Solutions :**

```bash
# 1. Permissions PVC
kubectl exec postgres-cluster-1 -- ls -la /var/lib/postgresql/data
# Corriger : chown -R 999:999 dans initContainer

# 2. MÃ©moire insuffisante
kubectl describe pod postgres-cluster-1 | grep -i oom
# Augmenter limits.memory

# 3. Configuration invalide
kubectl logs postgres-cluster-1 | grep FATAL
# Corriger postgresql.parameters
```

### ProblÃ¨me 2 : PVC en Pending

**SymptÃ´me :**

```bash
kubectl get pvc
# NAME                     STATUS    VOLUME   CAPACITY
# postgres-data-cluster-1  Pending
```

**Diagnostic :**

```bash
# Ã‰vÃ©nements PVC
kubectl describe pvc postgres-data-cluster-1

# Erreurs communes :
# - "no persistent volumes available" â†’ CrÃ©er PV ou dynamic provisioning
# - "storageclass not found" â†’ VÃ©rifier StorageClass existe
# - "insufficient capacity" â†’ Quotas dÃ©passÃ©s
```

**Solutions :**

```bash
# VÃ©rifier StorageClass
kubectl get storageclass

# CrÃ©er si manquant
kubectl apply -f storageclass.yaml

# VÃ©rifier quotas namespace
kubectl describe resourcequota -n default
```

### ProblÃ¨me 3 : Connexion RefusÃ©e

**SymptÃ´me :**

```bash
psql -h postgres-cluster-rw -U app
# connection refused
```

**Diagnostic :**

```bash
# Service existe ?
kubectl get svc postgres-cluster-rw

# Endpoints configurÃ©s ?
kubectl get endpoints postgres-cluster-rw

# Pod Ready ?
kubectl get pods -l cnpg.io/cluster=postgres-cluster

# Network Policy bloque ?
kubectl get networkpolicy
```

**Solutions :**

```bash
# Tester depuis pod dans cluster
kubectl run -it --rm debug --image=postgres:18 -- \
  psql postgresql://app:password@postgres-cluster-rw:5432/app

# Si Ã§a marche â†’ ProblÃ¨me NetworkPolicy ou DNS externe
# Si Ã§a marche pas â†’ ProblÃ¨me PostgreSQL (pg_hba.conf)
```

### ProblÃ¨me 4 : Failover ne Fonctionne Pas

**SymptÃ´me :**

Primary down, standby ne se promeut pas.

**Diagnostic :**

```bash
# Status cluster
kubectl get cluster postgres-cluster -o yaml

# VÃ©rifier leader election
kubectl logs -n cnpg-system deployment/cnpg-controller-manager

# VÃ©rifier rÃ©plication
kubectl exec postgres-cluster-1 -- \
  psql -U postgres -c "SELECT * FROM pg_stat_replication;"
```

**Solutions :**

```bash
# Forcer failover manuel
kubectl cnpg promote postgres-cluster postgres-cluster-2

# Ou via annotation
kubectl annotate cluster postgres-cluster \
  cnpg.io/forcedFailover=postgres-cluster-2
```

### ProblÃ¨me 5 : Performance DÃ©gradÃ©e

**Diagnostic :**

```bash
# MÃ©triques ressources
kubectl top pods -l cnpg.io/cluster=postgres-cluster

# Logs slow queries
kubectl logs postgres-cluster-1 | grep "duration:"

# VÃ©rifier I/O wait
kubectl exec postgres-cluster-1 -- iostat -x 1 5

# Connexions actives
kubectl exec postgres-cluster-1 -- \
  psql -U postgres -c "SELECT count(*) FROM pg_stat_activity WHERE state='active';"
```

**Solutions :**

```bash
# 1. Augmenter ressources
kubectl edit cluster postgres-cluster
# â†’ Modifier resources.limits

# 2. Tuning PostgreSQL
kubectl edit cluster postgres-cluster
# â†’ Ajuster postgresql.parameters

# 3. VÃ©rifier StorageClass IOPS
kubectl describe storageclass fast-ssd
```

---

## Migration vers Kubernetes

### StratÃ©gies de Migration

#### 1. Blue/Green Deployment

```
Infrastructure Actuelle (Blue) :
â”œâ”€ PostgreSQL VM/Bare Metal (production)
â””â”€ Applications connectÃ©es

Migration :
1. DÃ©ployer PostgreSQL sur K8s (Green)
2. Configurer rÃ©plication Blue â†’ Green
3. Synchronisation complÃ¨te
4. Basculer applications vers Green
5. Valider quelques jours
6. DÃ©sactiver Blue si OK
```

**Avantages :**
- Rollback facile
- Validation complÃ¨te avant bascule
- Risque minimal

#### 2. RÃ©plication Logique

```bash
# Sur PostgreSQL source (VM/Bare Metal)
CREATE PUBLICATION migration_pub FOR ALL TABLES;

# Sur PostgreSQL K8s
CREATE SUBSCRIPTION migration_sub
  CONNECTION 'host=old-postgres.example.com port=5432 dbname=production user=repl password=pass'
  PUBLICATION migration_pub;

# Synchronisation continue jusqu'Ã  bascule
```

#### 3. Dump/Restore (Downtime)

```bash
# 1. ArrÃªter applications
# 2. pg_dump depuis source
pg_dump -h old-postgres -U postgres -Fc production > backup.dump

# 3. Restore dans K8s
kubectl cp backup.dump postgres-cluster-1:/tmp/
kubectl exec postgres-cluster-1 -- \
  pg_restore -U postgres -d production /tmp/backup.dump

# 4. Valider donnÃ©es
# 5. Rediriger applications vers K8s
```

### Checklist Migration

```
PrÃ©paration :
â˜ Inventaire bases/utilisateurs/extensions
â˜ Dimensionnement K8s (CPU/RAM/Storage)
â˜ Choix Operator (CloudNativePG vs Zalando)
â˜ Configuration StorageClass performante
â˜ Tests performances K8s

Migration :
â˜ DÃ©ploiement PostgreSQL K8s (environnement test)
â˜ Tests fonctionnels application
â˜ Configuration rÃ©plication (si Blue/Green)
â˜ Monitoring configurÃ© (Prometheus/Grafana)
â˜ Runbook failback prÃ©parÃ©
â˜ Validation Ã©quipe

Bascule :
â˜ FenÃªtre de maintenance planifiÃ©e
â˜ Backup final source
â˜ Synchronisation complÃ¨te
â˜ Bascule DNS/Configuration app
â˜ Validation fonctionnelle
â˜ Monitoring intensif 24-48h

Post-Migration :
â˜ DÃ©sactivation source (aprÃ¨s validation)
â˜ Documentation Ã  jour
â˜ Formation Ã©quipe K8s
â˜ RÃ©trospective migration
```

---

## Conclusion : PostgreSQL sur Kubernetes

### RÃ©capitulatif

**Kubernetes transforme l'opÃ©ration de PostgreSQL :**

âœ… **Automatisation poussÃ©e**
- DÃ©ploiement 1 commande
- HA automatique avec failover
- Backups planifiÃ©s
- Rolling updates sans downtime

âœ… **ScalabilitÃ©**
- Read replicas Ã  la demande
- Utilisation optimale ressources cluster
- Multi-environnements (dev/staging/prod)

âœ… **PortabilitÃ©**
- Multi-cloud (AWS, GCP, Azure)
- Ã‰viter vendor lock-in
- Infrastructure as Code (GitOps)

âœ… **RÃ©silience**
- Self-healing automatique
- Distribution anti-affinity
- Pod Disruption Budgets

**Mais avec des dÃ©fis :**

âš ï¸ **ComplexitÃ©**
- Courbe apprentissage K8s + PostgreSQL
- Debugging multi-couches
- Expertise requise

âš ï¸ **Overhead**
- Performance ~95% du bare metal
- Latence rÃ©seau supplÃ©mentaire
- Couches d'abstraction

âš ï¸ **Stateful sur Stateless**
- K8s initialement conÃ§u pour stateless
- Gestion Ã©tat complexe
- PersistentVolumes critiques

### Comparaison Finale : Tous les DÃ©ploiements

| CritÃ¨re | Bare Metal | VM | Conteneurs | Kubernetes |
|---------|-----------|-----|-----------|------------|
| **Performance** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **SimplicitÃ©** | â­â­ | â­â­â­ | â­â­â­â­ | â­â­ |
| **HA Automatique** | âŒ | â­â­â­ | â­â­ | â­â­â­â­â­ |
| **ScalabilitÃ©** | â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **PortabilitÃ©** | â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **CoÃ»t Initial** | â‚¬â‚¬â‚¬â‚¬â‚¬ | â‚¬â‚¬â‚¬â‚¬ | â‚¬â‚¬ | â‚¬â‚¬â‚¬ |
| **Maintenance** | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ |
| **MaturitÃ©** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |

### Quand Utiliser Kubernetes pour PostgreSQL ?

**âœ… Kubernetes est IDÃ‰AL pour :**

1. **Organisations Cloud-Native**
   - Architecture microservices
   - CI/CD mature
   - Culture DevOps Ã©tablie

2. **Multi-Environnements**
   - Dev, staging, prod sur mÃªme infrastructure
   - Isolation namespace
   - Utilisation efficace ressources

3. **Croissance Rapide**
   - Scaling horizontal (read replicas)
   - Ajout environnements frÃ©quent
   - Besoin agilitÃ©

4. **Multi-Cloud / PortabilitÃ©**
   - Ã‰viter vendor lock-in
   - StratÃ©gie multi-cloud
   - Migrations cloud facilitÃ©es

5. **Ã‰quipe DevOps Mature**
   - Expertise Kubernetes
   - Automatisation poussÃ©e
   - GitOps / IaC

**âŒ Ã‰viter Kubernetes si :**

1. **Ã‰quipe sans Expertise K8s**
   - Courbe apprentissage trop raide
   - Risque opÃ©rationnel Ã©levÃ©
   - â†’ PrÃ©fÃ©rer VM avec Patroni

2. **Base de DonnÃ©es Unique Critique**
   - SimplicitÃ© prÃ©fÃ©rable
   - VM ou bare metal plus direct
   - Moins de points de dÃ©faillance

3. **Performance ExtrÃªme Requise**
   - Latence sub-milliseconde
   - Trading haute frÃ©quence
   - â†’ Bare metal optimisÃ©

4. **Budget / Ressources LimitÃ©s**
   - Overhead infrastructure K8s
   - Ressources cluster management
   - â†’ VMs plus Ã©conomiques

### Recommendations Finales

**Par Taille d'Organisation :**

```
Startup / Petite Ã‰quipe (1-10 personnes) :
â””â”€ Managed DBaaS (RDS, Cloud SQL, Azure Database)
   Ou : Conteneurs simples (Docker Compose)

PME (10-50 personnes, quelques bases) :
â””â”€ VM avec Patroni/RÃ©plication
   Ou : CloudNativePG si expertise K8s existante

Moyenne Entreprise (50-200 personnes, 10-50 bases) :
â””â”€ Kubernetes avec CloudNativePG (simplicitÃ©)
   Monitoring centralisÃ© (Prometheus/Grafana)

Grande Entreprise (200+ personnes, 50+ bases) :
â””â”€ Kubernetes avec Zalando Operator (scale)
   Platform Engineering team dÃ©diÃ©e
```

**Par Type d'Application :**

```
Application Web Moderne (SaaS) :
â””â”€ âœ… Kubernetes + CloudNativePG

Microservices :
â””â”€ âœ… Kubernetes + Operator

Application Legacy Monolithique :
â””â”€ âš ï¸ VM (migration progressive vers K8s)

Application Haute FrÃ©quence (Finance) :
â””â”€ âŒ Bare Metal (performance critique)

Plateforme Multi-Tenant :
â””â”€ âœ… Kubernetes (isolation, scaling)
```

### L'Avenir : PostgreSQL et Kubernetes

**Tendances 2025+ :**

1. **Operators de Plus en Plus Intelligents**
   - Auto-tuning IA
   - PrÃ©diction pannes
   - Optimisation automatique

2. **IntÃ©gration Cloud Native**
   - Service Mesh (Istio, Linkerd)
   - GitOps natif (ArgoCD, Flux)
   - Observability renforcÃ©e (OpenTelemetry)

3. **Serverless PostgreSQL sur K8s**
   - Scale-to-zero
   - Pay-per-query
   - Cold starts optimisÃ©s

4. **Edge Computing**
   - PostgreSQL sur edge K8s
   - Synchronisation multi-sites
   - IoT et donnÃ©es locales

**PostgreSQL 18 sur Kubernetes :**
- I/O asynchrone bÃ©nÃ©ficie mÃªme sur K8s
- Colonnes virtuelles simplifient schÃ©mas
- OAuth integration pour architectures modernes
- Compression zstd rÃ©duit trafic rÃ©seau

---

**Conclusion Ultime :**

Kubernetes et PostgreSQL forment une combinaison puissante pour les organisations modernes. Avec les bons outils (Operators), la complexitÃ© est maÃ®trisable et les bÃ©nÃ©fices (HA, scalabilitÃ©, portabilitÃ©) sont considÃ©rables.

**Pour dÃ©buter :**
1. âœ… Installer cluster K8s de test (Minikube, kind, k3s)
2. âœ… DÃ©ployer CloudNativePG (plus simple)
3. âœ… ExpÃ©rimenter failover, backups, scaling
4. âœ… Migrer environnement dev/staging
5. âœ… Production quand Ã©quipe Ã  l'aise

**La clÃ© du succÃ¨s :** Formation, pratique, patience. Kubernetes est un investissement qui paie sur le long terme.

---

**Prochaines Ã©tapes suggÃ©rÃ©es :**
- 19.2. PostgreSQL dans le Cloud (AWS RDS, Google Cloud SQL, Azure)
- 20. Drivers et Connexion Applicative
- 17. Haute DisponibilitÃ© et RÃ©plication (approfondissement)

---


â­ï¸ [PostgreSQL dans le Cloud](/19-postgresql-en-production/02-postgresql-dans-le-cloud.md)
