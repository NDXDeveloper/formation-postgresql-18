ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 19.1.2. Virtual Machines (VM)

## Introduction Ã  la Virtualisation

### Qu'est-ce qu'une Machine Virtuelle ?

Une **machine virtuelle (VM)** est un ordinateur logiciel qui s'exÃ©cute au-dessus d'un ordinateur physique. C'est comme avoir plusieurs ordinateurs indÃ©pendants fonctionnant simultanÃ©ment sur un seul serveur physique.

**Analogie simple :**

Imaginez un grand immeuble (serveur physique) divisÃ© en plusieurs appartements (VMs). Chaque appartement :
- A ses propres murs (isolation)
- Partage les infrastructures communes (Ã©lectricitÃ©, eau = CPU, RAM)
- Est indÃ©pendant des autres (un problÃ¨me dans l'appartement A n'affecte pas B)
- Peut Ãªtre agrandi ou rÃ©duit selon les besoins (ressources ajustables)

### Vocabulaire de Base

| Terme | DÃ©finition | Exemple |
|-------|-----------|---------|
| **Hyperviseur** | Logiciel qui crÃ©e et gÃ¨re les VMs | VMware ESXi, Proxmox, KVM |
| **HÃ´te (Host)** | Serveur physique hÃ©bergeant les VMs | Serveur Dell R750 avec ESXi |
| **InvitÃ© (Guest)** | SystÃ¨me d'exploitation dans la VM | Ubuntu Server dans une VM |
| **vCPU** | CPU virtuel allouÃ© Ã  une VM | 8 vCPU = 8 cÅ“urs virtuels |
| **Snapshot** | Copie instantanÃ©e de l'Ã©tat d'une VM | Backup avant mise Ã  jour |
| **Live Migration** | DÃ©placement VM sans interruption | DÃ©placer VM entre serveurs |

### Pourquoi Utiliser des VMs pour PostgreSQL ?

**Avantages clÃ©s :**

1. **Consolidation des ressources**
   - Un serveur physique peut hÃ©berger 10-20 VMs
   - Meilleure utilisation du matÃ©riel (taux d'occupation 70-80% vs 20-30% bare metal)
   - RÃ©duction des coÃ»ts matÃ©riels

2. **FlexibilitÃ©**
   - Ajustement des ressources Ã  chaud (CPU, RAM)
   - CrÃ©ation rapide de nouveaux environnements (minutes vs jours)
   - Clonage facile pour test/dÃ©veloppement

3. **Haute disponibilitÃ© simplifiÃ©e**
   - Live Migration : dÃ©placer une VM sans arrÃªt
   - Failover automatique en cas de panne matÃ©rielle
   - Snapshots pour restauration rapide

4. **Isolation**
   - Environnements sÃ©parÃ©s (production, staging, dÃ©veloppement)
   - SÃ©curitÃ© renforcÃ©e (isolation rÃ©seau et stockage)
   - Pas d'impact entre VMs en cas de problÃ¨me

### Comparaison : Bare Metal vs VM vs Conteneurs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BARE METAL                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PostgreSQL 18                          â”‚    â”‚
â”‚  â”‚              OS (Ubuntu 24.04)                      â”‚    â”‚
â”‚  â”‚              Hardware (CPU, RAM, Disque)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  Performance: â­â­â­â­â­ | FlexibilitÃ©: â­â­               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  VIRTUAL MACHINE                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚PostgreSQLâ”‚  â”‚PostgreSQLâ”‚  â”‚PostgreSQLâ”‚                   â”‚
â”‚  â”‚OS (Guest)â”‚  â”‚OS (Guest)â”‚  â”‚OS (Guest)â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hyperviseur (ESXi, KVM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚              Hardware (CPU, RAM, Disque)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  Performance: â­â­â­â­ | FlexibilitÃ©: â­â­â­â­â­           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTENEURS                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚PG 18â”‚  â”‚PG 18â”‚  â”‚PG 18â”‚  â”‚PG 18â”‚  â”‚PG 18â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ Runtime Conteneur (Docker) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚              OS Host (Linux)                  â”‚          â”‚
â”‚  â”‚              Hardware (CPU, RAM, Disque)      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  Performance: â­â­â­â­ | FlexibilitÃ©: â­â­â­â­â­â­         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| CritÃ¨re | Bare Metal | VM | Conteneurs |
|---------|-----------|-----|-----------|
| **Overhead** | 0% | 5-10% | 2-5% |
| **DÃ©marrage** | Minutes | 30-60s | <5s |
| **DensitÃ©** | 1 serveur | 10-20 par hÃ´te | 50-100+ par hÃ´te |
| **Isolation** | Totale | Forte | Moyenne |
| **PortabilitÃ©** | Faible | Moyenne | Excellente |

### Overhead de Virtualisation : Comprendre l'Impact

**Qu'est-ce que l'overhead ?**

L'overhead (surcharge) reprÃ©sente les ressources consommÃ©es par la couche de virtualisation elle-mÃªme. C'est le "coÃ»t" de la flexibilitÃ©.

**DÃ©composition de l'overhead typique :**

```
Serveur physique : 100% de ressources disponibles
â”œâ”€ Hyperviseur : 5-8% (gestion VMs, ordonnancement)
â”œâ”€ Virtualisation CPU : 2-3% (instructions CPU virtualisÃ©es)
â”œâ”€ Virtualisation I/O : 3-5% (disque et rÃ©seau)
â””â”€ Disponible pour PostgreSQL : 85-90%
```

**En pratique pour PostgreSQL :**
- **CPU :** 2-5% d'overhead (nÃ©gligeable avec technologies modernes)
- **RAM :** 0-2% (trÃ¨s faible impact)
- **I/O disque :** 5-15% (le plus impactant, amÃ©lioration avec paravirtualisation)
- **RÃ©seau :** 2-5% (nÃ©gligeable avec SR-IOV)

**Conclusion :** Sur hardware moderne avec hyperviseur optimisÃ©, l'overhead est < 10% pour PostgreSQL, ce qui est un excellent compromis pour la flexibilitÃ© obtenue.

---

## Technologies de Virtualisation (Hyperviseurs)

### Type 1 : Hyperviseurs Bare-Metal (RecommandÃ©s pour Production)

Les hyperviseurs de **Type 1** s'installent directement sur le matÃ©riel, sans systÃ¨me d'exploitation intermÃ©diaire.

#### VMware vSphere / ESXi

**PrÃ©sentation :**
- Leader du marchÃ© enterprise
- Hyperviseur propriÃ©taire de VMware
- Ã‰cosystÃ¨me riche (vCenter, vMotion, DRS)

**Avantages :**
- âœ… Performances excellentes (overhead < 5%)
- âœ… Outils d'administration avancÃ©s
- âœ… Haute disponibilitÃ© native (HA, FT)
- âœ… Live Migration trÃ¨s mature (vMotion)
- âœ… Support commercial 24/7

**InconvÃ©nients :**
- âŒ CoÃ»t Ã©levÃ© (licences par CPU)
- âŒ Vendor lock-in
- âŒ ComplexitÃ© de configuration

**Cas d'usage PostgreSQL :**
- Grandes entreprises avec infrastructure VMware existante
- Besoin de support commercial garanti
- Environnements multi-VMs complexes (>50 VMs)

**Configuration typique pour PostgreSQL :**
```
VM PostgreSQL sur ESXi
â”œâ”€ vCPU : 16 (ratio 1:1 avec cÅ“urs physiques)
â”œâ”€ RAM : 64 GB (rÃ©servation mÃ©moire activÃ©e)
â”œâ”€ Disque : vSAN ou SAN dÃ©diÃ© (datastore performant)
â”œâ”€ RÃ©seau : vmxnet3 (driver paravirtualisÃ©)
â””â”€ Features : Hot-add CPU/RAM, Snapshots, vMotion
```

**Prix indicatif :** 1 000 - 3 000 â‚¬ par CPU + support annuel 20%

---

#### Proxmox VE (RecommandÃ© Open Source)

**PrÃ©sentation :**
- Solution open source basÃ©e sur KVM/QEMU
- Interface web intuitive
- IntÃ©gration conteneurs (LXC) et VMs
- Distribution Debian-based

**Avantages :**
- âœ… **Gratuit** et open source (support payant optionnel)
- âœ… Interface moderne et intuitive
- âœ… Performances comparables Ã  ESXi
- âœ… CommunautÃ© active
- âœ… Live Migration incluse
- âœ… Backup intÃ©grÃ© (Proxmox Backup Server)

**InconvÃ©nients :**
- âŒ Support commercial optionnel (payant)
- âŒ Ã‰cosystÃ¨me moins mature que VMware
- âŒ Documentation parfois dispersÃ©e

**Cas d'usage PostgreSQL :**
- PME cherchant solution professionnelle Ã  coÃ»t maÃ®trisÃ©
- Ã‰quipes techniques maÃ®trisant Linux
- Infrastructure jusqu'Ã  10-20 hÃ´tes
- **â­ Notre recommandation pour dÃ©marrer**

**Configuration typique pour PostgreSQL :**
```
VM PostgreSQL sur Proxmox
â”œâ”€ vCPU : 16 (type host, pas de overcommit)
â”œâ”€ RAM : 64 GB (ballooning dÃ©sactivÃ©)
â”œâ”€ Disque :
â”‚   â”œâ”€ OS : 50 GB (stockage local SSD)
â”‚   â”œâ”€ PGDATA : 500 GB (Ceph ou ZFS)
â”‚   â””â”€ WAL : 100 GB (SSD dÃ©diÃ© si possible)
â”œâ”€ RÃ©seau : VirtIO (paravirtualisÃ©)
â””â”€ Features : HA, Backup automatisÃ©, Snapshots
```

**Prix indicatif :** 0 â‚¬ (Community) ou 90-800 â‚¬/an (support commercial)

---

#### KVM (Kernel-based Virtual Machine)

**PrÃ©sentation :**
- Technologie de virtualisation intÃ©grÃ©e au kernel Linux
- Base de nombreux clouds (AWS, Google Cloud)
- Pas d'interface graphique native (utilise libvirt + outils)

**Avantages :**
- âœ… Open source et gratuit
- âœ… Performances natives (partie intÃ©grante du kernel)
- âœ… Flexible et hautement personnalisable
- âœ… UtilisÃ© par les grands clouds

**InconvÃ©nients :**
- âŒ Courbe d'apprentissage Ã©levÃ©e
- âŒ Pas d'interface graphique par dÃ©faut
- âŒ NÃ©cessite expertise Linux avancÃ©e

**Cas d'usage PostgreSQL :**
- Ã‰quipes DevOps/SRE avec forte expertise Linux
- Infrastructure as Code (Terraform, Ansible)
- Clouds privÃ©s (OpenStack)

**Gestion avec outils :**
- **virsh** : Ligne de commande
- **virt-manager** : Interface graphique simple
- **oVirt** : Interface web (comme Proxmox)
- **Cockpit** : Administration web moderne

**Configuration typique pour PostgreSQL :**
```bash
# CrÃ©ation VM avec virt-install
virt-install \
  --name postgresql-prod \
  --ram 65536 \
  --vcpus 16,maxvcpus=32 \
  --cpu host-passthrough \
  --disk path=/var/lib/libvirt/images/pg-prod.qcow2,size=500,bus=virtio \
  --network bridge=br0,model=virtio \
  --os-variant ubuntu24.04
```

**Prix indicatif :** 0 â‚¬ (gratuit)

---

#### Microsoft Hyper-V

**PrÃ©sentation :**
- Hyperviseur Microsoft intÃ©grÃ© Ã  Windows Server
- Natif pour Ã©cosystÃ¨me Windows

**Avantages :**
- âœ… IntÃ©grÃ© Ã  Windows Server (pas de coÃ»t additionnel)
- âœ… Bonne intÃ©gration Active Directory
- âœ… Performances correctes

**InconvÃ©nients :**
- âŒ Moins performant que VMware/KVM pour Linux
- âŒ Moins d'outils pour gestion PostgreSQL
- âŒ Ã‰cosystÃ¨me Linux moins mature

**Cas d'usage PostgreSQL :**
- Infrastructure 100% Microsoft
- Ã‰quipes Windows sans expertise Linux
- Applications mixtes Windows/Linux

**Recommandation :** PrÃ©fÃ©rer VMware ou Proxmox pour PostgreSQL Linux.

**Prix indicatif :** Inclus dans Windows Server (1 000-6 000 â‚¬ selon Ã©dition)

---

### Type 2 : Hyperviseurs Applicatifs (Pour DÃ©veloppement)

Les hyperviseurs de **Type 2** s'exÃ©cutent comme application sur un OS existant.

#### VirtualBox (Oracle)

**Avantages :**
- âœ… Gratuit (usage personnel)
- âœ… Multi-plateforme (Windows, Mac, Linux)
- âœ… Simple d'utilisation

**InconvÃ©nients :**
- âŒ Performances limitÃ©es
- âŒ **Non adaptÃ© production**
- âŒ Overhead Ã©levÃ© (15-20%)

**Usage PostgreSQL :** DÃ©veloppement local, tests, formation uniquement.

#### VMware Workstation / Fusion

**Avantages :**
- âœ… Performances supÃ©rieures Ã  VirtualBox
- âœ… Bonnes fonctionnalitÃ©s (snapshots, clones)

**InconvÃ©nients :**
- âŒ Payant (environ 200 â‚¬)
- âŒ **Non adaptÃ© production**

**Usage PostgreSQL :** DÃ©veloppement local professionnel.

---

## Architecture MatÃ©rielle pour VMs PostgreSQL

### Dimensionnement du Serveur HÃ´te

#### RÃ¨gles de Consolidation

**Ratio de consolidation :**
Le nombre de VMs que vous pouvez hÃ©berger dÃ©pend de leur taille et charge.

**Exemple concret :**

```
Serveur physique :
â”œâ”€ CPU : 2Ã— AMD EPYC 7543 = 64 cÅ“urs / 128 threads
â”œâ”€ RAM : 512 GB
â”œâ”€ Disque : 8Ã— NVMe 2TB en RAID 10 = 8 TB utile
â””â”€ RÃ©seau : 2Ã— 25 GbE

Peut hÃ©berger :
â”œâ”€ Option 1 (OLTP) : 8 VMs PostgreSQL
â”‚   â””â”€ 8 vCPU, 64 GB RAM, 500 GB disque chacune
â”œâ”€ Option 2 (Mixte) : 4 VMs grosses + 8 VMs petites
â”‚   â”œâ”€ 4Ã— (16 vCPU, 96 GB, 1 TB)
â”‚   â””â”€ 8Ã— (4 vCPU, 16 GB, 200 GB)
â””â”€ Ressources rÃ©servÃ©es hyperviseur : 10%
```

#### RÃ¨gles d'Or du Dimensionnement

**1. CPU : Ã‰viter l'overcommit pour PostgreSQL**

```
Overcommit ratio = vCPU totaux allouÃ©s / CÅ“urs physiques

âœ… Ratio 1:1 (recommandÃ© OLTP)
   â””â”€ 64 cÅ“urs physiques = max 64 vCPU allouÃ©es total

âœ… Ratio 2:1 (acceptable mixte)
   â””â”€ 64 cÅ“urs physiques = max 128 vCPU allouÃ©es

âš ï¸ Ratio 4:1 (risquÃ© pour PostgreSQL)
   â””â”€ Contentions CPU possibles

âŒ Ratio >4:1 (Ã  Ã©viter)
   â””â”€ Performances dÃ©gradÃ©es garanties
```

**Pourquoi ?** PostgreSQL est sensible au CPU. Des contentions ralentissent significativement les requÃªtes.

**2. RAM : Jamais d'overcommit**

```
âŒ INTERDIT : Allouer plus de RAM virtuelle que RAM physique

Exemple :
Serveur avec 512 GB RAM
â”œâ”€ Hyperviseur : 25 GB rÃ©servÃ©s
â”œâ”€ Disponible VMs : 487 GB maximum
â””â”€ âš ï¸ Ne JAMAIS dÃ©passer cette limite

ConsÃ©quence overcommit RAM :
â””â”€ Swap utilisÃ© â†’ PostgreSQL ralentit de 100-1000Ã—
```

**Best practice :** RÃ©server 5-10% de RAM pour l'hyperviseur, ne jamais allouer plus que le reste.

**3. Disque : PrivilÃ©gier performance Ã  capacitÃ©**

```
Stockage VM PostgreSQL :
â”œâ”€ Jamais de stockage rÃ©seau lent (NFS, iSCSI standard)
â”œâ”€ PrivilÃ©gier :
â”‚   â”œâ”€ Stockage local NVMe (meilleur)
â”‚   â”œâ”€ SAN Flash haute performance
â”‚   â”œâ”€ vSAN VMware (bon compromis)
â”‚   â””â”€ Ceph avec rÃ©seaux dÃ©diÃ©s (open source)
â””â”€ Ã‰viter :
    â”œâ”€ NAS entry-level
    â”œâ”€ Stockage distant avec latence >2ms
    â””â”€ iSCSI sur rÃ©seau partagÃ©
```

---

### Configuration CPU Optimale

#### Types de vCPU

**1. CPU Passthrough (RecommandÃ© Production)**

```yaml
Configuration Proxmox :
  cpu: host
  # Expose toutes les instructions CPU natives Ã  la VM
  # Performances optimales, pas d'Ã©mulation
```

```xml
Configuration KVM/libvirt :
  <cpu mode='host-passthrough'>
    <topology sockets='2' cores='8' threads='1'/>
  </cpu>
```

**Avantages :**
- Performance maximale (overhead < 2%)
- Support instructions modernes (AVX-512, etc.)
- PostgreSQL voit le CPU rÃ©el

**InconvÃ©nient :**
- Migration limitÃ©e entre CPU diffÃ©rents

**2. CPU Ã‰mulÃ© (Compatible Migration)**

```yaml
Configuration Proxmox :
  cpu: x86-64-v3  # ou kvm64
  # CPU gÃ©nÃ©rique pour compatibilitÃ© migration
```

**Avantages :**
- Live Migration entre hÃ´tes diffÃ©rents
- CompatibilitÃ© maximale

**InconvÃ©nient :**
- Overhead 3-5%
- Instructions modernes dÃ©sactivÃ©es

**Recommandation PostgreSQL :**
- **Production critique :** host-passthrough
- **Environnement multi-hÃ´tes hÃ©tÃ©rogÃ¨nes :** CPU Ã©mulÃ© compatible

#### Topologie CPU : Sockets vs Cores

**Exemple :**
Une VM avec 16 vCPU peut Ãªtre configurÃ©e ainsi :

```
Option 1 : 1 socket Ã— 16 cÅ“urs
Option 2 : 2 sockets Ã— 8 cÅ“urs
Option 3 : 4 sockets Ã— 4 cÅ“urs
```

**Impact sur PostgreSQL :**

PostgreSQL n'est pas NUMA-aware par dÃ©faut. PrivilÃ©giez :

```yaml
RecommandÃ© :
  sockets: 1
  cores: 16
  threads: 1  # DÃ©sactiver hyperthreading si possible
```

**Pourquoi ?**
- Ã‰vite la complexitÃ© NUMA (Non-Uniform Memory Access)
- Meilleure localitÃ© mÃ©moire
- Plus simple Ã  gÃ©rer

**Exception :** Si >32 vCPU, suivez la topologie de l'hÃ´te physique.

#### CPU Pinning (Ã‰pinglage)

**Qu'est-ce que le CPU pinning ?**

Lier les vCPU d'une VM Ã  des cÅ“urs physiques spÃ©cifiques pour Ã©viter la migration entre cÅ“urs.

```yaml
Configuration Proxmox (affinity) :
  taskset: 0-15  # vCPU liÃ©s aux cÅ“urs physiques 0 Ã  15
```

**Avantages :**
- Performances prÃ©dictibles
- Pas de contention entre VMs
- Meilleur pour benchmark

**InconvÃ©nients :**
- Perte de flexibilitÃ©
- Moins de consolidation possible
- Gestion complexe

**Quand utiliser pour PostgreSQL ?**
- âœ… VM critique unique sur hÃ´te
- âœ… Besoins de performances maximales et constantes
- âŒ Environnement avec >5 VMs par hÃ´te

---

### Configuration RAM Optimale

#### RÃ©servation MÃ©moire (Memory Reservation)

**Concept :** Garantir qu'une quantitÃ© de RAM physique soit toujours disponible pour la VM.

```yaml
VM PostgreSQL Production :
  memory: 64 GB
  min_memory: 64 GB  # RÃ©servation 100%
  # âš ï¸ DÃ©sactive ballooning et swap
```

**Pourquoi c'est critique pour PostgreSQL ?**

PostgreSQL gÃ¨re sa propre mÃ©moire (shared_buffers, work_mem). Si la RAM physique n'est pas garantie :
1. Hyperviseur utilise ballooning ou swap
2. Performances PostgreSQL s'effondrent (Ã—10-1000 plus lent)
3. Timeouts applicatifs en cascade

**RÃ¨gle d'or :** Pour PostgreSQL production, **toujours rÃ©server 100% de la RAM**.

#### Ballooning : Ã€ DÃ©sactiver !

**Qu'est-ce que le ballooning ?**

MÃ©canisme permettant Ã  l'hyperviseur de "reprendre" de la RAM aux VMs si nÃ©cessaire.

```yaml
# VMware
sched.mem.maxmemctl: 0  # DÃ©sactive ballooning

# Proxmox / KVM
balloon: 0  # DÃ©sactive ballooning
```

**Pourquoi dÃ©sactiver pour PostgreSQL ?**
- PostgreSQL utilise toute sa RAM allouÃ©e
- Le ballooning cause des ralentissements imprÃ©visibles
- Incompatible avec performances stables

#### Huge Pages (Pages MÃ©moire GÃ©antes)

**Qu'est-ce que les Huge Pages ?**

Pages mÃ©moire de 2 MB au lieu de 4 KB standard. RÃ©duit le nombre d'entrÃ©es TLB (Translation Lookaside Buffer).

**Configuration dans la VM :**

```bash
# /etc/sysctl.conf dans la VM
vm.nr_hugepages = 33792  # Pour 64 GB (64*1024/2)

# PostgreSQL postgresql.conf
huge_pages = on
```

**Gains pour PostgreSQL :**
- RÃ©duction overhead TLB : 2-5% de performance CPU
- MÃ©moire non swappable
- RecommandÃ© pour VMs >32 GB RAM

---

### Configuration Stockage pour VMs

#### Types de Disques Virtuels

**1. Thin Provisioning vs Thick Provisioning**

```
Thick Provisioning (RecommandÃ© PostgreSQL) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Disque VM : 500 GB allouÃ©s             â”‚
â”‚ â†“                                      â”‚
â”‚ Datastore : 500 GB rÃ©servÃ©s            â”‚ â† Espace garanti
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Avantage : Performance stable, pas de surprise

Thin Provisioning (Ã€ Ã©viter production) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Disque VM : 500 GB virtuels            â”‚
â”‚ â†“                                      â”‚
â”‚ Datastore : 50 GB rÃ©ellement utilisÃ©s  â”‚ â† Grandit au besoin
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Risque : Saturation datastore inattendue, performances variables
```

**Recommandation PostgreSQL :** **Thick Provisioning Eager Zeroed** (VMware) ou Ã©quivalent.

**2. Format de Disque Virtuel**

| Format | Description | Performance | Usage |
|--------|-------------|-------------|--------|
| **Raw** | Fichier brut 1:1 | â­â­â­â­â­ | Production |
| **qcow2** | QEMU Copy-On-Write | â­â­â­â­ | Snapshots, dev |
| **VMDK** | VMware natif | â­â­â­â­â­ | VMware |
| **VHD/VHDX** | Hyper-V | â­â­â­â­ | Hyper-V |

**Recommandation :**
- **Production :** Raw (KVM/Proxmox) ou VMDK (VMware)
- **DÃ©veloppement :** qcow2 pour snapshots faciles

#### Drivers de Disque ParavirtualisÃ©s

**Qu'est-ce que la paravirtualisation ?**

Le guest OS "sait" qu'il est virtualisÃ© et utilise des drivers optimisÃ©s au lieu d'Ã©muler du matÃ©riel.

```
Ã‰mulation classique (lent) :
VM â†’ Ã‰mulation SATA â†’ Hyperviseur â†’ Disque physique
Overhead : 10-20%

Paravirtualisation (rapide) :
VM â†’ Driver VirtIO â†’ Hyperviseur â†’ Disque physique
Overhead : 2-5%
```

**Configuration recommandÃ©e :**

```yaml
# Proxmox / KVM
disk:
  bus: virtio
  driver: virtio-blk
  cache: none      # DÃ©sactive cache pour garantir fsync
  aio: native      # I/O asynchrone natif
  discard: on      # Support TRIM/DISCARD

# VMware
scsi_controller: pvscsi  # Paravirtual SCSI
disk_type: thin  # ou thick eager zeroed
```

#### Cache Policies (Politiques de Cache)

**Options de cache :**

| Mode | Description | Usage PostgreSQL |
|------|-------------|------------------|
| **none** | Pas de cache, I/O direct | âœ… **Production (recommandÃ©)** |
| **writethrough** | Cache lecture seulement | âœ… Acceptable |
| **writeback** | Cache lecture/Ã©criture | âŒ **Dangereux** (perte donnÃ©es) |
| **directsync** | Sync total | âš ï¸ TrÃ¨s lent |

**Pourquoi `cache=none` ?**

PostgreSQL gÃ¨re son propre cache (shared_buffers) et utilise fsync() pour garantir la durabilitÃ©. Un cache hyperviseur intermÃ©diaire :
- Risque de perte de donnÃ©es si crash
- Double cache inefficace
- Masque les vraies performances I/O

**Configuration recommandÃ©e :**

```bash
# Proxmox
qm set 100 --scsi0 local-lvm:vm-100-disk-0,cache=none,aio=native
```

#### SÃ©paration des Disques Virtuels

**Architecture recommandÃ©e :**

```
VM PostgreSQL
â”œâ”€ vDisk 1 (50 GB) : OS (/boot, /)
â”‚   â””â”€ Stockage : Datastore standard
â”œâ”€ vDisk 2 (500 GB) : PGDATA (/var/lib/postgresql)
â”‚   â””â”€ Stockage : Flash/NVMe haute performance
â”œâ”€ vDisk 3 (100 GB) : WAL (/pg_wal)
â”‚   â””â”€ Stockage : Flash/NVMe ultra rapide
â””â”€ vDisk 4 (1 TB) : Backups (/backup)
    â””â”€ Stockage : Capacity (SATA)
```

**Avantages :**
- Optimisation coÃ»t/performance par usage
- Isolation des I/O (WAL sÃ©parÃ© amÃ©liore performances)
- Snapshots sÃ©lectifs (snapshot OS sans data)

---

### Configuration RÃ©seau pour VMs

#### Types d'Interfaces RÃ©seau Virtuelles

**1. Ã‰mulation (e1000, rtl8139) - Ã€ Ã‰viter**

```
Performance : 100-200 MB/s
Overhead : 15-20%
Usage : CompatibilitÃ© legacy uniquement
```

**2. VirtIO (ParavirtualisÃ©) - RecommandÃ©**

```yaml
# Proxmox / KVM
network:
  model: virtio
  bridge: vmbr0
  mtu: 9000  # Jumbo frames

Performance : 1-10 GB/s
Overhead : 2-5%
Usage : Production Linux
```

**3. vmxnet3 (VMware ParavirtualisÃ©) - RecommandÃ© VMware**

```
Performance : 1-10 GB/s
Overhead : 2-5%
Usage : Production VMware
```

**4. SR-IOV (Single Root I/O Virtualization) - Performance Maximale**

```
Performance : 10-25 GB/s (quasi-native)
Overhead : <1%
ComplexitÃ© : Ã‰levÃ©e (nÃ©cessite carte rÃ©seau compatible)
Usage : TrÃ¨s haute performance, latence critique
```

**Recommandation PostgreSQL :**
- **Standard :** VirtIO (KVM) ou vmxnet3 (VMware)
- **Haute performance :** SR-IOV si infrastructure le permet

#### Segmentation RÃ©seau

**Architecture multi-rÃ©seaux :**

```
VM PostgreSQL Production
â”œâ”€ vNIC 1 (eth0) : RÃ©seau production (VLAN 100)
â”‚   â””â”€ Connexions clients applicatifs
â”œâ”€ vNIC 2 (eth1) : RÃ©seau rÃ©plication (VLAN 200)
â”‚   â””â”€ Streaming replication vers standby
â”œâ”€ vNIC 3 (eth2) : RÃ©seau backup (VLAN 300)
â”‚   â””â”€ pg_basebackup, transferts sauvegardes
â””â”€ vNIC 4 (eth3) : RÃ©seau admin (VLAN 900)
    â””â”€ SSH, monitoring, gestion
```

**Avantages :**
- Isolation trafic (pas d'impact backup sur production)
- SÃ©curitÃ© renforcÃ©e (firewall par VLAN)
- QoS (Quality of Service) par rÃ©seau
- Troubleshooting simplifiÃ©

#### Jumbo Frames

**Activer sur rÃ©seau 10 GbE+ :**

```bash
# Dans la VM
ip link set eth0 mtu 9000

# Persistant (Ubuntu)
# /etc/netplan/01-netcfg.yaml
network:
  ethernets:
    eth0:
      mtu: 9000
```

**Gain :** 10-20% de dÃ©bit rÃ©seau, moins d'interruptions CPU.

**âš ï¸ Important :** Tous les Ã©quipements du chemin rÃ©seau doivent supporter MTU 9000.

---

## Configuration PostgreSQL pour Environnement VirtualisÃ©

### Adaptations SpÃ©cifiques aux VMs

#### 1. Configuration MÃ©moire

```ini
# postgresql.conf pour VM 64 GB RAM

# Shared Buffers : LÃ©gÃ¨rement rÃ©duit vs bare metal
shared_buffers = 16GB  # 25% de RAM (vs 30-40% bare metal)
# Raison : Hyperviseur + guest OS consomment RAM

# Work Memory
work_mem = 64MB
# Identique bare metal

# Maintenance Work Memory
maintenance_work_mem = 2GB
# Identique bare metal

# Effective Cache Size : Tenir compte overhead VM
effective_cache_size = 48GB  # 75% de RAM VM (vs 85% bare metal)
# Le guest OS + buffers hyperviseur rÃ©duisent cache disponible

# Huge Pages : Toujours recommandÃ© en VM
huge_pages = on
# Configuration hÃ´te ET guest nÃ©cessaire
```

**Calcul simplifiÃ© :**

```
VM avec 64 GB RAM allouÃ©e :
â”œâ”€ Guest OS : 2-3 GB
â”œâ”€ Overhead hyperviseur : 0.5 GB (transparent pour guest)
â”œâ”€ Disponible PostgreSQL : ~61 GB
â””â”€ Configuration :
    â”œâ”€ shared_buffers : 16 GB (25%)
    â”œâ”€ effective_cache_size : 48 GB (75%)
    â””â”€ Marge pour work_mem Ã— connexions : ~45 GB
```

#### 2. Configuration I/O

```ini
# WAL et Checkpoints
wal_level = replica
max_wal_size = 16GB
min_wal_size = 4GB
wal_compression = zstd  # PostgreSQL 18
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9

# I/O Asynchrone (PostgreSQL 18) - AdaptÃ© VM
io_method = 'async'  # BÃ©nÃ©ficie mÃªme en VM
io_async_workers = 8  # Adapter au nombre vCPU
io_async_queue_depth = 128  # RÃ©duit vs bare metal (256)

# Raison : L'hyperviseur ajoute sa propre couche I/O asynchrone
# Ã‰viter de saturer la queue
```

#### 3. ParamÃ¨tres SpÃ©cifiques Virtualisation

```ini
# Logging Ã©tendu pour troubleshooting
log_line_prefix = '%t [%p-%l] %q%u@%d VM=%h '
log_lock_waits = on
log_temp_files = 10MB  # Log fichiers temporaires > 10 MB

# Statistiques
shared_preload_libraries = 'pg_stat_statements'
track_io_timing = on  # Important : dÃ©tecte latences I/O anormales en VM

# Autovacuum adaptÃ©
autovacuum_max_workers = 4  # RÃ©duit vs bare metal
autovacuum_vacuum_cost_delay = 5ms  # AugmentÃ© (moins agressif)
# Raison : En VM, autovacuum trop agressif peut impacter VMs voisines
```

---

## Snapshots et Gestion des VMs

### Snapshots : Usage et PrÃ©cautions

#### Qu'est-ce qu'un Snapshot ?

Un **snapshot** (instantanÃ©) capture l'Ã©tat complet d'une VM Ã  un instant T :
- Disques virtuels
- RAM (optionnel)
- Configuration

**Analogie :** C'est comme une photo de votre ordinateur. Vous pouvez "revenir dans le temps" Ã  cette photo.

#### Snapshots avec PostgreSQL : Bonnes Pratiques

**âš ï¸ Attention :** Les snapshots ne sont PAS des backups PostgreSQL valides !

**ProblÃ¨me :**

```
Snapshot classique (dangereux) :
14h00 : Snapshot pris
  â”œâ”€ Fichiers disque figÃ©s
  â”œâ”€ Transactions en cours NON cohÃ©rentes
  â””â”€ WAL potentiellement dÃ©synchronisÃ©

Restauration snapshot â†’ Base potentiellement corrompue
```

**Solution 1 : Snapshot avec arrÃªt PostgreSQL**

```bash
# ArrÃªt propre
sudo systemctl stop postgresql

# Prise snapshot (Proxmox)
qm snapshot 100 before-upgrade

# RedÃ©marrage
sudo systemctl start postgresql
```

- âœ… **Avantages :** Snapshot cohÃ©rent, rapide (downtime 1-2 min)
- âŒ **InconvÃ©nient :** Interruption de service

**Solution 2 : Snapshot avec pg_start_backup() (ObsolÃ¨te PG 15+)**

```sql
-- Avant PostgreSQL 15
SELECT pg_start_backup('snapshot-label', false, false);
-- Prendre snapshot hyperviseur
SELECT pg_stop_backup();
```

âš ï¸ **DÃ©prÃ©ciÃ©** : RemplacÃ© par `pg_backup_start()` mais complexe en VM.

**Solution 3 : Snapshot + WAL archiving (RecommandÃ©)**

```ini
# postgresql.conf
archive_mode = on
archive_command = 'test ! -f /mnt/wal_archive/%f && cp %p /mnt/wal_archive/%f'
```

```bash
# Snapshot VM
qm snapshot 100 pitr-base

# Restauration PITR possible depuis snapshot + WAL
```

- âœ… **Avantage :** Point-In-Time Recovery possible
- âŒ **ComplexitÃ© :** Configuration avancÃ©e

**Recommandation gÃ©nÃ©rale :**

Pour PostgreSQL, privilÃ©gier :
1. **pg_basebackup** pour backups (pas snapshots)
2. **Snapshots** uniquement pour :
   - Avant upgrade OS/PostgreSQL (avec arrÃªt propre)
   - Tests/dev (cohÃ©rence non critique)
   - Infrastructure as Code (VM template)

### Clonage de VMs PostgreSQL

**Use cases :**

1. **Environnement de dÃ©veloppement :**
   ```bash
   # Proxmox : Cloner VM production vers dev
   qm clone 100 200 --name postgresql-dev
   ```

2. **Tests de migration :**
   ```bash
   # Test upgrade PostgreSQL 17 â†’ 18 sur clone
   qm clone 100 150 --name pg18-test
   ```

**âš ï¸ PrÃ©cautions aprÃ¨s clonage :**

```bash
# 1. Changer nom d'hÃ´te
sudo hostnamectl set-hostname postgresql-dev

# 2. Modifier configuration rÃ©seau (nouvelle IP)

# 3. RÃ©gÃ©nÃ©rer identifiants systÃ¨me
sudo rm -f /var/lib/dbus/machine-id
sudo dbus-uuidgen --ensure

# 4. PostgreSQL : Modifier cluster_name
# postgresql.conf
cluster_name = 'postgresql-dev'
```

---

## Haute DisponibilitÃ© avec VMs

### Live Migration (vMotion / Migration Ã  Chaud)

**Qu'est-ce que la Live Migration ?**

DÃ©placer une VM d'un hÃ´te physique Ã  un autre **sans interruption de service**.

```
Avant Migration :
Host-1 [VM-PostgreSQL] â”€â”€â”
Host-2 [Vide]          â”€â”€â”˜

Pendant Migration (invisible pour clients) :
Host-1 [VM-PostgreSQL] â”€â”€â”
          â†“              â”‚ Ã‰tat transfÃ©rÃ©
Host-2 [VM-PostgreSQL] â”€â”€â”˜

AprÃ¨s Migration :
Host-1 [Vide]          â”€â”€â”
Host-2 [VM-PostgreSQL] â”€â”€â”˜
```

**Technologies :**
- **VMware :** vMotion
- **Proxmox / KVM :** Live Migration
- **Hyper-V :** Live Migration

**DurÃ©e typique :** 10-30 secondes (downtime < 1 seconde perÃ§u)

**Cas d'usage PostgreSQL :**

1. **Maintenance sans interruption :**
   - Mise Ã  jour firmware hÃ´te
   - Changement matÃ©riel
   - RÃ©partition de charge

2. **Optimisation ressources :**
   - DRS (Distributed Resource Scheduler) VMware
   - DÃ©placer VMs vers hÃ´tes moins chargÃ©s

**PrÃ©requis pour Live Migration :**

```
âœ… Stockage partagÃ© (SAN, vSAN, Ceph)
âœ… RÃ©seau rapide entre hÃ´tes (10 GbE minimum)
âœ… CPU compatibles (mÃªme famille, ou Ã©mulation)
âœ… Configuration rÃ©seau identique
```

**Impact sur PostgreSQL :**

```
Pendant migration :
â”œâ”€ Connexions maintenues (du point de vue client)
â”œâ”€ LÃ©gÃ¨re augmentation latence (~50-100ms pendant 1-2s)
â”œâ”€ Aucune perte de transaction
â””â”€ Logs PostgreSQL : RAS (transparent)

Surveillance :
â””â”€ pg_stat_activity montre augmentation momentanÃ©e wait_event
```

**Limitation :** Ne remplace PAS la rÃ©plication PostgreSQL native pour la HA applicative !

---

### HA au Niveau Hyperviseur

#### VMware HA (High Availability)

**Fonctionnement :**

```
Cluster VMware (3 hÃ´tes minimum) :
â”œâ”€ Host-1 : VM-PostgreSQL-Primary
â”œâ”€ Host-2 : Standby
â”œâ”€ Host-3 : Standby

Si Host-1 crash :
â””â”€ vSphere HA redÃ©marre automatiquement VM-PostgreSQL sur Host-2
   â””â”€ Downtime : 2-5 minutes (reboot VM)
```

**Configuration :**

```yaml
HA Settings :
  admission_control: enabled  # RÃ©serve ressources
  restart_priority: highest   # PostgreSQL prioritaire
  host_isolation_response: power_off  # VM Ã©teinte si hÃ´te isolÃ©
  monitoring_sensitivity: high
```

**Avantages :**
- âœ… Automatique
- âœ… Pas de configuration PostgreSQL spÃ©cifique

**Limites :**
- âš ï¸ Downtime 2-5 minutes (reboot)
- âš ï¸ Perte transactions en vol (non-committed)
- âš ï¸ NÃ©cessite IP virtuelle ou load balancer

#### Proxmox HA

**Fonctionnement similaire VMware :**

```bash
# Activer HA sur VM PostgreSQL
ha-manager add vm:100 --state started --max_restart 3 --max_relocate 3
```

**Configuration Fencing (STONITH) :**

```bash
# Proxmox nÃ©cessite quorum (3 nÅ“uds minimum)
pvecm status  # VÃ©rifier cluster

# Watchdog pour dÃ©tection panne
echo "softdog" >> /etc/modules
systemctl enable watchdog-mux.service
```

---

### Combinaison HA Hyperviseur + RÃ©plication PostgreSQL

**Architecture optimale :**

```
Datacenter Principal
â”œâ”€ Host-1 : VM-PostgreSQL-Primary (production)
â”‚   â””â”€ Protected by HA Cluster
â”œâ”€ Host-2 : VM-PostgreSQL-Standby-1 (local)
â”‚   â””â”€ Streaming Replication async
â””â”€ Host-3 : Standby disponible pour HA

Datacenter Secondaire (DR)
â””â”€ Host-4 : VM-PostgreSQL-Standby-2 (distant)
    â””â”€ Streaming Replication async

Niveaux de protection :
1. HA Hyperviseur : Panne hÃ´te â†’ RedÃ©marrage sur Host-2/3 (2-5 min)
2. RÃ©plication PostgreSQL : Failover standby â†’ Promotion (30s-2min)
3. DR : Disaster â†’ Basculement datacenter secondaire (15-30 min)
```

**Avantages combinÃ©s :**
- HA matÃ©rielle (panne hÃ´te)
- HA logicielle (corruption PostgreSQL)
- Disaster Recovery (incendie, coupure longue)

---

## Solutions de Stockage pour VMs

### Stockage Local vs PartagÃ©

#### Stockage Local

**Configuration :**

```
Host-1
â”œâ”€ NVMe 1-2 : RAID 1 (Hyperviseur)
â””â”€ NVMe 3-8 : RAID 10 (VMs locales)
    â”œâ”€ VM-PostgreSQL : 500 GB
    â”œâ”€ VM-App : 200 GB
    â””â”€ VM-Web : 100 GB
```

**Avantages :**
- âœ… Performance maximale (latence < 100Âµs)
- âœ… SimplicitÃ© (pas de SAN)
- âœ… CoÃ»t rÃ©duit
- âœ… Pas de rÃ©seau stockage

**InconvÃ©nients :**
- âŒ Pas de Live Migration (sauf avec Ceph/DRBD)
- âŒ Snapshots limitÃ©s Ã  l'hÃ´te
- âŒ Perte donnÃ©es si panne hÃ´te (sans rÃ©plication)

**Usage PostgreSQL :**
- VM unique sans besoin mobilitÃ©
- Performance critique (trading, analytics temps rÃ©el)
- Budget limitÃ©

#### SAN (Storage Area Network)

**Architecture :**

```
Cluster Hyperviseur (3 hÃ´tes)
â”œâ”€ Host-1 â”€â”
â”œâ”€ Host-2 â”€â”¼â”€ RÃ©seau Fiber Channel / iSCSI â”€â”€â†’ SAN
â””â”€ Host-3 â”€â”˜                                    â”œâ”€ Baie All-Flash
                                                 â””â”€ LUNs partagÃ©es
```

**Avantages :**
- âœ… Live Migration possible
- âœ… Snapshots centralisÃ©s
- âœ… Haute disponibilitÃ© stockage
- âœ… Gestion centralisÃ©e

**InconvÃ©nients :**
- âŒ CoÃ»t Ã©levÃ© (50 000 - 500 000â‚¬+)
- âŒ ComplexitÃ© (expertise SAN)
- âŒ Latence supÃ©rieure local (+100-500Âµs)
- âŒ Point de contention (bande passante partagÃ©e)

**Types de SAN :**

| Type | Latence | DÃ©bit | ComplexitÃ© | CoÃ»t |
|------|---------|-------|------------|------|
| **Fiber Channel** | <200Âµs | 16-32 Gb/s | Ã‰levÃ©e | â‚¬â‚¬â‚¬â‚¬â‚¬ |
| **iSCSI (10GbE)** | <500Âµs | 10 Gb/s | Moyenne | â‚¬â‚¬â‚¬ |
| **iSCSI (25GbE+)** | <300Âµs | 25-100 Gb/s | Moyenne | â‚¬â‚¬â‚¬â‚¬ |
| **NVMe-oF** | <100Âµs | 25-100 Gb/s | Ã‰levÃ©e | â‚¬â‚¬â‚¬â‚¬â‚¬ |

**Recommandation PostgreSQL :**
- **SAN All-Flash** obligatoire (pas de HDD)
- **Fiber Channel** ou **NVMe-oF** pour haute performance
- **iSCSI 25GbE+** en compromis coÃ»t/perf

#### vSAN (VMware Virtual SAN)

**Concept :**

```
Cluster VMware (4 hÃ´tes) :
Chaque hÃ´te contribue ses disques locaux
â”œâ”€ Host-1 : 4Ã— NVMe â”€â”
â”œâ”€ Host-2 : 4Ã— NVMe â”€â”¼â”€ Pool vSAN distribuÃ©
â”œâ”€ Host-3 : 4Ã— NVMe â”€â”¤  (Software-Defined Storage)
â””â”€ Host-4 : 4Ã— NVMe â”€â”˜

DonnÃ©es rÃ©pliquÃ©es entre hÃ´tes (RAID logiciel)
```

**Avantages :**
- âœ… Pas de SAN externe (Ã©conomie)
- âœ… Performance excellente (local + distribuÃ©)
- âœ… ScalabilitÃ© linÃ©aire (ajout hÃ´tes)
- âœ… Live Migration supportÃ©e

**InconvÃ©nients :**
- âŒ RÃ©seau 10GbE+ obligatoire
- âŒ Licence VMware coÃ»teuse
- âŒ Minimum 3-4 hÃ´tes
- âŒ Overhead CPU/rÃ©seau

**Configuration vSAN pour PostgreSQL :**

```yaml
Storage Policy :
  failures_to_tolerate: 1  # RÃ©plication 2 copies
  stripe_width: 2          # Performance lecture
  object_space_reservation: 100%  # Thick provisioning
  disable_site_read_locality: false  # Read local
```

**Performance typique :**
- Latence : 200-500Âµs
- IOPS : 100 000 - 500 000+
- DÃ©bit : 2-10 GB/s par hÃ´te

#### Ceph (Open Source)

**Architecture :**

```
Cluster Ceph (5 serveurs dÃ©diÃ©s stockage) :
â”œâ”€ Node-1 : 8Ã— NVMe â”€â”
â”œâ”€ Node-2 : 8Ã— NVMe â”€â”¤
â”œâ”€ Node-3 : 8Ã— NVMe â”€â”¼â”€ Pool RBD (images block)
â”œâ”€ Node-4 : 8Ã— NVMe â”€â”¤  RÃ©plication 3Ã—
â””â”€ Node-5 : 8Ã— NVMe â”€â”˜

Cluster Proxmox (VMs) :
â”œâ”€ Host-1 â”€â”
â”œâ”€ Host-2 â”€â”¼â”€ RÃ©seau 25GbE â”€â”€â†’ Cluster Ceph
â””â”€ Host-3 â”€â”˜
```

**Avantages :**
- âœ… Open source et gratuit
- âœ… ScalabilitÃ© massive (pÃ©taoctets)
- âœ… Haute disponibilitÃ© native
- âœ… IntÃ©gration Proxmox native

**InconvÃ©nients :**
- âŒ ComplexitÃ© importante
- âŒ Overhead rÃ©seau significatif
- âŒ Performances variables (dÃ©pend config)
- âŒ NÃ©cessite rÃ©seau dÃ©diÃ© 25GbE+

**Configuration Ceph pour PostgreSQL :**

```bash
# Pool RBD optimisÃ© PostgreSQL
ceph osd pool create postgresql 128 128
ceph osd pool set postgresql size 3  # RÃ©plication 3Ã—
ceph osd pool set postgresql min_size 2  # Minimum 2 copies
ceph osd pool set postgresql pg_autoscale_mode on

# Tuning client
rbd_cache = false  # DÃ©sactiver cache (PostgreSQL gÃ¨re)
```

**Performance typique (cluster bien configurÃ©) :**
- Latence : 500-2000Âµs (3-10Ã— local)
- IOPS : 50 000 - 200 000
- DÃ©bit : 5-20 GB/s (total cluster)

**Recommandation :** Excellent pour infrastructure >10 hÃ´tes, nÃ©cessite expertise.

---

## Best Practices PostgreSQL sur VMs

### Checklist Configuration Optimale

#### Niveau Hyperviseur

**CPU :**
- [ ] Type CPU : host-passthrough (ou Ã©mulÃ© pour migration)
- [ ] Topologie : 1 socket Ã— N cÅ“urs
- [ ] Pas d'overcommit (ratio â‰¤ 2:1)
- [ ] Hyper-Threading dÃ©sactivÃ© si possible
- [ ] CPU pinning pour VMs critiques uniquement

**RAM :**
- [ ] RÃ©servation mÃ©moire : 100%
- [ ] Ballooning : dÃ©sactivÃ©
- [ ] Swap : dÃ©sactivÃ© dans la VM
- [ ] Huge Pages : activÃ© (VMs >16GB)

**Disque :**
- [ ] Type : Thick provisioning
- [ ] Bus : VirtIO (KVM) ou PVSCSI (VMware)
- [ ] Cache : none
- [ ] AIO : native
- [ ] Discard : activÃ© (TRIM support)
- [ ] SÃ©paration : OS / PGDATA / WAL / Backup

**RÃ©seau :**
- [ ] Interface : VirtIO (KVM) ou vmxnet3 (VMware)
- [ ] MTU : 9000 (jumbo frames si 10GbE+)
- [ ] Multi-NICs : sÃ©paration traffic (prod/repli/backup/admin)

#### Niveau OS Guest (dans la VM)

- [ ] OS : Ubuntu Server LTS / Rocky Linux
- [ ] Kernel : ParamÃ¨tres sysctl optimisÃ©s
  ```bash
  vm.swappiness = 1
  vm.dirty_ratio = 10
  vm.dirty_background_ratio = 3
  ```
- [ ] Huge Pages : configurÃ©
  ```bash
  vm.nr_hugepages = <calculÃ© selon RAM>
  ```
- [ ] Limits : nofile et nproc augmentÃ©s
- [ ] Filesystem : ext4 ou XFS avec noatime

#### Niveau PostgreSQL

- [ ] shared_buffers : 25% RAM VM
- [ ] effective_cache_size : 75% RAM VM
- [ ] work_mem : adaptÃ© au nombre connexions
- [ ] maintenance_work_mem : 2-4 GB
- [ ] io_method : 'async' (PostgreSQL 18)
- [ ] huge_pages : on
- [ ] track_io_timing : on (dÃ©tecter latence I/O)
- [ ] log_lock_waits : on
- [ ] pg_stat_statements : activÃ©

#### Monitoring SpÃ©cifique VM

**MÃ©triques hyperviseur Ã  surveiller :**

```sql
-- CPU Ready Time (VMware)
-- % temps oÃ¹ vCPU attend CPU physique
-- Target : < 5%
-- Si > 10% : overcommit CPU ou contention

-- Memory Ballooning
-- Target : 0 MB balloonnÃ©
-- Si > 0 : overcommit RAM â†’ problÃ¨me

-- Disk Latency
-- Target : < 5ms (SSD/Flash)
-- Si > 10ms : problÃ¨me stockage ou rÃ©seau SAN

-- Network Drops
-- Target : 0 packets dropped
-- Si > 0 : saturation rÃ©seau ou configuration
```

**Dans PostgreSQL :**

```sql
-- DÃ©tecter problÃ¨mes I/O (latence anormale en VM)
SELECT
  relname,
  heap_blks_read,
  heap_blks_hit,
  idx_blks_read,
  idx_blks_hit
FROM pg_statio_user_tables
WHERE heap_blks_read > 100000
ORDER BY heap_blks_read DESC;

-- Si ratio blks_hit/blks_read < 99% : cache insuffisant ou I/O lent
```

---

## Troubleshooting : ProblÃ¨mes Courants

### Performance DÃ©gradÃ©e Soudaine

**SymptÃ´me :** PostgreSQL lent sans raison apparente.

**Diagnostics :**

1. **VÃ©rifier overcommit CPU :**
   ```bash
   # Sur l'hÃ´te
   # VMware
   esxtop  # Regarder %RDY (CPU Ready)

   # Proxmox / KVM
   top  # Regarder %steal (temps volÃ© aux VMs)
   ```

   **Solution :** RÃ©duire nombre vCPU ou ajouter hÃ´tes au cluster.

2. **VÃ©rifier ballooning mÃ©moire :**
   ```bash
   # Dans la VM
   free -h  # Regarder utilisation swap
   vmstat 1  # Colonnes si/so (swap in/out)
   ```

   **Solution :** DÃ©sactiver ballooning, rÃ©server mÃ©moire.

3. **VÃ©rifier latence stockage :**
   ```bash
   # Dans la VM
   iostat -x 1
   # Regarder await et svctm
   # Si await > 10ms : problÃ¨me stockage
   ```

   **Solution :** VÃ©rifier saturation SAN ou rÃ©seau stockage.

4. **VÃ©rifier "noisy neighbors" :**
   ```bash
   # Sur l'hÃ´te : identifier VMs consommant trop
   # Proxmox
   pvesh get /cluster/resources --type vm

   # Migrer VMs ou ajouter limites CPU/IO
   ```

### Corruption AprÃ¨s Snapshot

**SymptÃ´me :** Base corrompue aprÃ¨s restauration snapshot.

**Cause :** Snapshot pris pendant Ã©critures actives.

**PrÃ©vention :**
1. Toujours arrÃªter PostgreSQL avant snapshot
2. Ou utiliser pg_basebackup au lieu de snapshots
3. Configurer WAL archiving pour PITR

**RÃ©cupÃ©ration :**
```bash
# Si corruption dÃ©tectÃ©e
pg_resetwal -f /var/lib/postgresql/18/main
# âš ï¸ Perte potentielle de transactions

# Restaurer depuis backup sain
pg_restore -d dbname /backup/last_good_backup.dump
```

### Live Migration Ã‰choue

**Causes frÃ©quentes :**

1. **Stockage non partagÃ© :**
   - Solution : Migrer vers SAN/vSAN/Ceph

2. **CPUs incompatibles :**
   - Solution : Utiliser Ã©mulation CPU au lieu de passthrough

3. **RAM insuffisante sur cible :**
   - Solution : LibÃ©rer RAM sur hÃ´te de destination

4. **RÃ©seau saturÃ© :**
   - Solution : Utiliser rÃ©seau dÃ©diÃ© 10GbE+ pour migrations

---

## Comparaison CoÃ»ts : Bare Metal vs VM

### Exemple Concret : Infrastructure PostgreSQL Production

**ScÃ©nario :** 5 bases PostgreSQL production (4Ã— 32 GB RAM, 1Ã— 128 GB RAM)

#### Option 1 : Bare Metal (5 serveurs)

```
Investissement initial :
â”œâ”€ 4Ã— Serveurs moyens (32 GB RAM, 16 cores, 2Ã— NVMe)
â”‚   â””â”€ 4Ã— 8 000 â‚¬ = 32 000 â‚¬
â”œâ”€ 1Ã— Serveur puissant (128 GB RAM, 32 cores, 4Ã— NVMe)
â”‚   â””â”€ 1Ã— 18 000 â‚¬ = 18 000 â‚¬
â”œâ”€ RÃ©seau (switch 10GbE, cÃ¢bles)
â”‚   â””â”€ 3 000 â‚¬
â””â”€ Rack, PDU, climatisation
    â””â”€ 5 000 â‚¬
TOTAL : 58 000 â‚¬

CoÃ»ts annuels :
â”œâ”€ Ã‰lectricitÃ© (5 serveurs Ã— 300W Ã— 24h Ã— 365j Ã— 0.15â‚¬/kWh)
â”‚   â””â”€ 1 970 â‚¬/an
â”œâ”€ HÃ©bergement datacenter
â”‚   â””â”€ 3 000 â‚¬/an
â”œâ”€ Maintenance (piÃ¨ces, interventions)
â”‚   â””â”€ 2 000 â‚¬/an
TOTAL : 6 970 â‚¬/an

CoÃ»t 5 ans : 58 000 + (6 970 Ã— 5) = 92 850 â‚¬
```

#### Option 2 : VirtualisÃ© (2 hÃ´tes + stockage)

```
Investissement initial :
â”œâ”€ 2Ã— HÃ´tes puissants (512 GB RAM, 64 cores, 8Ã— NVMe)
â”‚   â””â”€ 2Ã— 35 000 â‚¬ = 70 000 â‚¬
â”œâ”€ SAN All-Flash (10 TB utile)
â”‚   â””â”€ 40 000 â‚¬
â”œâ”€ Switch 25GbE (stockage + VM)
â”‚   â””â”€ 8 000 â‚¬
â”œâ”€ Licences hyperviseur (Proxmox support)
â”‚   â””â”€ 1 600 â‚¬/an Ã— 2 hÃ´tes = 3 200 â‚¬
â””â”€ Rack, PDU, climatisation
    â””â”€ 6 000 â‚¬
TOTAL : 127 200 â‚¬

CoÃ»ts annuels :
â”œâ”€ Ã‰lectricitÃ© (2 hÃ´tes + SAN Ã— 500W Ã— 24h Ã— 365j Ã— 0.15â‚¬/kWh)
â”‚   â””â”€ 1 314 â‚¬/an
â”œâ”€ HÃ©bergement datacenter
â”‚   â””â”€ 2 000 â‚¬/an
â”œâ”€ Support Proxmox
â”‚   â””â”€ 3 200 â‚¬/an
â”œâ”€ Maintenance
â”‚   â””â”€ 2 500 â‚¬/an
TOTAL : 9 014 â‚¬/an

CoÃ»t 5 ans : 127 200 + (9 014 Ã— 5) = 172 270 â‚¬
```

#### Analyse

| CritÃ¨re | Bare Metal | VirtualisÃ© | Gagnant |
|---------|-----------|------------|---------|
| **Investissement initial** | 58 000 â‚¬ | 127 200 â‚¬ | Bare Metal |
| **CoÃ»t 5 ans** | 92 850 â‚¬ | 172 270 â‚¬ | Bare Metal |
| **Performance** | 100% | 90-95% | Bare Metal |
| **FlexibilitÃ©** | Faible | TrÃ¨s Ã©levÃ©e | VM |
| **HA** | Manuelle | Automatique | VM |
| **ScalabilitÃ©** | Difficile | Facile | VM |
| **Maintenance** | Complexe | SimplifiÃ©e | VM |

**Conclusion :**

- **< 3-5 serveurs :** Bare metal plus Ã©conomique
- **> 5 serveurs :** Virtualisation devient rentable
- **Besoin HA/DR :** Virtualisation fortement recommandÃ©e
- **Performance critique :** Bare metal

---

## Conclusion : Quand Choisir les VMs ?

### VMs : Le Choix Optimal Pour

âœ… **Infrastructure moyenne Ã  grande (5+ serveurs)**
- Consolidation rentable
- Gestion centralisÃ©e
- FlexibilitÃ© maximale

âœ… **Besoin de haute disponibilitÃ©**
- Live Migration
- HA automatique hyperviseur
- Snapshots pour rollback rapide

âœ… **Environnements multiples**
- Production + Staging + Dev sur mÃªme hardware
- Isolation sÃ©curisÃ©e
- Clonage rapide

âœ… **Croissance incertaine**
- Ajout ressources Ã  chaud
- ScalabilitÃ© progressive
- Pas de sur-dimensionnement initial

âœ… **Ã‰quipe technique moyenne**
- Outils d'administration graphiques
- Moins d'expertise bare metal nÃ©cessaire
- Automatisation facilitÃ©e

### Ã‰viter les VMs Si

âŒ **Performance absolue requise**
- Trading haute frÃ©quence
- Analytics temps rÃ©el extrÃªme
- Overhead inacceptable

âŒ **Infrastructure trÃ¨s petite (1-2 serveurs)**
- CoÃ»t overhead non justifiÃ©
- ComplexitÃ© inutile

âŒ **ConformitÃ© interdisant virtualisation**
- Certifications spÃ©cifiques
- RÃ©glementations secteur bancaire strict

### Points ClÃ©s Ã  Retenir

1. **Performance :** VMs modernes offrent 90-95% des performances bare metal avec hyperviseur optimisÃ©.

2. **Configuration :** Les rÃ©glages hyperviseur (CPU pinning, memory reservation, cache policies) sont **critiques** pour PostgreSQL.

3. **Stockage :** Le choix stockage (local, SAN, vSAN, Ceph) impacte davantage que la virtualisation elle-mÃªme.

4. **HA :** Les VMs simplifient radicalement la haute disponibilitÃ© vs bare metal.

5. **CoÃ»t :** VirtualisÃ© devient rentable Ã  partir de 5+ serveurs, grÃ¢ce Ã  la consolidation.

6. **PostgreSQL 18 :** Les amÃ©liorations I/O asynchrones rÃ©duisent l'Ã©cart de performance avec bare metal.

**Recommandation gÃ©nÃ©rale :**

Pour la majoritÃ© des cas d'usage PostgreSQL en entreprise, **la virtualisation avec Proxmox (open source) ou VMware vSphere (enterprise)** offre le meilleur compromis flexibilitÃ©/performance/coÃ»t.

RÃ©servez le bare metal aux charges ultra-critiques ou infrastructures <3 serveurs.

---

**Prochaines Ã©tapes suggÃ©rÃ©es :**
- 19.1.3. Conteneurs (Docker, Podman)
- 19.1.4. Kubernetes (StatefulSets, Operators)
- 17. Haute DisponibilitÃ© et RÃ©plication (approfondissement)

â­ï¸ [Conteneurs (Docker, Podman)](/19-postgresql-en-production/01.3-conteneurs.md)
