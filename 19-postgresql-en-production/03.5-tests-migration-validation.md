ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 19.3.5. Tests de migration et validation

## Introduction

La migration vers PostgreSQL 18 est un Ã©vÃ©nement majeur pour votre infrastructure. Comme pour tout changement critique, **tester avant de migrer** n'est pas optionnel, c'est **essentiel**. Cette section vous guide Ã  travers les diffÃ©rentes Ã©tapes de tests et de validation pour garantir une migration rÃ©ussie.

## Pourquoi tester est crucial

### Analogie : Le voyage en avion

Imaginez prendre un avion sans que celui-ci ait Ã©tÃ© testÃ© :

**Sans tests** :
```
âœˆï¸ Avion neuf â†’ Embarquement direct â†’ Vol avec passagers
   â†“
   âŒ Risque d'accident catastrophique
```

**Avec tests** :
```
âœˆï¸ Avion neuf â†’ Tests au sol â†’ Vol d'essai vide â†’ Vol avec Ã©quipage rÃ©duit
   â†’ Vol d'essai avec quelques passagers â†’ Vol commercial normal
   â†“
   âœ… SÃ©curitÃ© maximale
```

Pour PostgreSQL, c'est identique :

```
PostgreSQL 18 â†’ Tests en DEV â†’ Tests en STAGING â†’ Tests de charge
   â†’ Migration de prÃ©production â†’ Migration production
   â†“
   âœ… Migration rÃ©ussie et sÃ©curisÃ©e
```

### Les risques sans tests

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ConsÃ©quences d'une migration non testÃ©e                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  âŒ IncompatibilitÃ© SQL dÃ©couverte en production            â”‚
â”‚     â†’ Applications qui plantent                             â”‚
â”‚     â†’ Perte de revenus                                      â”‚
â”‚                                                             â”‚
â”‚  âŒ Performances dÃ©gradÃ©es inattendues                      â”‚
â”‚     â†’ RequÃªtes 10Ã— plus lentes                              â”‚
â”‚     â†’ Timeout utilisateurs                                  â”‚
â”‚                                                             â”‚
â”‚  âŒ Perte de donnÃ©es ou corruption                          â”‚
â”‚     â†’ Types de donnÃ©es incompatibles                        â”‚
â”‚     â†’ Contraintes violÃ©es                                   â”‚
â”‚                                                             â”‚
â”‚  âŒ Rollback impossible ou difficile                        â”‚
â”‚     â†’ Downtime prolongÃ©                                     â”‚
â”‚     â†’ Panique et stress de l'Ã©quipe                         â”‚
â”‚                                                             â”‚
â”‚  ğŸ’° CoÃ»t : Millions d'euros potentiels                      â”‚
â”‚  ğŸ˜° Stress : Maximal                                        â”‚
â”‚  ğŸ‘¥ RÃ©putation : EndommagÃ©e                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les bÃ©nÃ©fices des tests

```
âœ… Confiance : L'Ã©quipe sait que Ã§a fonctionne
âœ… Anticipation : ProblÃ¨mes dÃ©couverts avant production
âœ… Documentation : ProcÃ©dures validÃ©es et documentÃ©es
âœ… Rollback prÃ©parÃ© : Plan B testÃ© et prÃªt
âœ… SÃ©rÃ©nitÃ© : Migration en production sans surprises
âœ… Business continuity : Service maintenu sans interruption
```

## Les environnements de test

### Pyramide des environnements

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pyramide des environnements de test                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                       ğŸ¢ PRODUCTION                         â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                    â”‚   PostgreSQL 18  â”‚                     â”‚
â”‚                    â”‚   (Migration finale)                   â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                           â†‘                                 â”‚
â”‚                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                 /                    \                      â”‚
â”‚                                                             â”‚
â”‚              ğŸ”§ STAGING / PRÃ‰PRODUCTION                     â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚           â”‚   Clone quasi-identique prod â”‚                  â”‚
â”‚           â”‚   Tests finaux rÃ©alistes     â”‚                  â”‚
â”‚           â”‚   Validation performance     â”‚                  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                        â†‘                                    â”‚
â”‚               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚              /                    \                         â”‚
â”‚                                                             â”‚
â”‚                   ğŸ’» DÃ‰VELOPPEMENT                          â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚          â”‚   Environnement dÃ©veloppeur      â”‚               â”‚
â”‚          â”‚   Tests unitaires et intÃ©gration â”‚               â”‚
â”‚          â”‚   DÃ©couverte des incompatibilitÃ©sâ”‚               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Description des environnements

#### 1. DÃ©veloppement (DEV)

**CaractÃ©ristiques** :
```
DonnÃ©es : Sous-ensemble ou donnÃ©es de test (< 1 GB)
Infrastructure : Serveur modeste (2-4 cores, 8 GB RAM)
Objectif : DÃ©couvrir rapidement les incompatibilitÃ©s
FrÃ©quence : Tests quotidiens, multiples itÃ©rations
Audience : DÃ©veloppeurs uniquement
```

**Utilisation** :
```sql
-- DEV : Tester les requÃªtes SQL sur PG 18
-- Exemple : VÃ©rifier qu'une requÃªte fonctionne toujours

-- Cette requÃªte fonctionnait sur PG 17
SELECT * FROM users WHERE id = ANY('{1,2,3}');

-- VÃ©rifier qu'elle fonctionne toujours sur PG 18
-- Tester les nouvelles fonctionnalitÃ©s
SELECT * FROM users WHERE created_at > NOW() - INTERVAL '1 day';
```

**Avantages** :
- Cycle de test rapide (minutes)
- Permet d'itÃ©rer rapidement
- Pas d'impact sur la production

**Limitations** :
- DonnÃ©es limitÃ©es (ne dÃ©tecte pas tous les problÃ¨mes)
- Performances non reprÃ©sentatives
- Certains bugs n'apparaissent qu'avec de vraies donnÃ©es

#### 2. Staging / PrÃ©production (STAGING)

**CaractÃ©ristiques** :
```
DonnÃ©es : Clone de production ou anonymisÃ©es (taille rÃ©elle)
Infrastructure : Identique ou proche de production
Objectif : Validation finale avant migration prod
FrÃ©quence : Tests hebdomadaires ou avant chaque release
Audience : Ã‰quipe complÃ¨te (dev, ops, QA, business)
```

**Utilisation** :
```bash
# CrÃ©er un clone de production pour STAGING
# Option 1 : Via pg_dump/restore
pg_dump -h prod-server -U postgres mydb | \
    psql -h staging-server -U postgres mydb

# Option 2 : Via snapshot disque (AWS, Azure)
aws rds create-db-snapshot --db-instance-identifier staging-pg18

# Option 3 : Via rÃ©plication puis promotion
# (Utiliser rÃ©plication physique, puis promouvoir le replica)
```

**Tests Ã  effectuer** :
- Migration complÃ¨te avec pg_upgrade
- Tests de charge rÃ©alistes
- Validation avec applications rÃ©elles
- Tests de rollback

**Avantages** :
- DonnÃ©es rÃ©elles (ou anonymisÃ©es)
- Performances rÃ©alistes
- DÃ©tecte les vrais problÃ¨mes

**Limitations** :
- CoÃ»t infrastructure (serveur dÃ©diÃ©)
- Synchronisation des donnÃ©es avec prod (peuvent Ãªtre obsolÃ¨tes)

#### 3. Production (PROD)

**CaractÃ©ristiques** :
```
DonnÃ©es : DonnÃ©es rÃ©elles
Infrastructure : Configuration optimale
Objectif : Service en conditions rÃ©elles
Migration : AprÃ¨s validation complÃ¨te en DEV et STAGING
```

**Principe** :
```
La production n'est PAS un environnement de test !
On y applique uniquement ce qui a Ã©tÃ© validÃ© en STAGING.
```

### StratÃ©gie de donnÃ©es de test

#### Anonymisation des donnÃ©es

Pour STAGING, il est souvent nÃ©cessaire d'anonymiser les donnÃ©es sensibles :

```sql
-- Exemple : Anonymiser les donnÃ©es personnelles
-- Sur le clone STAGING aprÃ¨s copie

-- Anonymiser emails
UPDATE users
SET email = 'user_' || id || '@test.example.com';

-- Anonymiser noms
UPDATE users
SET
    first_name = 'FirstName' || id,
    last_name = 'LastName' || id;

-- Anonymiser adresses
UPDATE addresses
SET
    street = 'Test Street ' || id,
    city = 'TestCity',
    postal_code = LPAD(id::text, 5, '0');

-- Supprimer donnÃ©es trÃ¨s sensibles
DELETE FROM payment_methods;
UPDATE orders SET notes = 'Test order';
```

**Outils d'anonymisation** :
- **pg_anonymize** : Extension PostgreSQL
- **PostgreSQL Anonymizer** : Solution complÃ¨te
- Scripts personnalisÃ©s SQL

#### DonnÃ©es synthÃ©tiques

Alternative : GÃ©nÃ©rer des donnÃ©es de test rÃ©alistes

```sql
-- GÃ©nÃ©rer 1 million d'utilisateurs de test
INSERT INTO users (first_name, last_name, email, created_at)
SELECT
    'User' || generate_series AS first_name,
    'Test' || generate_series AS last_name,
    'user' || generate_series || '@test.com' AS email,
    NOW() - (random() * INTERVAL '365 days') AS created_at
FROM generate_series(1, 1000000);

-- GÃ©nÃ©rer des commandes alÃ©atoires
INSERT INTO orders (user_id, total, status, created_at)
SELECT
    (random() * 1000000)::int + 1 AS user_id,
    (random() * 1000)::numeric(10,2) AS total,
    (ARRAY['pending', 'paid', 'shipped', 'delivered'])[floor(random() * 4 + 1)] AS status,
    NOW() - (random() * INTERVAL '90 days') AS created_at
FROM generate_series(1, 5000000);
```

## Phase 1 : Tests prÃ©-migration

### Analyse de compatibilitÃ©

#### VÃ©rifier les dÃ©prÃ©ciations PostgreSQL 18

PostgreSQL 18 peut dÃ©prÃ©cier certaines fonctionnalitÃ©s ou changer des comportements :

```sql
-- 1. Identifier les fonctions dÃ©prÃ©ciÃ©es
-- Consulter les release notes PostgreSQL 18

-- 2. Rechercher l'utilisation de md5 (dÃ©prÃ©ciÃ© en faveur de scram-sha-256)
SELECT usename, passwd
FROM pg_shadow
WHERE passwd LIKE 'md5%';

-- Si rÃ©sultats, migrer vers scram-sha-256 avant migration

-- 3. VÃ©rifier les types de donnÃ©es personnalisÃ©s
SELECT typname, typtype
FROM pg_type
WHERE typtype = 'c'  -- composite types
    AND typnamespace::regnamespace::text NOT IN ('pg_catalog', 'information_schema');
```

#### VÃ©rifier la compatibilitÃ© des extensions

```sql
-- Lister toutes les extensions installÃ©es
SELECT
    extname AS extension_name,
    extversion AS version,
    extrelocatable AS relocatable
FROM pg_extension
ORDER BY extname;

-- VÃ©rifier la disponibilitÃ© dans PostgreSQL 18
-- Consulter : https://www.postgresql.org/docs/18/
```

**Extensions populaires et compatibilitÃ© PG 18** :

```
âœ… PostGIS : Compatible (version 3.4+)
âœ… pg_stat_statements : Inclus (amÃ©liorÃ©)
âœ… pgcrypto : Compatible
âœ… uuid-ossp : Compatible (mais UUIDv7 natif maintenant)
âœ… hstore : Compatible
âœ… pg_trgm : Compatible
âœ… pgvector : Compatible (version 0.6.0+)
âš ï¸  Extension_X : VÃ©rifier la documentation spÃ©cifique
```

#### Analyser les requÃªtes SQL

```sql
-- Identifier les requÃªtes qui utilisent des syntaxes potentiellement problÃ©matiques

-- 1. Activer pg_stat_statements (si pas dÃ©jÃ  fait)
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- 2. Collecter les requÃªtes les plus frÃ©quentes
SELECT
    query,
    calls,
    mean_exec_time,
    total_exec_time
FROM pg_stat_statements
WHERE query NOT LIKE 'SELECT % FROM pg_%'  -- Exclure requÃªtes systÃ¨me
ORDER BY calls DESC
LIMIT 100;

-- 3. Exporter pour analyse
\copy (SELECT query FROM pg_stat_statements ORDER BY calls DESC LIMIT 1000) TO '/tmp/queries.sql'
```

**VÃ©rifier manuellement** :
- Utilisation de fonctionnalitÃ©s dÃ©prÃ©ciÃ©es
- RequÃªtes non standard
- Utilisations de types spÃ©cifiques

#### Test de pg_upgrade --check

```bash
# Sur un clone de production en DEV
pg_upgrade \
    --old-datadir=/var/lib/postgresql/17/main \
    --new-datadir=/var/lib/postgresql/18/main \
    --old-bindir=/usr/lib/postgresql/17/bin \
    --new-bindir=/usr/lib/postgresql/18/bin \
    --check

# Sortie :
# Performing Consistency Checks
# -----------------------------
# Checking cluster versions                                  ok
# Checking database user is the install user                 ok
# Checking database connection settings                      ok
# Checking for prepared transactions                         ok
# ...

# Si "ok" partout â†’ Compatible âœ…
# Si erreurs â†’ Ã€ corriger avant migration
```

### Tests des applications

#### Test de connexion

```python
# Python : VÃ©rifier que l'application se connecte Ã  PG 18
import psycopg2

try:
    conn = psycopg2.connect(
        host="staging-pg18",
        database="mydb",
        user="appuser",
        password="secret"
    )

    cursor = conn.cursor()
    cursor.execute("SELECT version();")
    version = cursor.fetchone()
    print(f"âœ… Connexion rÃ©ussie : {version[0]}")

    cursor.close()
    conn.close()
except Exception as e:
    print(f"âŒ Erreur de connexion : {e}")
```

```javascript
// Node.js : Test similaire
const { Pool } = require('pg');

const pool = new Pool({
    host: 'staging-pg18',
    database: 'mydb',
    user: 'appuser',
    password: 'secret'
});

pool.query('SELECT version()', (err, result) => {
    if (err) {
        console.error('âŒ Erreur:', err);
    } else {
        console.log('âœ… Version:', result.rows[0].version);
    }
    pool.end();
});
```

#### Tests unitaires et d'intÃ©gration

```bash
# Lancer les tests automatisÃ©s contre PostgreSQL 18
# Exemple avec Jest (Node.js)
DATABASE_URL=postgresql://user:pass@staging-pg18:5432/mydb npm test

# Exemple avec pytest (Python)
DATABASE_URL=postgresql://user:pass@staging-pg18:5432/mydb pytest

# Observer les rÃ©sultats
# âœ… 1250 tests passed
# âŒ 3 tests failed â† Ã€ investiguer !
```

**Analyser les Ã©checs** :
```
Test failed: test_user_creation
Error: column "uuid" does not exist

â†’ Investigation : UUID gÃ©nÃ©rÃ© diffÃ©remment en PG 18 ?
â†’ Solution : Utiliser UUIDv7 natif ou ajuster le code
```

#### Tests fonctionnels manuels

CrÃ©er une **checklist de tests fonctionnels** :

```
â–¡ Authentification utilisateur
  â–¡ Login avec email/password
  â–¡ Login avec OAuth
  â–¡ Logout

â–¡ OpÃ©rations CRUD basiques
  â–¡ CrÃ©er un utilisateur
  â–¡ Lire les donnÃ©es utilisateur
  â–¡ Mettre Ã  jour un profil
  â–¡ Supprimer un compte

â–¡ FonctionnalitÃ©s mÃ©tier critiques
  â–¡ CrÃ©er une commande
  â–¡ Payer avec carte bancaire
  â–¡ Rechercher des produits
  â–¡ Exporter un rapport

â–¡ FonctionnalitÃ©s avancÃ©es
  â–¡ Full-text search
  â–¡ Filtres complexes
  â–¡ AgrÃ©gations et statistiques
```

### Tests de performance

#### Ã‰tablir une baseline (rÃ©fÃ©rence)

**Sur PostgreSQL 17 (production actuelle)** :

```sql
-- 1. Activer le timing
\timing on

-- 2. ExÃ©cuter des requÃªtes reprÃ©sentatives et noter les temps
-- RequÃªte 1 : Recherche utilisateur
SELECT * FROM users WHERE email = 'test@example.com';
-- Time: 2.345 ms

-- RequÃªte 2 : Liste des commandes rÃ©centes
SELECT * FROM orders
WHERE created_at > NOW() - INTERVAL '7 days'
ORDER BY created_at DESC
LIMIT 100;
-- Time: 45.678 ms

-- RequÃªte 3 : AgrÃ©gation complexe
SELECT
    DATE_TRUNC('day', created_at) AS day,
    COUNT(*) AS orders_count,
    SUM(total) AS revenue
FROM orders
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY day
ORDER BY day;
-- Time: 234.567 ms
```

**Documenter dans un fichier baseline.txt** :

```
PostgreSQL 17 - Baseline Performance
====================================
Date: 2024-11-23
Server: production-pg17 (16 cores, 64 GB RAM, NVMe)

Query 1 - User search: 2.345 ms
Query 2 - Recent orders: 45.678 ms
Query 3 - Daily aggregation: 234.567 ms
Query 4 - Complex join: 567.890 ms
...
```

#### Comparer avec PostgreSQL 18

**Sur STAGING PostgreSQL 18** :

```bash
# Script de benchmark
#!/bin/bash
# benchmark_pg18.sh

echo "PostgreSQL 18 - Performance Benchmark"
echo "======================================"
date

psql -h staging-pg18 -U postgres -d mydb << 'EOF'
\timing on

-- Query 1
SELECT * FROM users WHERE email = 'test@example.com';

-- Query 2
SELECT * FROM orders
WHERE created_at > NOW() - INTERVAL '7 days'
ORDER BY created_at DESC
LIMIT 100;

-- Query 3
SELECT
    DATE_TRUNC('day', created_at) AS day,
    COUNT(*) AS orders_count,
    SUM(total) AS revenue
FROM orders
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY day
ORDER BY day;

-- Autres requÃªtes...
EOF
```

**RÃ©sultats** :
```
PostgreSQL 18 - Performance Benchmark
======================================

Query 1 - User search: 2.123 ms (âœ… 9% plus rapide)
Query 2 - Recent orders: 38.456 ms (âœ… 16% plus rapide)
Query 3 - Daily aggregation: 198.765 ms (âœ… 15% plus rapide)
Query 4 - Complex join: 456.789 ms (âœ… 20% plus rapide)
```

**InterprÃ©tation** :
- Si performances similaires ou meilleures : âœ… Bon signe
- Si dÃ©gradation < 10% : âš ï¸ Acceptable (peut Ãªtre optimisÃ©)
- Si dÃ©gradation > 20% : âŒ Investigation requise

#### Tests de charge avec pgbench

```bash
# 1. Initialiser pgbench sur STAGING
pgbench -i -s 100 -h staging-pg18 -U postgres mydb
# s=100 â†’ ~10 GB de donnÃ©es

# 2. Benchmark en lecture seule
pgbench -c 50 -j 4 -T 300 -S -h staging-pg18 -U postgres mydb
# -c 50 : 50 clients concurrents
# -j 4 : 4 threads
# -T 300 : DurÃ©e 5 minutes
# -S : Read-only (SELECT only)

# RÃ©sultats :
# transaction type: <builtin: select only>
# scaling factor: 100
# number of clients: 50
# number of threads: 4
# duration: 300 s
# number of transactions: 1234567
# tps = 4115.223456 (including connections establishing)
# tps = 4116.789012 (excluding connections establishing)

# 3. Benchmark en lecture/Ã©criture
pgbench -c 50 -j 4 -T 300 -h staging-pg18 -U postgres mydb

# 4. Comparer avec PostgreSQL 17
# ExÃ©cuter les mÃªmes tests sur PG 17 et comparer les TPS
```

**Benchmark personnalisÃ©** :

```bash
# Script SQL personnalisÃ© pour pgbench
cat > custom_benchmark.sql << 'EOF'
-- Simulation de charge rÃ©aliste
BEGIN;
-- Insert
INSERT INTO orders (user_id, total, status)
VALUES (random() * 1000000, random() * 1000, 'pending');
-- Update
UPDATE users SET last_login = NOW() WHERE id = :id;
-- Select
SELECT * FROM products WHERE category = 'electronics' LIMIT 10;
COMMIT;
EOF

# ExÃ©cuter
pgbench -c 20 -j 4 -T 60 -f custom_benchmark.sql -h staging-pg18 mydb
```

#### Tests de charge applicative

```python
# test_load.py - Simuler une charge applicative rÃ©elle
import psycopg2
import random
import time
from concurrent.futures import ThreadPoolExecutor

def simulate_user_session(user_id):
    """Simule une session utilisateur"""
    conn = psycopg2.connect(
        host="staging-pg18",
        database="mydb",
        user="appuser",
        password="secret"
    )
    cursor = conn.cursor()

    try:
        # 1. Login (SELECT)
        cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
        user = cursor.fetchone()

        # 2. Browse products (SELECT with JOIN)
        cursor.execute("""
            SELECT p.*, c.name as category_name
            FROM products p
            JOIN categories c ON p.category_id = c.id
            WHERE p.active = true
            LIMIT 20
        """)
        products = cursor.fetchall()

        # 3. Create order (INSERT)
        cursor.execute("""
            INSERT INTO orders (user_id, total, status)
            VALUES (%s, %s, 'pending')
            RETURNING id
        """, (user_id, random.uniform(10, 500)))
        order_id = cursor.fetchone()[0]

        # 4. Update profile (UPDATE)
        cursor.execute("""
            UPDATE users
            SET last_activity = NOW()
            WHERE id = %s
        """, (user_id,))

        conn.commit()
        return True

    except Exception as e:
        conn.rollback()
        print(f"Error for user {user_id}: {e}")
        return False
    finally:
        cursor.close()
        conn.close()

# Simuler 100 utilisateurs concurrents
with ThreadPoolExecutor(max_workers=100) as executor:
    start = time.time()
    results = list(executor.map(simulate_user_session, range(1, 1001)))
    duration = time.time() - start

    success_rate = sum(results) / len(results) * 100
    print(f"Duration: {duration:.2f}s")
    print(f"Success rate: {success_rate:.1f}%")
    print(f"Throughput: {len(results)/duration:.2f} sessions/sec")
```

## Phase 2 : Tests pendant la migration

### Simulation de migration complÃ¨te

#### Sur STAGING : RÃ©pÃ©tition gÃ©nÃ©rale

```bash
#!/bin/bash
# full_migration_rehearsal.sh
# Simulation complÃ¨te de la migration

echo "ğŸ¬ RÃ©pÃ©tition gÃ©nÃ©rale de la migration"
echo "======================================"

# 1. Ã‰tat initial
echo "ğŸ“Š Ã‰tat initial"
psql -h staging-pg17 -c "\l+"
psql -h staging-pg17 -d mydb -c "SELECT COUNT(*) FROM users;"

# 2. ArrÃªt de PostgreSQL 17
echo "â¸ï¸  ArrÃªt PostgreSQL 17"
sudo systemctl stop postgresql@17-main

# 3. Migration pg_upgrade
echo "ğŸ”„ Migration pg_upgrade"
time sudo -u postgres pg_upgrade \
    --old-datadir=/var/lib/postgresql/17/main \
    --new-datadir=/var/lib/postgresql/18/main \
    --old-bindir=/usr/lib/postgresql/17/bin \
    --new-bindir=/usr/lib/postgresql/18/bin \
    --jobs=8 \
    --swap

# 4. DÃ©marrage PostgreSQL 18
echo "â–¶ï¸  DÃ©marrage PostgreSQL 18"
sudo systemctl start postgresql@18-main

# 5. Validation immÃ©diate
echo "âœ… Validation"
psql -h localhost -p 5432 -c "SELECT version();"
psql -h localhost -d mydb -c "SELECT COUNT(*) FROM users;"

# 6. Tests de fumÃ©e
echo "ğŸ’¨ Smoke tests"
./smoke_tests.sh

# 7. Mesure du downtime
DOWNTIME=$(grep "Upgrade Complete" pg_upgrade_server.log | \
    awk '{print $NF}')
echo "â±ï¸  Downtime simulÃ©: $DOWNTIME"

echo "âœ… RÃ©pÃ©tition terminÃ©e"
```

#### ChronomÃ©trer chaque Ã©tape

```
RÃ©sultats de la rÃ©pÃ©tition gÃ©nÃ©rale :
=====================================

ArrÃªt PG 17 :           15 secondes
pg_upgrade --check :    8 minutes
pg_upgrade migration :  45 minutes
DÃ©marrage PG 18 :       30 secondes
Smoke tests :           2 minutes

DOWNTIME TOTAL ESTIMÃ‰ : 47 minutes
```

### Tests de rollback

Il est **crucial** de tester le rollback avant la migration en production :

```bash
#!/bin/bash
# test_rollback.sh

echo "ğŸ”™ Test de rollback"
echo "==================="

# ScÃ©nario : Migration PG 18 effectuÃ©e, mais problÃ¨me dÃ©tectÃ©

# 1. Ã‰tat actuel : PG 18 en production simulÃ©e
echo "Ã‰tat actuel : PostgreSQL 18"
psql -h staging-pg18 -c "SELECT version();"

# 2. Simulation d'un problÃ¨me
echo "âš ï¸  ProblÃ¨me dÃ©tectÃ© : Performance dÃ©gradÃ©e"

# 3. ArrÃªt PG 18
echo "â¸ï¸  ArrÃªt PostgreSQL 18"
sudo systemctl stop postgresql@18-main

# 4. Restauration PG 17 depuis .old (si --swap utilisÃ©)
echo "ğŸ”„ Restauration PostgreSQL 17"
sudo -u postgres mv /var/lib/postgresql/18/main /var/lib/postgresql/18/main.failed
sudo -u postgres mv /var/lib/postgresql/.old /var/lib/postgresql/17/main

# 5. RedÃ©marrage PG 17
echo "â–¶ï¸  RedÃ©marrage PostgreSQL 17"
sudo systemctl start postgresql@17-main

# 6. Validation
echo "âœ… Validation rollback"
psql -h localhost -c "SELECT version();"
psql -h localhost -d mydb -c "SELECT COUNT(*) FROM users;"

# 7. Mesurer le temps
echo "â±ï¸  Temps de rollback: < 2 minutes"

echo "âœ… Rollback rÃ©ussi"
```

**Important** : Le rollback n'est possible que si aucune donnÃ©e n'a Ã©tÃ© modifiÃ©e sur PG 18 aprÃ¨s la migration.

## Phase 3 : Tests post-migration

### Validation immÃ©diate (T+0 Ã  T+1h)

#### Checklist de validation post-migration

```bash
#!/bin/bash
# post_migration_validation.sh

echo "âœ… Validation post-migration PostgreSQL 18"
echo "=========================================="

# 1. Version
echo "1ï¸âƒ£  VÃ©rification version"
VERSION=$(psql -t -c "SELECT version();" | grep "PostgreSQL 18")
if [ -n "$VERSION" ]; then
    echo "âœ… Version PostgreSQL 18 confirmÃ©e"
else
    echo "âŒ Version incorrecte !"
    exit 1
fi

# 2. Connexions
echo "2ï¸âƒ£  VÃ©rification connexions"
CONNECTIONS=$(psql -t -c "SELECT count(*) FROM pg_stat_activity;")
echo "   Connexions actives : $CONNECTIONS"

# 3. RÃ©plicas (si applicable)
echo "3ï¸âƒ£  VÃ©rification rÃ©plication"
psql -c "SELECT * FROM pg_stat_replication;"

# 4. IntÃ©gritÃ© donnÃ©es
echo "4ï¸âƒ£  VÃ©rification intÃ©gritÃ© donnÃ©es"
psql -d mydb << 'EOF'
-- Compter les enregistrements de chaque table
SELECT
    schemaname,
    tablename,
    n_live_tup
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC
LIMIT 20;
EOF

# 5. Extensions
echo "5ï¸âƒ£  VÃ©rification extensions"
psql -d mydb -c "SELECT extname, extversion FROM pg_extension;"

# 6. Test de lecture
echo "6ï¸âƒ£  Test de lecture"
psql -d mydb -c "SELECT COUNT(*) FROM users;"

# 7. Test d'Ã©criture
echo "7ï¸âƒ£  Test d'Ã©criture"
psql -d mydb << 'EOF'
BEGIN;
INSERT INTO test_migration (message, created_at)
VALUES ('Post-migration test', NOW());
SELECT * FROM test_migration WHERE message = 'Post-migration test';
ROLLBACK;
EOF

# 8. Performance simple
echo "8ï¸âƒ£  Test performance"
psql -d mydb -c "\timing on" -c "SELECT COUNT(*) FROM orders;"

echo ""
echo "âœ… Validation post-migration terminÃ©e"
```

#### Tests de fumÃ©e (Smoke tests)

```python
# smoke_tests.py - Tests rapides post-migration

import psycopg2
import sys

def run_smoke_tests():
    """ExÃ©cute les tests de fumÃ©e essentiels"""

    tests_passed = 0
    tests_failed = 0

    try:
        conn = psycopg2.connect(
            host="localhost",
            database="mydb",
            user="postgres",
            password="secret"
        )
        cursor = conn.cursor()

        # Test 1: Version
        print("ğŸ§ª Test 1: Version PostgreSQL")
        cursor.execute("SELECT version();")
        version = cursor.fetchone()[0]
        if "PostgreSQL 18" in version:
            print("   âœ… PASS")
            tests_passed += 1
        else:
            print(f"   âŒ FAIL - Version: {version}")
            tests_failed += 1

        # Test 2: Lecture table users
        print("ğŸ§ª Test 2: Lecture table users")
        cursor.execute("SELECT COUNT(*) FROM users;")
        count = cursor.fetchone()[0]
        if count > 0:
            print(f"   âœ… PASS - {count} users")
            tests_passed += 1
        else:
            print("   âŒ FAIL - Aucun utilisateur")
            tests_failed += 1

        # Test 3: Ã‰criture (transaction rollback)
        print("ğŸ§ª Test 3: Ã‰criture")
        conn.autocommit = False
        cursor.execute("""
            INSERT INTO users (email, first_name, last_name)
            VALUES ('test@migration.com', 'Test', 'Migration')
            RETURNING id
        """)
        user_id = cursor.fetchone()[0]
        conn.rollback()
        print(f"   âœ… PASS - INSERT rÃ©ussi (rollback)")
        tests_passed += 1

        # Test 4: Index fonctionnent
        print("ğŸ§ª Test 4: Index")
        cursor.execute("SELECT schemaname, tablename, indexname FROM pg_indexes WHERE schemaname = 'public' LIMIT 5;")
        indexes = cursor.fetchall()
        if len(indexes) > 0:
            print(f"   âœ… PASS - {len(indexes)} index trouvÃ©s")
            tests_passed += 1
        else:
            print("   âŒ FAIL - Aucun index")
            tests_failed += 1

        # Test 5: Extensions
        print("ğŸ§ª Test 5: Extensions")
        cursor.execute("SELECT extname FROM pg_extension WHERE extname = 'pg_stat_statements';")
        if cursor.fetchone():
            print("   âœ… PASS - Extensions chargÃ©es")
            tests_passed += 1
        else:
            print("   âš ï¸  WARNING - pg_stat_statements non trouvÃ©")

        cursor.close()
        conn.close()

        # RÃ©sumÃ©
        print("\n" + "="*50)
        print(f"Tests rÃ©ussis : {tests_passed}")
        print(f"Tests Ã©chouÃ©s : {tests_failed}")

        if tests_failed > 0:
            print("âŒ Ã‰CHEC - Certains tests ont Ã©chouÃ©")
            sys.exit(1)
        else:
            print("âœ… SUCCÃˆS - Tous les tests sont passÃ©s")
            sys.exit(0)

    except Exception as e:
        print(f"âŒ ERREUR CRITIQUE : {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_smoke_tests()
```

### Validation continue (T+1h Ã  T+24h)

#### Monitoring des performances

```sql
-- Surveiller les requÃªtes lentes dans les premiÃ¨res heures
-- (pg_stat_statements doit Ãªtre activÃ©)

-- Top 20 des requÃªtes les plus lentes
SELECT
    substring(query, 1, 100) AS query_preview,
    calls,
    mean_exec_time,
    max_exec_time,
    total_exec_time,
    stddev_exec_time
FROM pg_stat_statements
WHERE mean_exec_time > 100  -- > 100ms
ORDER BY mean_exec_time DESC
LIMIT 20;

-- Comparer avec la baseline PG 17
-- Si dÃ©gradation importante, investiguer
```

#### Monitoring des ressources

```bash
#!/bin/bash
# monitor_resources.sh
# Ã€ lancer toutes les 5 minutes via cron

echo "ğŸ“Š Monitoring PostgreSQL 18 - $(date)"

# CPU
echo "CPU:"
top -b -n 1 | grep postgres | head -5

# RAM
echo "RAM:"
free -h | grep Mem

# Disk I/O
echo "Disk I/O:"
iostat -x 1 2 | tail -n +4

# Connexions actives
echo "Connexions:"
psql -t -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"

# Cache hit ratio
echo "Cache Hit Ratio:"
psql -t -c "
SELECT
    ROUND(
        (sum(blks_hit) * 100.0 / NULLIF(sum(blks_hit + blks_read), 0)), 2
    ) AS cache_hit_ratio
FROM pg_stat_database;
"

# Locks en attente
LOCKS=$(psql -t -c "SELECT count(*) FROM pg_locks WHERE NOT granted;")
if [ "$LOCKS" -gt "0" ]; then
    echo "âš ï¸  $LOCKS locks en attente"
fi

echo "---"
```

#### Alertes automatiques

```python
# monitoring_alerts.py - Alertes automatiques

import psycopg2
import smtplib
from email.mime.text import MIMEText

def check_and_alert():
    """VÃ©rifie l'Ã©tat et envoie des alertes si nÃ©cessaire"""

    conn = psycopg2.connect(
        host="localhost",
        database="mydb",
        user="postgres"
    )
    cursor = conn.cursor()

    alerts = []

    # Alert 1: Connexions > 90% de max_connections
    cursor.execute("""
        SELECT
            count(*) AS current,
            (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') AS max
        FROM pg_stat_activity
    """)
    current, max_conn = cursor.fetchone()
    if current > max_conn * 0.9:
        alerts.append(f"âš ï¸  Connexions Ã©levÃ©es: {current}/{max_conn}")

    # Alert 2: Cache hit ratio < 90%
    cursor.execute("""
        SELECT
            ROUND((sum(blks_hit) * 100.0 / NULLIF(sum(blks_hit + blks_read), 0)), 2)
        FROM pg_stat_database
    """)
    cache_ratio = cursor.fetchone()[0]
    if cache_ratio < 90:
        alerts.append(f"âš ï¸  Cache hit ratio faible: {cache_ratio}%")

    # Alert 3: RequÃªtes lentes
    cursor.execute("""
        SELECT count(*)
        FROM pg_stat_activity
        WHERE state = 'active'
            AND now() - query_start > interval '5 minutes'
    """)
    long_queries = cursor.fetchone()[0]
    if long_queries > 0:
        alerts.append(f"âš ï¸  {long_queries} requÃªtes longues (> 5 min)")

    # Alert 4: Locks en attente
    cursor.execute("SELECT count(*) FROM pg_locks WHERE NOT granted")
    waiting_locks = cursor.fetchone()[0]
    if waiting_locks > 10:
        alerts.append(f"âš ï¸  {waiting_locks} locks en attente")

    # Envoyer les alertes
    if alerts:
        message = "\n".join(alerts)
        send_alert(message)
        print(message)
    else:
        print("âœ… Tout va bien")

    cursor.close()
    conn.close()

def send_alert(message):
    """Envoie une alerte par email"""
    # Configuration email
    msg = MIMEText(message)
    msg['Subject'] = 'âš ï¸  Alerte PostgreSQL 18 Migration'
    msg['From'] = 'monitoring@example.com'
    msg['To'] = 'dba@example.com'

    # Envoyer (exemple avec SMTP)
    # server = smtplib.SMTP('smtp.example.com')
    # server.send_message(msg)
    # server.quit()

if __name__ == "__main__":
    check_and_alert()
```

### Validation approfondie (T+24h Ã  T+7 jours)

#### Comparaison dÃ©taillÃ©e avec PG 17

```sql
-- Comparer les statistiques d'exÃ©cution avec PG 17
-- (nÃ©cessite d'avoir sauvegardÃ© les stats de PG 17 avant migration)

-- CrÃ©er une table pour archiver les stats PG 17
CREATE TABLE pg17_baseline AS
SELECT
    query,
    calls,
    mean_exec_time,
    total_exec_time
FROM pg_stat_statements;

-- AprÃ¨s migration vers PG 18, comparer
SELECT
    pg17.query,
    pg17.mean_exec_time AS pg17_mean_time,
    pg18.mean_exec_time AS pg18_mean_time,
    ROUND(
        ((pg18.mean_exec_time - pg17.mean_exec_time) / pg17.mean_exec_time * 100)::numeric,
        2
    ) AS percent_change
FROM pg17_baseline pg17
JOIN pg_stat_statements pg18 ON pg17.query = pg18.query
WHERE pg17.calls > 100  -- Seulement les requÃªtes frÃ©quentes
ORDER BY ABS(pg18.mean_exec_time - pg17.mean_exec_time) DESC
LIMIT 50;

-- Identifier les rÃ©gressions de performance
SELECT *
FROM (
    -- MÃªme requÃªte que ci-dessus
) AS comparison
WHERE percent_change > 20  -- > 20% plus lent
ORDER BY percent_change DESC;
```

#### Tests de rÃ©gression

```python
# regression_tests.py - Tests de rÃ©gression automatisÃ©s

import psycopg2
import json
import time

def load_baseline(filename='baseline_pg17.json'):
    """Charge les performances baseline de PG 17"""
    with open(filename, 'r') as f:
        return json.load(f)

def run_performance_tests(baseline):
    """ExÃ©cute les tests de performance et compare avec baseline"""

    conn = psycopg2.connect(
        host="localhost",
        database="mydb",
        user="postgres"
    )
    cursor = conn.cursor()

    results = []
    regressions = []

    for test in baseline['tests']:
        query = test['query']
        expected_time = test['mean_time_ms']

        # ExÃ©cuter la requÃªte 10 fois et moyenner
        times = []
        for _ in range(10):
            start = time.time()
            cursor.execute(query)
            cursor.fetchall()
            duration = (time.time() - start) * 1000  # En ms
            times.append(duration)

        actual_time = sum(times) / len(times)
        difference = ((actual_time - expected_time) / expected_time) * 100

        result = {
            'query': query[:100],
            'expected': expected_time,
            'actual': actual_time,
            'difference_pct': difference
        }
        results.append(result)

        # RÃ©gression si > 20% plus lent
        if difference > 20:
            regressions.append(result)
            print(f"âŒ RÃ‰GRESSION: {query[:50]}... ({difference:.1f}% plus lent)")
        elif difference < -10:
            print(f"âœ… AMÃ‰LIORATION: {query[:50]}... ({abs(difference):.1f}% plus rapide)")
        else:
            print(f"âœ… OK: {query[:50]}... ({difference:.1f}%)")

    # Rapport final
    print("\n" + "="*70)
    print(f"Tests total: {len(results)}")
    print(f"RÃ©gressions: {len(regressions)}")

    if regressions:
        print("\nâš ï¸  RÃ‰GRESSIONS DÃ‰TECTÃ‰ES:")
        for reg in regressions:
            print(f"  - {reg['query']}")
            print(f"    Attendu: {reg['expected']:.2f}ms")
            print(f"    Actuel: {reg['actual']:.2f}ms")
            print(f"    DiffÃ©rence: +{reg['difference_pct']:.1f}%")
    else:
        print("\nâœ… Aucune rÃ©gression dÃ©tectÃ©e")

    cursor.close()
    conn.close()

    return len(regressions) == 0

if __name__ == "__main__":
    baseline = load_baseline()
    success = run_performance_tests(baseline)
    exit(0 if success else 1)
```

## Outils de test

### pgTAP : Tests unitaires pour PostgreSQL

```sql
-- Installation
CREATE EXTENSION pgtap;

-- Exemple de tests
BEGIN;
SELECT plan(5);  -- Nombre de tests prÃ©vus

-- Test 1: La table existe
SELECT has_table('public', 'users', 'Table users should exist');

-- Test 2: La colonne existe
SELECT has_column('public', 'users', 'email', 'Column email should exist');

-- Test 3: L'index existe
SELECT has_index('public', 'users', 'idx_users_email', 'Index on email should exist');

-- Test 4: Contrainte PK
SELECT has_pk('public', 'users', 'users should have a primary key');

-- Test 5: Nombre d'enregistrements
SELECT ok(
    (SELECT count(*) FROM users) > 0,
    'users table should not be empty'
);

SELECT * FROM finish();
ROLLBACK;
```

### pgBadger : Analyse des logs

```bash
# Analyser les logs PostgreSQL 18
pgbadger /var/log/postgresql/postgresql-18-main.log \
    -o /var/www/html/pgbadger_report.html

# GÃ©nÃ©rer un rapport HTML avec :
# - RequÃªtes les plus lentes
# - RequÃªtes les plus frÃ©quentes
# - Distribution des temps de rÃ©ponse
# - Erreurs et warnings
```

### check-pgactivity : Monitoring Nagios-like

```bash
# Installer
apt-get install check-pgactivity

# VÃ©rifier les connexions
check_pgactivity -s connections --critical=90% --warning=80%

# VÃ©rifier les backends bloquÃ©s
check_pgactivity -s backends_status --critical=5 --warning=2

# VÃ©rifier le temps des transactions
check_pgactivity -s oldest_xact --critical=1h --warning=30m

# IntÃ©grer dans un systÃ¨me de monitoring (Nagios, Icinga, etc.)
```

### Scripts personnalisÃ©s

```bash
#!/bin/bash
# comprehensive_test_suite.sh
# Suite de tests complÃ¨te pour validation migration

echo "ğŸ§ª Suite de tests PostgreSQL 18 Migration"
echo "========================================="

FAILED=0

# Test 1: Version
echo "Test 1: Version PostgreSQL"
if psql -t -c "SELECT version();" | grep -q "PostgreSQL 18"; then
    echo "âœ… PASS"
else
    echo "âŒ FAIL"
    FAILED=$((FAILED + 1))
fi

# Test 2: Extensions
echo "Test 2: Extensions critiques"
for ext in pg_stat_statements uuid-ossp pgcrypto; do
    if psql -t -c "SELECT 1 FROM pg_extension WHERE extname = '$ext';" | grep -q 1; then
        echo "âœ… $ext prÃ©sent"
    else
        echo "âŒ $ext manquant"
        FAILED=$((FAILED + 1))
    fi
done

# Test 3: Tables critiques
echo "Test 3: Tables critiques"
for table in users orders products; do
    COUNT=$(psql -t -d mydb -c "SELECT count(*) FROM $table;")
    if [ "$COUNT" -gt 0 ]; then
        echo "âœ… $table: $COUNT rows"
    else
        echo "âŒ $table: vide"
        FAILED=$((FAILED + 1))
    fi
done

# Test 4: Index
echo "Test 4: Index critiques"
MISSING_INDEXES=$(psql -t -d mydb -c "
    SELECT count(*)
    FROM pg_indexes
    WHERE schemaname = 'public'
    AND indexname IN ('idx_users_email', 'idx_orders_user_id', 'idx_products_category')
")
if [ "$MISSING_INDEXES" -eq 3 ]; then
    echo "âœ… Tous les index prÃ©sents"
else
    echo "âŒ Index manquants"
    FAILED=$((FAILED + 1))
fi

# Test 5: Performance baseline
echo "Test 5: Performance baseline"
TIME=$(psql -t -d mydb -c "\timing on" -c "SELECT count(*) FROM orders;" 2>&1 | grep "Time" | awk '{print $2}')
if [ $(echo "$TIME < 1000" | bc) -eq 1 ]; then
    echo "âœ… Performance OK ($TIME ms)"
else
    echo "âš ï¸  Performance degradÃ©e ($TIME ms)"
fi

# RÃ©sumÃ©
echo ""
echo "========================================="
if [ "$FAILED" -eq 0 ]; then
    echo "âœ… SUCCÃˆS - Tous les tests sont passÃ©s"
    exit 0
else
    echo "âŒ Ã‰CHEC - $FAILED test(s) Ã©chouÃ©(s)"
    exit 1
fi
```

## Documentation des tests

### Template de rapport de test

```markdown
# Rapport de Test - Migration PostgreSQL 18

## Informations gÃ©nÃ©rales

- **Date du test** : 2024-11-23
- **Environnement** : STAGING
- **Testeur** : Jean Dupont
- **Version source** : PostgreSQL 17.2
- **Version cible** : PostgreSQL 18.0
- **Taille base** : 1.5 TB
- **MÃ©thode migration** : pg_upgrade --swap --jobs=8

## Tests prÃ©-migration

### Analyse de compatibilitÃ©
- âœ… pg_upgrade --check : OK
- âœ… Extensions compatibles : OK
- âœ… Types de donnÃ©es : OK
- âš ï¸  3 tables sans PK (corrigÃ©)

### Tests applications
- âœ… Connexion : OK
- âœ… Tests unitaires : 1247/1250 passÃ©s
- âŒ 3 tests Ã©chouÃ©s :
  - test_uuid_generation (rÃ©solu: utiliser gen_random_uuid())
  - test_json_aggregation (rÃ©solu: syntaxe JSON ajustÃ©e)
  - test_full_text_search (en cours d'investigation)

### Tests performance
- âœ… Baseline Ã©tablie
- âœ… Benchmark pgbench : +15% TPS vs PG 17

## Tests migration

### RÃ©pÃ©tition gÃ©nÃ©rale
- âœ… Migration complÃ¨te : OK
- âœ… DurÃ©e : 47 minutes
- âœ… Downtime estimÃ© : 50 minutes (avec marge)
- âœ… Rollback testÃ© : OK (< 2 minutes)

## Tests post-migration

### Validation immÃ©diate
- âœ… Version : PostgreSQL 18.0
- âœ… Connexions : OK
- âœ… IntÃ©gritÃ© donnÃ©es : OK
- âœ… Extensions : Toutes prÃ©sentes
- âœ… Smoke tests : OK

### Validation continue (24h)
- âœ… Monitoring : Aucune alerte
- âœ… Cache hit ratio : 97.5% (excellent)
- âœ… RequÃªtes lentes : Aucune rÃ©gression
- âš ï¸  1 requÃªte +12% plus lente (acceptable)

## ProblÃ¨mes rencontrÃ©s

### ProblÃ¨me 1 : Tests unitaires UUID
**Description** : 3 tests Ã©chouaient car UUID gÃ©nÃ©rÃ© diffÃ©remment
**Solution** : Utiliser gen_random_uuid() natif PG 18
**Statut** : âœ… RÃ©solu

### ProblÃ¨me 2 : Full-text search
**Description** : 1 test FTS Ã©choue
**Solution** : En cours d'investigation
**Statut** : âš ï¸  En cours

## Recommandations

1. âœ… GO pour migration production
2. âš ï¸  RÃ©soudre le problÃ¨me FTS avant (non bloquant)
3. ğŸ“Š PrÃ©voir monitoring renforcÃ© 48h post-migration
4. ğŸ”„ Conserver Blue 2 semaines minimum

## Conclusion

**Migration PostgreSQL 18 validÃ©e pour PRODUCTION**

Signature : _________________
Date : _________________
```

## Bonnes pratiques et recommandations

### Avant les tests

```
â–¡ Planifier suffisamment de temps (2-4 semaines minimum)
â–¡ Impliquer toutes les parties prenantes (dev, ops, QA, business)
â–¡ Provisionner environnements dÃ©diÃ©s (DEV, STAGING)
â–¡ Documenter l'Ã©tat actuel (baseline)
â–¡ PrÃ©parer les scripts de test automatisÃ©s
â–¡ Former l'Ã©quipe sur PostgreSQL 18 (nouveautÃ©s)
```

### Pendant les tests

```
â–¡ Commencer par DEV (itÃ©rations rapides)
â–¡ Progresser vers STAGING (tests rÃ©alistes)
â–¡ Documenter chaque problÃ¨me rencontrÃ©
â–¡ Mesurer systÃ©matiquement les performances
â–¡ Tester les cas limites et edge cases
â–¡ Valider le rollback fonctionne
```

### AprÃ¨s les tests

```
â–¡ Compiler un rapport de test complet
â–¡ Obtenir l'approbation des stakeholders
â–¡ Planifier la migration production (date, fenÃªtre)
â–¡ Communiquer largement (Ã©quipes, utilisateurs)
â–¡ PrÃ©parer le plan de communication de crise
â–¡ Documenter les procÃ©dures (runbook)
```

## Checklist complÃ¨te de validation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Checklist ComplÃ¨te - Validation Migration PostgreSQL 18    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  PHASE 1 : PRÃ‰-MIGRATION                                    â”‚
â”‚  â–¡ pg_upgrade --check : OK                                  â”‚
â”‚  â–¡ CompatibilitÃ© extensions vÃ©rifiÃ©e                        â”‚
â”‚  â–¡ CompatibilitÃ© applications testÃ©e                        â”‚
â”‚  â–¡ Performance baseline documentÃ©e                          â”‚
â”‚  â–¡ Tests de charge effectuÃ©s                                â”‚
â”‚  â–¡ Rollback testÃ© et validÃ©                                 â”‚
â”‚                                                             â”‚
â”‚  PHASE 2 : MIGRATION                                        â”‚
â”‚  â–¡ RÃ©pÃ©tition gÃ©nÃ©rale rÃ©ussie                              â”‚
â”‚  â–¡ Downtime mesurÃ© et acceptable                            â”‚
â”‚  â–¡ ProcÃ©dures documentÃ©es                                   â”‚
â”‚  â–¡ Ã‰quipe formÃ©e et prÃªte                                   â”‚
â”‚                                                             â”‚
â”‚  PHASE 3 : POST-MIGRATION                                   â”‚
â”‚  â–¡ Validation immÃ©diate (T+0) : OK                          â”‚
â”‚  â–¡ Smoke tests passÃ©s                                       â”‚
â”‚  â–¡ Monitoring actif                                         â”‚
â”‚  â–¡ Performance validÃ©e (T+24h)                              â”‚
â”‚  â–¡ Aucune rÃ©gression dÃ©tectÃ©e (T+7j)                        â”‚
â”‚  â–¡ Rapport final rÃ©digÃ©                                     â”‚
â”‚                                                             â”‚
â”‚  âœ… Migration validÃ©e pour PRODUCTION                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Conclusion

Les tests et la validation sont la **clÃ© du succÃ¨s** d'une migration PostgreSQL 18. Investir le temps nÃ©cessaire dans les tests permet d'Ã©viter les catastrophes en production.

### Points clÃ©s Ã  retenir

1. **Tester, tester, tester** : Jamais trop de tests
2. **Environnements multiples** : DEV â†’ STAGING â†’ PROD
3. **Tests automatisÃ©s** : Scripts rÃ©utilisables et fiables
4. **Documentation** : Tracer chaque Ã©tape et problÃ¨me
5. **Performance** : Comparer systÃ©matiquement avec baseline
6. **Rollback** : Toujours testÃ© avant migration prod

### Temps recommandÃ©

```
Projet petit (< 100 GB) :
- Tests DEV : 1 semaine
- Tests STAGING : 1 semaine
- Total : 2 semaines minimum

Projet moyen (100 GB - 1 TB) :
- Tests DEV : 2 semaines
- Tests STAGING : 2 semaines
- Total : 4 semaines (1 mois)

Projet large (> 1 TB) :
- Tests DEV : 3 semaines
- Tests STAGING : 3 semaines
- Total : 6-8 semaines (2 mois)
```

### Citations d'experts

> "Weeks of coding can save hours of planning."
>
> "If you don't have time to do it right, when will you have time to do it over?" - John Wooden
>
> "Test early, test often, test in production... wait, no, not that last one." - DevOps Wisdom

### Ressources complÃ©mentaires

Pour approfondir :
- **Section 19.3.1 Ã  19.3.4** : Techniques de migration avancÃ©es
- **Section 19.4** : Troubleshooting et gestion de crises
- **Section 14** : ObservabilitÃ© et monitoring
- **Section 13** : Optimisation et performance

---

**Note** : Tester une migration peut sembler long et fastidieux, mais c'est comme apprendre Ã  conduire : vous passez du temps sur le simulateur et avec un moniteur avant de conduire seul sur l'autoroute. Les tests vous donnent la confiance et l'expÃ©rience nÃ©cessaires pour rÃ©ussir la migration en production sans surprises dÃ©sagrÃ©ables. Ne sautez jamais cette Ã©tape !

â­ï¸ [Troubleshooting et Crises](/19-postgresql-en-production/04-troubleshooting-et-crises.md)
