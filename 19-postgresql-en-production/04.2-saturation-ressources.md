üîù Retour au [Sommaire](/SOMMAIRE.md)

# 19.4.2. Saturation des ressources (CPU, RAM, I/O)

## Introduction : Les trois piliers de la performance

Imaginez PostgreSQL comme une cuisine professionnelle :
- Le **CPU** est le chef cuisinier qui pr√©pare les plats (ex√©cute les requ√™tes)
- La **RAM** est le plan de travail o√π tout est √† port√©e de main (donn√©es en cache)
- L'**I/O** (disque) est le garde-manger o√π sont stock√©s tous les ingr√©dients (donn√©es persistantes)

Quand l'un de ces trois √©l√©ments sature, **toute la performance s'effondre**. Dans ce chapitre, nous allons apprendre √† diagnostiquer et r√©soudre ces saturations.

---

## Comprendre la saturation : Qu'est-ce que c'est ?

Une **saturation de ressource** se produit quand une composante du syst√®me atteint ou d√©passe sa capacit√© maximale :

- **CPU satur√©** : Le processeur travaille √† 100% et ne peut pas traiter plus de requ√™tes
- **RAM satur√©e** : La m√©moire est pleine, le syst√®me commence √† utiliser le swap (disque)
- **I/O satur√©** : Les disques ne peuvent plus traiter les lectures/√©critures assez rapidement

### Les sympt√¥mes g√©n√©raux

- ‚è±Ô∏è **Lenteur g√©n√©ralis√©e** : Toutes les requ√™tes deviennent lentes
- üìä **Timeouts** : Les applications re√ßoivent des erreurs de timeout
- üî• **Augmentation de la latence** : Ce qui prenait 10ms prend maintenant 5 secondes
- üö® **Alertes de monitoring** : Vos outils de surveillance se d√©clenchent

---

## Partie 1 : Saturation CPU

### Qu'est-ce que le CPU fait dans PostgreSQL ?

Le CPU ex√©cute toutes les op√©rations de calcul :
- Analyse des requ√™tes (parsing)
- Planification des requ√™tes (query planning)
- Ex√©cution des jointures, tris, agr√©gations
- Compression/d√©compression des donn√©es
- Calculs sur les index
- Traitement des fonctions PL/pgSQL

### Sympt√¥mes d'une saturation CPU

```bash
# V√©rifier l'utilisation CPU globale du syst√®me
top
# ou
htop

# V√©rifier l'utilisation CPU des processus PostgreSQL
ps aux | grep postgres | head -20
```

**Signes d'alerte** :
- CPU constamment au-dessus de 80-90% d'utilisation
- Processus PostgreSQL consomment la majorit√© du CPU
- Load average √©lev√© (sup√©rieur au nombre de c≈ìurs)

### Diagnostiquer : Identifier les requ√™tes gourmandes en CPU

#### M√©thode 1 : Via pg_stat_activity (√©tat en temps r√©el)

```sql
SELECT
    pid,
    usename,
    state,
    query,
    (now() - query_start) AS duration
FROM pg_stat_activity
WHERE state = 'active'
  AND query NOT LIKE '%pg_stat_activity%'
ORDER BY query_start
LIMIT 10;
```

**Explication** :
- Les requ√™tes qui tournent depuis longtemps (`duration` √©lev√©e) sont suspectes
- Si `state = 'active'`, la requ√™te consomme actuellement du CPU
- Exclure les requ√™tes de monitoring (`pg_stat_activity`) pour √©viter la r√©cursion

#### M√©thode 2 : Via pg_stat_statements (vue historique)

```sql
-- Installation de l'extension (√† faire une seule fois)
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Trouver les requ√™tes les plus gourmandes en CPU
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    max_exec_time,
    stddev_exec_time,
    rows
FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 10;
```

**Colonnes importantes** :
- `total_exec_time` : Temps CPU total cumul√© (en millisecondes)
- `mean_exec_time` : Temps moyen par ex√©cution
- `calls` : Nombre de fois que la requ√™te a √©t√© ex√©cut√©e
- `max_exec_time` : Pire cas d'ex√©cution

**Interpr√©ter les r√©sultats** :
- **Haute `total_exec_time` + basse `mean_exec_time`** ‚Üí Requ√™te tr√®s fr√©quente, optimiser pour r√©duire les appels
- **Haute `mean_exec_time`** ‚Üí Requ√™te lente, optimiser la requ√™te elle-m√™me
- **Haute `max_exec_time`** ‚Üí Comportement erratique, investiguer les cas extr√™mes

#### M√©thode 3 : Via l'extension pg_stat_kcache (m√©triques syst√®me)

```sql
-- Installation (n√©cessite compilation ou package syst√®me)
CREATE EXTENSION IF NOT EXISTS pg_stat_kcache;

-- Requ√™tes avec plus de d√©tails CPU syst√®me
SELECT
    pss.query,
    pss.calls,
    pss.total_exec_time,
    psk.user_time,    -- Temps CPU en mode utilisateur
    psk.system_time,  -- Temps CPU en mode noyau
    psk.reads,        -- Lectures disque
    psk.writes        -- √âcritures disque
FROM pg_stat_statements pss
JOIN pg_stat_kcache psk USING (userid, dbid, queryid)
ORDER BY psk.user_time + psk.system_time DESC
LIMIT 10;
```

### R√©soudre une saturation CPU

#### Solution 1 : Optimiser les requ√™tes lentes

```sql
-- Analyser une requ√™te sp√©cifique
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM orders
WHERE user_id = 12345 AND status = 'pending';
```

**Points d'attention dans EXPLAIN** :
- **Seq Scan** sur grande table ‚Üí Ajouter un index
- **Nested Loop** co√ªteux ‚Üí V√©rifier les jointures
- **Sort** avec grande quantit√© de donn√©es ‚Üí Augmenter `work_mem` ou ajouter index

#### Solution 2 : Ajouter des index manquants

```sql
-- Identifier les tables sans index bien utilis√©s
SELECT
    schemaname,
    tablename,
    seq_scan,              -- Nombre de scans s√©quentiels
    seq_tup_read,          -- Lignes lues en scan s√©quentiel
    idx_scan,              -- Nombre de scans d'index
    seq_tup_read / NULLIF(seq_scan, 0) AS avg_seq_tup
FROM pg_stat_user_tables
WHERE seq_scan > 0
ORDER BY seq_tup_read DESC
LIMIT 10;
```

**Interpr√©tation** :
- `seq_scan` √©lev√© avec grande table ‚Üí Index manquant probable
- `seq_tup_read / seq_scan` > 1000 ‚Üí Chaque scan lit beaucoup de lignes, tr√®s inefficace

**Cr√©er un index** :
```sql
-- Exemple : index sur colonne fr√©quemment filtr√©e
CREATE INDEX CONCURRENTLY idx_orders_user_id
ON orders(user_id);

-- Index compos√© pour requ√™tes multi-colonnes
CREATE INDEX CONCURRENTLY idx_orders_user_status
ON orders(user_id, status);
```

**Note** : `CONCURRENTLY` permet de cr√©er l'index sans bloquer les √©critures (mais prend plus de temps).

#### Solution 3 : Limiter les connexions actives

```sql
-- Voir le nombre de connexions actives
SELECT
    state,
    count(*)
FROM pg_stat_activity
GROUP BY state;
```

Si trop de connexions sont actives (ex: 300 connexions pour 4 c≈ìurs CPU) :

```sql
-- Dans postgresql.conf, limiter les connexions
max_connections = 100

-- Utiliser un connection pooler (PgBouncer)
-- PgBouncer peut g√©rer 10000 connexions applicatives
-- et n'utiliser que 100 connexions PostgreSQL r√©elles
```

#### Solution 4 : Utiliser le parall√©lisme PostgreSQL

PostgreSQL peut parall√©liser certaines requ√™tes sur plusieurs c≈ìurs :

```sql
-- Activer le parall√©lisme (d√©j√† activ√© par d√©faut en g√©n√©ral)
SET max_parallel_workers_per_gather = 4;
SET max_parallel_workers = 8;

-- V√©rifier si une requ√™te utilise le parall√©lisme
EXPLAIN SELECT count(*) FROM large_table;
-- Recherchez "Parallel Seq Scan" dans le plan
```

**Attention** : Le parall√©lisme consomme plus de ressources, √† utiliser judicieusement.

---

## Partie 2 : Saturation RAM (M√©moire)

### R√¥le de la RAM dans PostgreSQL

La RAM sert principalement √† :
1. **Cache des donn√©es** (`shared_buffers`) : √âvite les lectures disque r√©p√©t√©es
2. **M√©moire de travail** (`work_mem`) : Pour tris, hash tables, jointures
3. **Maintenance** (`maintenance_work_mem`) : Pour VACUUM, CREATE INDEX
4. **Connexions** : Chaque connexion consomme de la RAM

### Sympt√¥mes d'une saturation RAM

```bash
# V√©rifier l'utilisation m√©moire globale
free -h
# ou
vmstat 1 5

# V√©rifier si le syst√®me utilise le swap (TR√àS MAUVAIS pour PostgreSQL)
cat /proc/meminfo | grep -i swap
```

**Signes d'alerte** :
- M√©moire disponible < 10% de la RAM totale
- **Swap utilis√©** (signe critique de saturation)
- OOM Killer (Out Of Memory) tue des processus PostgreSQL
- Ralentissement soudain et g√©n√©ralis√©

### Diagnostiquer : Identifier la consommation m√©moire

#### M√©thode 1 : Param√®tres de configuration PostgreSQL

```sql
-- Voir la configuration m√©moire actuelle
SHOW shared_buffers;
SHOW work_mem;
SHOW maintenance_work_mem;
SHOW effective_cache_size;

-- Version d√©taill√©e
SELECT
    name,
    setting,
    unit,
    context
FROM pg_settings
WHERE name IN ('shared_buffers', 'work_mem', 'maintenance_work_mem',
               'effective_cache_size', 'max_connections');
```

**R√®gles empiriques** (pour un serveur d√©di√© √† PostgreSQL) :
- `shared_buffers` : 25% de la RAM totale (ex: 8GB pour 32GB de RAM)
- `effective_cache_size` : 50-75% de la RAM totale
- `work_mem` : RAM / (max_connections * 2 √† 4)
- `maintenance_work_mem` : 512MB √† 2GB

#### M√©thode 2 : Identifier les requ√™tes gourmandes en m√©moire

```sql
-- Requ√™tes utilisant beaucoup de work_mem
-- (n√©cessite log_temp_files configur√©)
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    rows
FROM pg_stat_statements
WHERE query LIKE '%ORDER BY%'
   OR query LIKE '%GROUP BY%'
   OR query LIKE '%DISTINCT%'
ORDER BY mean_exec_time DESC
LIMIT 10;
```

Ces types de requ√™tes utilisent `work_mem` pour :
- Tris (`ORDER BY`)
- Agr√©gations (`GROUP BY`)
- Hash joins
- Op√©rations `DISTINCT`

#### M√©thode 3 : V√©rifier l'efficacit√© du cache

```sql
-- Cache hit ratio (doit √™tre > 95%)
SELECT
    sum(heap_blks_read) AS heap_read,
    sum(heap_blks_hit) AS heap_hit,
    round(
        sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit) + sum(heap_blks_read), 0) * 100,
        2
    ) AS cache_hit_ratio
FROM pg_statio_user_tables;
```

**Interpr√©tation** :
- **> 99%** : Excellent, presque tout est en cache
- **95-99%** : Bon, acceptable
- **< 95%** : Probl√®me, trop de lectures disque, augmentez `shared_buffers` ou optimisez les requ√™tes

#### M√©thode 4 : Analyser les temporary files

Quand `work_mem` est insuffisant, PostgreSQL √©crit temporairement sur disque :

```sql
-- Dans postgresql.conf, activer la journalisation
log_temp_files = 0  -- Log tous les fichiers temporaires

-- Puis consulter les logs PostgreSQL
-- Exemple de ligne de log :
-- LOG: temporary file: path "base/pgsql_tmp/pgsql_tmp12345.0", size 52428800
```

**Requ√™te pour trouver les bases avec beaucoup de fichiers temporaires** :

```sql
SELECT
    datname,
    temp_files,
    temp_bytes,
    pg_size_pretty(temp_bytes) AS temp_size
FROM pg_stat_database
WHERE temp_files > 0
ORDER BY temp_bytes DESC;
```

Si `temp_files` et `temp_bytes` sont √©lev√©s ‚Üí augmentez `work_mem`.

### R√©soudre une saturation RAM

#### Solution 1 : Augmenter shared_buffers

```sql
-- Dans postgresql.conf
shared_buffers = 8GB  -- Environ 25% de la RAM

-- Red√©marrage n√©cessaire
-- sudo systemctl restart postgresql
```

**Attention** : Ne pas d√©passer 40% de la RAM pour `shared_buffers`. Au-del√†, la performance se d√©grade.

#### Solution 2 : Augmenter work_mem (avec pr√©caution)

```sql
-- Global (dans postgresql.conf)
work_mem = 64MB

-- Ou par session pour requ√™tes sp√©cifiques
SET work_mem = '256MB';
SELECT * FROM large_table ORDER BY created_at;
RESET work_mem;

-- Ou par transaction
BEGIN;
SET LOCAL work_mem = '512MB';
-- Requ√™te lourde ici
COMMIT;
```

**Calcul important** :
```
M√©moire potentielle = work_mem √ó max_connections √ó 2 √† 4
```

Si `work_mem = 256MB` et `max_connections = 200` :
```
Consommation possible = 256MB √ó 200 √ó 3 = 150GB !
```

‚ö†Ô∏è **Risque d'OOM** ! Augmentez `work_mem` progressivement et surveillez.

#### Solution 3 : Optimiser les requ√™tes pour r√©duire l'usage m√©moire

```sql
-- ‚ùå MAUVAIS : Tri en m√©moire de toute la table
SELECT * FROM logs ORDER BY created_at DESC;

-- ‚úÖ BON : Utiliser un index pour √©viter le tri
CREATE INDEX idx_logs_created ON logs(created_at DESC);
SELECT * FROM logs ORDER BY created_at DESC;

-- ‚ùå MAUVAIS : Distinct sur grande table sans index
SELECT DISTINCT user_id FROM events;

-- ‚úÖ BON : Utiliser GROUP BY avec index
SELECT user_id FROM events GROUP BY user_id;
```

#### Solution 4 : R√©duire le nombre de connexions

Chaque connexion consomme de la RAM (~10MB par connexion) :

```sql
-- Voir les connexions actives et leur √©tat
SELECT
    datname,
    state,
    count(*)
FROM pg_stat_activity
GROUP BY datname, state;

-- Fermer les connexions inactives
-- Configurer dans postgresql.conf
idle_in_transaction_session_timeout = 60000  -- 60 secondes
```

**Utiliser PgBouncer** :
```ini
# pgbouncer.ini
[databases]
mydb = host=localhost port=5432 dbname=mydb

[pgbouncer]
pool_mode = transaction
max_client_conn = 10000
default_pool_size = 25
```

#### Solution 5 : D√©sactiver le swap pour PostgreSQL

Le swap tue la performance de PostgreSQL :

```bash
# V√©rifier l'utilisation du swap
swapon --show

# D√©sactiver le swap (temporaire)
sudo swapoff -a

# D√©sactiver d√©finitivement (dans /etc/fstab, commenter la ligne swap)
```

**Alternative** : Configurer le syst√®me pour minimiser le swap :

```bash
# R√©duire la tendance √† swapper (vm.swappiness)
sudo sysctl vm.swappiness=1
echo "vm.swappiness = 1" | sudo tee -a /etc/sysctl.conf
```

---

## Partie 3 : Saturation I/O (Disque)

### R√¥le de l'I/O dans PostgreSQL

Les disques stockent :
1. **Les donn√©es** : Tables, index, TOAST
2. **Les WAL** (Write-Ahead Logs) : Journal des transactions
3. **Les fichiers temporaires** : Quand `work_mem` est d√©pass√©

### Types d'op√©rations I/O

- **Lectures** : Acc√®s aux tables et index non en cache
- **√âcritures** : INSERT, UPDATE, DELETE, VACUUM, checkpoints
- **WAL** : √âcriture continue du journal transactionnel
- **Synchronisation** : fsync, sync des donn√©es sur disque

### Sympt√¥mes d'une saturation I/O

```bash
# V√©rifier les I/O par disque
iostat -x 1 5
# Colonnes importantes :
# - %util : Utilisation du disque (> 80% = satur√©)
# - await : Latence moyenne (> 10ms = lent)
# - r/s, w/s : Op√©rations lecture/√©criture par seconde

# Voir les processus gourmands en I/O
iotop

# Statistiques de disque d√©taill√©es
dstat --disk --io --filesystem
```

**Signes d'alerte** :
- `%util` constamment > 80-90%
- Latence d'I/O (`await`) > 10-20ms pour SSD, > 50ms pour HDD
- Queue depth √©lev√©e (beaucoup d'op√©rations en attente)

### Diagnostiquer : Identifier les sources d'I/O

#### M√©thode 1 : Via pg_stat_database (statistiques globales)

```sql
SELECT
    datname,
    blks_read,          -- Blocs lus depuis le disque
    blks_hit,           -- Blocs lus depuis le cache
    tup_returned,       -- Lignes retourn√©es
    tup_fetched,        -- Lignes r√©ellement r√©cup√©r√©es
    tup_inserted,       -- Insertions
    tup_updated,        -- Mises √† jour
    tup_deleted,        -- Suppressions
    blk_read_time,      -- Temps pass√© √† lire (si track_io_timing = on)
    blk_write_time      -- Temps pass√© √† √©crire
FROM pg_stat_database
WHERE datname = current_database();
```

**Activer le tracking du temps I/O** :
```sql
-- Dans postgresql.conf
track_io_timing = on
-- Puis recharger : SELECT pg_reload_conf();
```

#### M√©thode 2 : Via pg_statio_user_tables (I/O par table)

```sql
SELECT
    schemaname,
    tablename,
    heap_blks_read,     -- Blocs lus depuis disque
    heap_blks_hit,      -- Blocs lus depuis cache
    idx_blks_read,      -- Index lus depuis disque
    idx_blks_hit,       -- Index lus depuis cache
    toast_blks_read,    -- TOAST lu depuis disque
    toast_blks_hit,     -- TOAST lu depuis cache
    CASE
        WHEN heap_blks_read + heap_blks_hit > 0
        THEN round(100.0 * heap_blks_hit / (heap_blks_read + heap_blks_hit), 2)
        ELSE 0
    END AS cache_hit_ratio
FROM pg_statio_user_tables
ORDER BY heap_blks_read DESC
LIMIT 10;
```

**Interpr√©tation** :
- Tables avec `heap_blks_read` √©lev√© ‚Üí Causent beaucoup de lectures disque
- `cache_hit_ratio` < 95% ‚Üí Probl√®me de cache, consid√©rez augmenter `shared_buffers`

#### M√©thode 3 : Via pg_stat_statements avec I/O timing

```sql
SELECT
    query,
    calls,
    total_exec_time,
    blk_read_time,      -- Temps I/O lecture (ms)
    blk_write_time,     -- Temps I/O √©criture (ms)
    shared_blks_read,   -- Blocs lus depuis disque
    shared_blks_hit,    -- Blocs trouv√©s en cache
    round(
        100.0 * shared_blks_hit / NULLIF(shared_blks_read + shared_blks_hit, 0),
        2
    ) AS cache_hit_ratio
FROM pg_stat_statements
WHERE blk_read_time > 0
ORDER BY blk_read_time + blk_write_time DESC
LIMIT 10;
```

Les requ√™tes avec `blk_read_time` √©lev√© sont celles qui attendent le disque.

#### M√©thode 4 : Analyser les checkpoints

Les checkpoints synchronisent la m√©moire vers le disque et peuvent causer des pics d'I/O :

```sql
-- Voir les statistiques de checkpoints
SELECT
    checkpoints_timed,      -- Checkpoints planifi√©s
    checkpoints_req,        -- Checkpoints forc√©s (mauvais signe)
    checkpoint_write_time,  -- Temps d'√©criture (ms)
    checkpoint_sync_time,   -- Temps de sync (ms)
    buffers_checkpoint,     -- Buffers √©crits
    buffers_clean,          -- Buffers nettoy√©s par bgwriter
    buffers_backend,        -- Buffers √©crits directement par backends
    buffers_backend_fsync   -- fsync par backends (tr√®s mauvais)
FROM pg_stat_bgwriter;
```

**Analyser les r√©sultats** :
- `checkpoints_req` √©lev√© ‚Üí Checkpoints trop fr√©quents, augmentez `max_wal_size`
- `buffers_backend_fsync` > 0 ‚Üí **Probl√®me grave**, backends attendent le disque

**Dans les logs PostgreSQL** (avec `log_checkpoints = on`) :
```
LOG:  checkpoint starting: time
LOG:  checkpoint complete: wrote 16384 buffers (25.0%);
      0 WAL file(s) added, 0 removed, 5 recycled;
      write=25.123 s, sync=0.456 s, total=25.634 s;
      sync files=165, longest=0.123 s, average=0.003 s
```

#### M√©thode 5 : Surveiller le WAL

Le WAL (Write-Ahead Log) est critique pour les performances d'√©criture :

```sql
-- Volume de WAL g√©n√©r√©
SELECT
    pg_current_wal_lsn(),
    pg_walfile_name(pg_current_wal_lsn());

-- Nouveaut√© PG 18 : Statistiques I/O et WAL par backend
SELECT
    pid,
    usename,
    query,
    backend_type,
    wal_bytes,          -- Octets de WAL g√©n√©r√©s
    wal_records,        -- Nombre d'enregistrements WAL
    wal_fpi             -- Full Page Images
FROM pg_stat_activity
WHERE backend_type = 'client backend'
ORDER BY wal_bytes DESC
LIMIT 10;
```

### R√©soudre une saturation I/O

#### Solution 1 : Augmenter le cache pour r√©duire les lectures

```sql
-- Augmenter shared_buffers (n√©cessite red√©marrage)
-- Dans postgresql.conf
shared_buffers = 16GB  -- Ajustez selon votre RAM
```

#### Solution 2 : Optimiser les checkpoints

```sql
-- Dans postgresql.conf
max_wal_size = 4GB              -- Augmente l'intervalle entre checkpoints
checkpoint_timeout = 15min      -- Augmente le d√©lai maximum
checkpoint_completion_target = 0.9  -- √âtaler les √©critures sur 90% de l'intervalle
```

**Effet** : Checkpoints moins fr√©quents et plus √©tal√©s = moins de pics d'I/O.

#### Solution 3 : S√©parer les WAL sur un disque d√©di√©

Si possible, placez les WAL sur un disque SSD s√©par√© :

```bash
# D√©placer le r√©pertoire pg_wal
# 1. Arr√™ter PostgreSQL
sudo systemctl stop postgresql

# 2. D√©placer pg_wal vers disque rapide
sudo mv /var/lib/postgresql/data/pg_wal /mnt/fast-ssd/pg_wal

# 3. Cr√©er un lien symbolique
sudo ln -s /mnt/fast-ssd/pg_wal /var/lib/postgresql/data/pg_wal

# 4. Red√©marrer PostgreSQL
sudo systemctl start postgresql
```

#### Solution 4 : Utiliser FILLFACTOR pour r√©duire les UPDATE

Les `UPDATE` en PostgreSQL cr√©ent de nouvelles versions de lignes. Avec `FILLFACTOR`, vous r√©servez de l'espace pour ces mises √† jour :

```sql
-- Table avec beaucoup d'UPDATE
ALTER TABLE users SET (fillfactor = 80);

-- Reconstruire la table pour appliquer
VACUUM FULL users;
-- ou
CLUSTER users USING users_pkey;
```

**Explication** : Chaque page de donn√©es gardera 20% d'espace libre pour les futures mises √† jour, r√©duisant le besoin de cr√©er de nouvelles pages.

#### Solution 5 : Partitionnement pour distribuer l'I/O

Partitionner de grandes tables peut am√©liorer les performances I/O :

```sql
-- Exemple : Partitionnement par date
CREATE TABLE logs (
    id SERIAL,
    message TEXT,
    created_at TIMESTAMP
) PARTITION BY RANGE (created_at);

-- Cr√©er des partitions
CREATE TABLE logs_2024_11 PARTITION OF logs
    FOR VALUES FROM ('2024-11-01') TO ('2024-12-01');

CREATE TABLE logs_2024_12 PARTITION OF logs
    FOR VALUES FROM ('2024-12-01') TO ('2025-01-01');
```

**Avantage** : Les requ√™tes sur une seule partition lisent moins de donn√©es.

#### Solution 6 : Utiliser des tablespaces

Distribuez les tables sur diff√©rents disques physiques :

```sql
-- Cr√©er un tablespace sur un disque rapide
CREATE TABLESPACE fast_storage
    LOCATION '/mnt/nvme-ssd/pgdata';

-- D√©placer une table gourmande en I/O
ALTER TABLE large_table SET TABLESPACE fast_storage;

-- D√©placer les index d'une table
ALTER INDEX large_table_pkey SET TABLESPACE fast_storage;
```

#### Solution 7 : Compression et TOAST

Pour r√©duire les I/O, compressez les donn√©es volumineuses :

```sql
-- Activer la compression pour colonne volumineuse
ALTER TABLE documents
    ALTER COLUMN content SET STORAGE EXTENDED;  -- Compression + TOAST

-- Ou compression uniquement
ALTER TABLE documents
    ALTER COLUMN content SET STORAGE MAIN;  -- Compression, pas de TOAST
```

**Nouveaut√© PG 18** : Algorithmes de compression am√©lior√©s.

#### Solution 8 : VACUUM et maintenance r√©guli√®re

Un `VACUUM` r√©gulier r√©duit le bloat et am√©liore l'I/O :

```sql
-- VACUUM manuel sur table probl√©matique
VACUUM ANALYZE large_table;

-- V√©rifier l'autovacuum
SELECT
    schemaname,
    tablename,
    last_vacuum,
    last_autovacuum,
    n_dead_tup,     -- Lignes mortes
    n_live_tup      -- Lignes vivantes
FROM pg_stat_user_tables
WHERE n_dead_tup > 10000
ORDER BY n_dead_tup DESC;
```

**Configurer autovacuum plus agressif** :

```sql
-- Dans postgresql.conf
autovacuum_max_workers = 6
autovacuum_naptime = 10s  -- V√©rifier toutes les 10 secondes

-- Nouveaut√© PG 18 : Ajustement dynamique
autovacuum_vacuum_max_threshold = 1000000
```

#### Solution 9 : Upgrade vers SSD NVMe

Si vous √™tes sur HDD :
- **HDD** : ~100-200 IOPS, latence 5-10ms
- **SSD SATA** : ~10,000-50,000 IOPS, latence < 1ms
- **SSD NVMe** : ~100,000-1,000,000 IOPS, latence < 0.1ms

Un upgrade mat√©riel peut r√©soudre radicalement les probl√®mes d'I/O.

#### Solution 10 : Activer l'I/O asynchrone (Nouveaut√© PG 18)

```sql
-- Dans postgresql.conf
io_method = 'async'  -- Utilise les I/O asynchrones (AIO)

-- Red√©marrage n√©cessaire
```

**Avantage** : Jusqu'√† **3√ó plus rapide** pour les op√©rations I/O intensives.

---

## Surveiller les ressources en temps r√©el

### Script de monitoring global

Voici un script SQL utile pour avoir une vue d'ensemble :

```sql
-- Vue globale : CPU, RAM, I/O
WITH cpu_stats AS (
    SELECT
        count(*) FILTER (WHERE state = 'active') AS active_queries,
        count(*) AS total_connections,
        max(EXTRACT(EPOCH FROM (now() - query_start))) AS longest_query_sec
    FROM pg_stat_activity
    WHERE pid != pg_backend_pid()
),
cache_stats AS (
    SELECT
        round(
            100.0 * sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit) + sum(heap_blks_read), 0),
            2
        ) AS cache_hit_ratio
    FROM pg_statio_user_tables
),
io_stats AS (
    SELECT
        sum(blk_read_time) AS total_read_time_ms,
        sum(blk_write_time) AS total_write_time_ms
    FROM pg_stat_statements
)
SELECT
    cpu.active_queries,
    cpu.total_connections,
    round(cpu.longest_query_sec::numeric, 2) AS longest_query_sec,
    cache.cache_hit_ratio,
    io.total_read_time_ms,
    io.total_write_time_ms,
    pg_size_pretty(pg_database_size(current_database())) AS db_size
FROM cpu_stats cpu, cache_stats cache, io_stats io;
```

### Outils de monitoring externes

#### 1. Prometheus + Grafana

```bash
# Installer postgres_exporter
wget https://github.com/prometheus-community/postgres_exporter/releases/download/v0.15.0/postgres_exporter-0.15.0.linux-amd64.tar.gz
tar xvfz postgres_exporter-0.15.0.linux-amd64.tar.gz
cd postgres_exporter-0.15.0.linux-amd64

# Configurer la connexion
export DATA_SOURCE_NAME="postgresql://user:password@localhost:5432/postgres?sslmode=disable"

# Lancer
./postgres_exporter
```

**Dashboards Grafana recommand√©s** :
- PostgreSQL Database (ID: 9628)
- PostgreSQL Overview (ID: 12485)

#### 2. pgBadger (Analyse de logs)

```bash
# Installer pgBadger
sudo apt install pgbadger

# Configurer les logs dans postgresql.conf
log_min_duration_statement = 0  # Log toutes les requ√™tes
log_line_prefix = '%t [%p]: user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0

# Analyser les logs
pgbadger /var/log/postgresql/postgresql-*.log -o /tmp/report.html

# Ouvrir le rapport
firefox /tmp/report.html
```

#### 3. pg_top (Monitoring en temps r√©el)

```bash
# Installer pg_top
sudo apt install pgtop

# Lancer
pg_top -h localhost -U postgres -d mydb

# Interface interactive comme 'top' pour PostgreSQL
```

---

## Tableau r√©capitulatif : Sympt√¥mes et solutions

| Ressource | Sympt√¥me principal | Diagnostic | Solution rapide |
|-----------|-------------------|------------|-----------------|
| **CPU** | Lenteur g√©n√©ralis√©e, CPU > 90% | `pg_stat_statements` ordre par `total_exec_time` | Optimiser requ√™tes lentes, ajouter index |
| **RAM** | Swap utilis√©, OOM killer | `free -h`, cache hit ratio < 95% | Augmenter `shared_buffers`, optimiser `work_mem` |
| **I/O** | Latence √©lev√©e, `%util` > 80% | `iostat -x`, `pg_statio_user_tables` | Augmenter cache, optimiser checkpoints, SSD |

---

## Configuration de r√©f√©rence pour √©viter les saturations

### Pour un serveur avec 32GB RAM, 8 c≈ìurs CPU, SSD

```ini
# postgresql.conf - Configuration optimis√©e

# CONNEXIONS
max_connections = 100
superuser_reserved_connections = 5

# M√âMOIRE
shared_buffers = 8GB                    # 25% de RAM
work_mem = 64MB                         # √Ä ajuster selon workload
maintenance_work_mem = 2GB              # Pour VACUUM, CREATE INDEX
effective_cache_size = 24GB             # 75% de RAM

# WAL
wal_buffers = 16MB
max_wal_size = 4GB
min_wal_size = 1GB
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9

# I/O
effective_io_concurrency = 200          # Pour SSD
random_page_cost = 1.1                  # Pour SSD (4.0 pour HDD)
io_method = 'async'                     # Nouveaut√© PG 18

# QUERY TUNING
default_statistics_target = 100
track_io_timing = on

# AUTOVACUUM
autovacuum = on
autovacuum_max_workers = 4
autovacuum_naptime = 10s

# LOGGING (pour debugging)
log_min_duration_statement = 1000       # Log requ√™tes > 1s
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0
```

---

## Checklist de diagnostic rapide

Quand un probl√®me de performance survient, suivez ces √©tapes :

### 1. Identifier la ressource satur√©e (2 minutes)

```bash
# CPU
top
ps aux | grep postgres | head -10

# RAM
free -h
cat /proc/meminfo | grep -i swap

# I/O
iostat -x 1 3
```

### 2. Identifier les requ√™tes probl√©matiques (5 minutes)

```sql
-- Requ√™tes actives longues
SELECT pid, usename, query, (now() - query_start) AS duration
FROM pg_stat_activity
WHERE state = 'active' AND query NOT LIKE '%pg_stat%'
ORDER BY duration DESC;

-- Top requ√™tes cumul√©es
SELECT query, calls, total_exec_time, mean_exec_time
FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 5;
```

### 3. V√©rifier la sant√© g√©n√©rale (3 minutes)

```sql
-- Cache hit ratio
SELECT round(
    100.0 * sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit) + sum(heap_blks_read), 0),
    2
) AS cache_hit_ratio
FROM pg_statio_user_tables;

-- Connexions
SELECT count(*), state FROM pg_stat_activity GROUP BY state;

-- Locks bloquants
SELECT count(*) FROM pg_stat_activity
WHERE cardinality(pg_blocking_pids(pid)) > 0;
```

### 4. Analyser et agir (temps variable)

- **Si CPU satur√©** ‚Üí Analyser EXPLAIN, ajouter index
- **Si RAM satur√©e** ‚Üí V√©rifier swap, ajuster `work_mem`, limiter connexions
- **Si I/O satur√©** ‚Üí Augmenter cache, optimiser checkpoints, consid√©rer SSD

---

## Bonnes pratiques de pr√©vention

### 1. Monitoring proactif

Mettez en place des alertes **avant** que les probl√®mes surviennent :

- CPU > 80% pendant 5 minutes
- RAM disponible < 15%
- Swap utilis√© > 0
- Cache hit ratio < 95%
- Latence I/O > 10ms pour SSD

### 2. Tests de charge r√©guliers

```bash
# Utiliser pgbench pour tester la capacit√©
pgbench -i -s 100 mydb  # Initialiser
pgbench -c 50 -j 4 -T 300 mydb  # 50 clients, 4 threads, 5 minutes
```

### 3. Revue de configuration trimestrielle

√Ä mesure que votre application grandit, r√©√©valuez :
- `max_connections`
- `shared_buffers`
- `work_mem`
- `max_wal_size`

### 4. Maintenance r√©guli√®re

```sql
-- Planifier avec pg_cron
CREATE EXTENSION pg_cron;

-- VACUUM quotidien des grandes tables
SELECT cron.schedule('vacuum-logs', '0 2 * * *', 'VACUUM ANALYZE logs');

-- R√©indexer hebdomadairement
SELECT cron.schedule('reindex', '0 3 * * 0', 'REINDEX DATABASE mydb');
```

---

## Ressources compl√©mentaires

- **Documentation PostgreSQL** : [Resource Consumption](https://www.postgresql.org/docs/current/runtime-config-resource.html)
- **PGTune** : [pgtune.leopard.in.ua](https://pgtune.leopard.in.ua/) - G√©n√®re une configuration optimale
- **explain.depesz.com** : Analyseur visuel de plans EXPLAIN
- **Wiki PostgreSQL** : [Tuning Your PostgreSQL Server](https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server)

---

## R√©sum√© des points cl√©s

‚úÖ **CPU, RAM, I/O** : Les trois piliers de la performance PostgreSQL

‚úÖ **Diagnostiquer avant d'agir** : Utilisez `pg_stat_statements`, `pg_stat_activity`, `iostat`

‚úÖ **Cache hit ratio** : Doit √™tre > 95%, sinon augmentez `shared_buffers`

‚úÖ **work_mem** : Ajustez avec pr√©caution, peut exploser la consommation RAM

‚úÖ **Checkpoints** : √âtalez-les (`max_wal_size`, `checkpoint_completion_target`)

‚úÖ **I/O asynchrone (PG 18)** : Activation simple, gains majeurs

‚úÖ **Swap = ennemi** : D√©sactivez-le ou minimisez-le (`vm.swappiness=1`)

‚úÖ **Monitoring continu** : Prometheus, Grafana, pgBadger pour anticiper les probl√®mes

‚úÖ **Tests de charge** : Validez votre configuration sous charge r√©aliste

---

## Conclusion

La saturation des ressources est l'une des causes les plus fr√©quentes de d√©gradation de performance dans PostgreSQL. En comprenant comment diagnostiquer et r√©soudre les saturations de **CPU**, **RAM** et **I/O**, vous √™tes maintenant √©quip√© pour :

1. **Identifier rapidement** la ressource probl√©matique
2. **Diagnostiquer** les requ√™tes ou configurations responsables
3. **Appliquer les solutions** appropri√©es (optimisation, configuration, mat√©riel)
4. **Pr√©venir** les futurs probl√®mes par monitoring et maintenance

N'oubliez pas : **mesurez avant d'optimiser**, et **surveillez apr√®s avoir chang√©**. Les performances de base de donn√©es sont un processus it√©ratif d'am√©lioration continue.

---

**Prochaine √©tape** : 19.4.3 - Transaction Wraparound (XID exhaustion) et pr√©vention

---


‚è≠Ô∏è [Transaction Wraparound (XID exhaustion) et pr√©vention](/19-postgresql-en-production/04.3-transaction-wraparound.md)
