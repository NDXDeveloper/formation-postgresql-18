ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 21.2.4 Columnar Storage : Hydra et pg_analytics

## Introduction

Les bases de donnÃ©es traditionnelles, dont PostgreSQL, stockent les donnÃ©es **ligne par ligne** (row-oriented). Cette approche est optimale pour les opÃ©rations transactionnelles (OLTP) mais devient inefficace pour l'analytique sur de grands volumes de donnÃ©es.

Le **stockage colonnaire** (columnar storage) offre une alternative rÃ©volutionnaire pour les workloads analytiques (OLAP). Ce chapitre explore comment PostgreSQL s'enrichit de capacitÃ©s colonnaires grÃ¢ce Ã  des extensions comme Hydra et pg_analytics, transformant PostgreSQL en une plateforme unifiÃ©e pour les transactions ET l'analytique.

---

## Partie 1 : Comprendre le Stockage Colonnaire

### 1.1 Row Store vs Column Store

La diffÃ©rence fondamentale rÃ©side dans l'organisation physique des donnÃ©es sur le disque.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Row Store (PostgreSQL Standard)                      â”‚
â”‚                                                                         â”‚
â”‚   Table "ventes" :                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚   id   â”‚    date    â”‚ produit  â”‚ quantitÃ©â”‚  montant â”‚               â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚   â”‚   1    â”‚ 2025-01-15 â”‚  Widget  â”‚    10   â”‚   99.90  â”‚               â”‚
â”‚   â”‚   2    â”‚ 2025-01-15 â”‚  Gadget  â”‚     5   â”‚  149.95  â”‚               â”‚
â”‚   â”‚   3    â”‚ 2025-01-16 â”‚  Widget  â”‚     8   â”‚   79.92  â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                         â”‚
â”‚   Stockage sur disque (par ligne) :                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ [1|2025-01-15|Widget|10|99.90][2|2025-01-15|Gadget|5|149.95]... â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   âœ“ Optimal pour : SELECT * FROM ventes WHERE id = 1                    â”‚
â”‚   âœ— Inefficace pour : SELECT SUM(montant) FROM ventes                   â”‚
â”‚     (doit lire TOUTES les colonnes pour chaque ligne)                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Column Store (Stockage Colonnaire)                   â”‚
â”‚                                                                         â”‚
â”‚   MÃªme table "ventes", stockÃ©e diffÃ©remment :                           â”‚
â”‚                                                                         â”‚
â”‚   Stockage sur disque (par colonne) :                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ id:       [1, 2, 3, 4, 5, ...]                                  â”‚   â”‚
â”‚   â”‚ date:     [2025-01-15, 2025-01-15, 2025-01-16, ...]             â”‚   â”‚
â”‚   â”‚ produit:  [Widget, Gadget, Widget, ...]                         â”‚   â”‚
â”‚   â”‚ quantitÃ©: [10, 5, 8, ...]                                       â”‚   â”‚
â”‚   â”‚ montant:  [99.90, 149.95, 79.92, ...]                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   âœ“ Optimal pour : SELECT SUM(montant) FROM ventes                      â”‚
â”‚     (lit UNIQUEMENT la colonne montant)                                 â”‚
â”‚   âœ— Moins efficace pour : SELECT * FROM ventes WHERE id = 1             â”‚
â”‚     (doit reconstruire la ligne depuis plusieurs colonnes)              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Avantages du Stockage Colonnaire

| Avantage | Description | Impact |
|----------|-------------|--------|
| **Lecture sÃ©lective** | Lit uniquement les colonnes nÃ©cessaires | I/O rÃ©duit de 10-100Ã— |
| **Compression** | Valeurs similaires groupÃ©es = meilleure compression | Stockage rÃ©duit de 5-10Ã— |
| **Vectorisation** | Traitement par lots de valeurs | CPU plus efficace |
| **Cache efficace** | DonnÃ©es utiles restent en cache | Moins d'accÃ¨s disque |
| **AgrÃ©gations rapides** | SUM, AVG, COUNT optimisÃ©s | RequÃªtes 10-1000Ã— plus rapides |

### 1.3 Quand Utiliser le Stockage Colonnaire ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cas d'Usage : Row vs Column                          â”‚
â”‚                                                                         â”‚
â”‚   ROW STORE (PostgreSQL standard)         COLUMN STORE (Hydra, etc.)    â”‚
â”‚                                                                         â”‚
â”‚   âœ“ Transactions (OLTP)                   âœ“ Analytique (OLAP)           â”‚
â”‚   âœ“ INSERT/UPDATE/DELETE frÃ©quents        âœ“ Lecture massive             â”‚
â”‚   âœ“ SELECT * (toutes colonnes)            âœ“ AgrÃ©gations (SUM, AVG)      â”‚
â”‚   âœ“ RequÃªtes par clÃ© primaire             âœ“ Scans de grandes tables     â”‚
â”‚   âœ“ DonnÃ©es chaudes (accÃ¨s rÃ©cent)        âœ“ DonnÃ©es historiques         â”‚
â”‚   âœ“ < 10 millions de lignes               âœ“ > 100 millions de lignes    â”‚
â”‚                                                                         â”‚
â”‚   Exemples :                              Exemples :                    â”‚
â”‚   â€¢ Application web classique             â€¢ Data warehouse              â”‚
â”‚   â€¢ E-commerce (commandes)                â€¢ Business Intelligence       â”‚
â”‚   â€¢ CRM, ERP                              â€¢ Logs et Ã©vÃ©nements          â”‚
â”‚   â€¢ Sessions utilisateur                  â€¢ Time series analytics       â”‚
â”‚                                           â€¢ Machine Learning features   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.4 Compression dans le Stockage Colonnaire

Le stockage colonnaire permet une compression exceptionnelle car les valeurs d'une mÃªme colonne sont souvent similaires.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Techniques de Compression                            â”‚
â”‚                                                                         â”‚
â”‚   1. RUN-LENGTH ENCODING (RLE)                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Avant : [France, France, France, France, USA, USA, USA]       â”‚   â”‚
â”‚   â”‚   AprÃ¨s : [(France, 4), (USA, 3)]                               â”‚   â”‚
â”‚   â”‚   Compression : 70%+                                            â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   2. DICTIONARY ENCODING                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   Dictionnaire : {0: "Ã‰lectronique", 1: "VÃªtements", 2: "Maison"}â”‚  â”‚
â”‚   â”‚   DonnÃ©es : [0, 0, 1, 2, 0, 1, 2, 2, 0, ...]                     â”‚  â”‚
â”‚   â”‚   Compression : 80%+                                             â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   3. DELTA ENCODING (pour valeurs sÃ©quentielles)                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Avant : [1000, 1001, 1002, 1003, 1004, 1005]                  â”‚   â”‚
â”‚   â”‚   AprÃ¨s : [1000, +1, +1, +1, +1, +1]                            â”‚   â”‚
â”‚   â”‚   Compression : 60%+                                            â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   4. BIT-PACKING                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Valeurs 0-15 stockÃ©es sur 4 bits au lieu de 32                â”‚   â”‚
â”‚   â”‚   Compression : 87.5%                                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   RÃ©sultat typique : 1 TB de donnÃ©es â†’ 100-200 GB compressÃ©             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.5 OLTP vs OLAP : Le Dilemme Traditionnel

Historiquement, les entreprises devaient maintenir deux systÃ¨mes sÃ©parÃ©s :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Architecture Traditionnelle                          â”‚
â”‚                                                                         â”‚
â”‚   Applications                          Analytique / BI                 â”‚
â”‚       â”‚                                      â”‚                          â”‚
â”‚       â–¼                                      â–¼                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ETL           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚   â”‚    OLTP     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    OLAP     â”‚                    â”‚
â”‚   â”‚ (PostgreSQL)â”‚   (nuit/batch)     â”‚ (Snowflake, â”‚                    â”‚
â”‚   â”‚             â”‚                    â”‚  Redshift)  â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                         â”‚
â”‚   ProblÃ¨mes :                                                           â”‚
â”‚   â€¢ DonnÃ©es dupliquÃ©es                                                  â”‚
â”‚   â€¢ Latence (donnÃ©es pas Ã  jour)                                        â”‚
â”‚   â€¢ CoÃ»t de deux systÃ¨mes                                               â”‚
â”‚   â€¢ ComplexitÃ© ETL                                                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Architecture HTAP (Hybride)                          â”‚
â”‚                                                                         â”‚
â”‚   Applications          Analytique / BI                                 â”‚
â”‚       â”‚                      â”‚                                          â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                  â–¼                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    PostgreSQL + Columnar                        â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚   â”‚   â”‚   Row Tables    â”‚    â”‚ Columnar Tables â”‚                    â”‚   â”‚
â”‚   â”‚   â”‚   (OLTP)        â”‚    â”‚   (OLAP)        â”‚                    â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   MÃªme base, mÃªme SQL, temps rÃ©el !                             â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   Avantages :                                                           â”‚
â”‚   â€¢ Une seule source de vÃ©ritÃ©                                          â”‚
â”‚   â€¢ DonnÃ©es toujours Ã  jour                                             â”‚
â”‚   â€¢ RÃ©duction des coÃ»ts                                                 â”‚
â”‚   â€¢ SimplicitÃ© opÃ©rationnelle                                           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Partie 2 : Hydra - PostgreSQL OLAP Open Source

### 2.1 PrÃ©sentation de Hydra

**Hydra** est un projet open source qui ajoute le stockage colonnaire Ã  PostgreSQL, permettant des requÃªtes analytiques jusqu'Ã  100Ã— plus rapides.

#### CaractÃ©ristiques Principales

| CaractÃ©ristique | Description |
|-----------------|-------------|
| **Open Source** | Licence Apache 2.0 |
| **Compatible PostgreSQL** | Extension native, SQL standard |
| **VectorisÃ©** | Traitement SIMD pour les agrÃ©gations |
| **CompressÃ©** | Compression automatique par colonne |
| **ParallÃ©lisÃ©** | Utilise les workers parallÃ¨les PostgreSQL |
| **Hybride** | Mix row/columnar dans la mÃªme base |

### 2.2 Architecture Hydra

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Architecture Hydra                                   â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      PostgreSQL Server                          â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚                    Query Planner                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚         (choisit row ou columnar selon la requÃªte)      â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                           â”‚                                     â”‚   â”‚
â”‚   â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚   â”‚
â”‚   â”‚           â–¼                               â–¼                     â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚   â”‚   â”‚   Heap Tables   â”‚           â”‚ Columnar Tables â”‚             â”‚   â”‚
â”‚   â”‚   â”‚   (Row Store)   â”‚           â”‚ (Hydra Engine)  â”‚             â”‚   â”‚
â”‚   â”‚   â”‚                 â”‚           â”‚                 â”‚             â”‚   â”‚
â”‚   â”‚   â”‚   â€¢ users       â”‚           â”‚   â€¢ events      â”‚             â”‚   â”‚
â”‚   â”‚   â”‚   â€¢ orders      â”‚           â”‚   â€¢ logs        â”‚             â”‚   â”‚
â”‚   â”‚   â”‚   â€¢ products    â”‚           â”‚   â€¢ metrics     â”‚             â”‚   â”‚
â”‚   â”‚   â”‚                 â”‚           â”‚                 â”‚             â”‚   â”‚
â”‚   â”‚   â”‚   AccÃ¨s OLTP    â”‚           â”‚   AccÃ¨s OLAP    â”‚             â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚              Vectorized Execution Engine                â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â€¢ Traitement par batch (1024 tuples)                  â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â€¢ Instructions SIMD (AVX2, AVX-512)                   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â€¢ ParallÃ©lisation automatique                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Installation de Hydra

#### Avec Docker (RecommandÃ© pour dÃ©buter)

```bash
# Lancer Hydra avec Docker
docker run -d \
  --name hydra \
  -e POSTGRES_PASSWORD=secret \
  -p 5432:5432 \
  ghcr.io/hydradatabase/hydra:latest

# Se connecter
psql -h localhost -U postgres
```

#### Installation depuis les Sources

```bash
# PrÃ©requis
sudo apt install postgresql-server-dev-16 build-essential

# Cloner et compiler
git clone https://github.com/hydradatabase/hydra.git
cd hydra/columnar
make
sudo make install

# Activer l'extension
psql -c "CREATE EXTENSION columnar;"
```

### 2.4 Utilisation de Hydra

#### CrÃ©er une Table Colonnaire

```sql
-- Activer l'extension
CREATE EXTENSION IF NOT EXISTS columnar;

-- CrÃ©er une table colonnaire (syntaxe USING)
CREATE TABLE events (
    id BIGSERIAL,
    event_time TIMESTAMP NOT NULL,
    user_id INTEGER,
    event_type VARCHAR(50),
    properties JSONB,
    country VARCHAR(2),
    device VARCHAR(20),
    revenue NUMERIC(10,2)
) USING columnar;

-- VÃ©rifier le type de stockage
SELECT relname, relam::regam
FROM pg_class
WHERE relname = 'events';
```

#### Convertir une Table Existante

```sql
-- MÃ©thode 1 : CrÃ©er une nouvelle table colonnaire
CREATE TABLE events_columnar (LIKE events) USING columnar;
INSERT INTO events_columnar SELECT * FROM events;
DROP TABLE events;
ALTER TABLE events_columnar RENAME TO events;

-- MÃ©thode 2 : Utiliser la fonction de conversion
SELECT columnar.alter_table_set_access_method('events', 'columnar');
```

#### Options de Compression

```sql
-- Configurer la compression par table
SELECT columnar.alter_columnar_table_set(
    'events',
    compression => 'zstd',          -- Algorithme : zstd, lz4, none
    stripe_row_limit => 150000,     -- Lignes par stripe
    chunk_group_row_limit => 10000  -- Lignes par chunk
);

-- Voir les options actuelles
SELECT * FROM columnar.options WHERE relation = 'events'::regclass;
```

### 2.5 RequÃªtes Analytiques avec Hydra

```sql
-- Exemple : Analyse des Ã©vÃ©nements (trÃ¨s rapide sur columnar)
-- Cette requÃªte ne lit que les colonnes nÃ©cessaires

-- Revenus par pays et type d'appareil
SELECT
    country,
    device,
    COUNT(*) AS total_events,
    SUM(revenue) AS total_revenue,
    AVG(revenue) AS avg_revenue
FROM events
WHERE event_time >= '2025-01-01'
  AND event_time < '2025-02-01'
GROUP BY country, device
ORDER BY total_revenue DESC;

-- Tendance journaliÃ¨re
SELECT
    DATE_TRUNC('day', event_time) AS jour,
    event_type,
    COUNT(*) AS occurrences
FROM events
WHERE event_time >= NOW() - INTERVAL '30 days'
GROUP BY 1, 2
ORDER BY 1, 3 DESC;

-- Analyse de cohorte utilisateur
SELECT
    DATE_TRUNC('week', first_event) AS cohorte,
    COUNT(DISTINCT user_id) AS nouveaux_users,
    SUM(total_revenue) AS revenue
FROM (
    SELECT
        user_id,
        MIN(event_time) AS first_event,
        SUM(revenue) AS total_revenue
    FROM events
    GROUP BY user_id
) user_stats
GROUP BY 1
ORDER BY 1;
```

### 2.6 Performances Hydra vs Row Store

```sql
-- Benchmark : Table de 100 millions de lignes

-- Configuration test
CREATE TABLE events_row (LIKE events_columnar);  -- Row store
-- ... insÃ©rer 100M lignes identiques ...

-- RequÃªte d'agrÃ©gation
EXPLAIN ANALYZE
SELECT
    country,
    SUM(revenue),
    COUNT(*)
FROM events_columnar  -- ou events_row
WHERE event_time >= '2025-01-01'
GROUP BY country;
```

**RÃ©sultats Typiques** :

| MÃ©trique | Row Store | Columnar (Hydra) | AmÃ©lioration |
|----------|-----------|------------------|--------------|
| **Temps d'exÃ©cution** | 45 secondes | 0.8 secondes | **56Ã—** |
| **DonnÃ©es lues** | 12 GB | 180 MB | **67Ã—** |
| **Taille stockage** | 15 GB | 2.1 GB | **7Ã—** |

### 2.7 Bonnes Pratiques Hydra

```sql
-- 1. Choisir les bonnes tables pour le columnar
-- âœ“ Tables de faits (Ã©vÃ©nements, logs, mÃ©triques)
-- âœ“ Tables historiques rarement modifiÃ©es
-- âœ“ Tables > 1 million de lignes
-- âœ— Tables de dimensions petites (utiliser row)
-- âœ— Tables avec UPDATE/DELETE frÃ©quents

-- 2. Optimiser les stripes pour votre workload
-- Plus de lignes par stripe = meilleure compression
-- Moins de lignes par stripe = meilleure granularitÃ© de lecture
SELECT columnar.alter_columnar_table_set(
    'events',
    stripe_row_limit => 150000  -- DÃ©faut pour la plupart des cas
);

-- 3. Utiliser le partitionnement avec columnar
CREATE TABLE events_partitioned (
    id BIGSERIAL,
    event_time TIMESTAMP NOT NULL,
    user_id INTEGER,
    event_type VARCHAR(50),
    revenue NUMERIC(10,2)
) PARTITION BY RANGE (event_time);

-- Partitions rÃ©centes en row (pour les Ã©critures)
CREATE TABLE events_2025_current PARTITION OF events_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- Partitions historiques en columnar (pour l'analytique)
CREATE TABLE events_2024_q4 PARTITION OF events_partitioned
    FOR VALUES FROM ('2024-10-01') TO ('2025-01-01')
    USING columnar;

-- 4. Batch les insertions (Ã©viter les petits INSERTs)
-- Mauvais : INSERT INTO events VALUES (...); -- 1 ligne
-- Bon : INSERT INTO events SELECT ... FROM staging; -- milliers de lignes
-- Ou : COPY events FROM '/data/events.csv';
```

---

## Partie 3 : pg_analytics (ParadeDB)

### 3.1 PrÃ©sentation de pg_analytics

**pg_analytics** est une extension dÃ©veloppÃ©e par ParadeDB qui transforme PostgreSQL en un moteur analytique haute performance. Elle utilise le format de fichier **Parquet** et le moteur d'exÃ©cution **DataFusion** (Apache Arrow).

#### DiffÃ©rences avec Hydra

| CritÃ¨re | Hydra | pg_analytics |
|---------|-------|--------------|
| **Format** | Format columnar natif | Parquet (standard industrie) |
| **Moteur** | Extension PostgreSQL | DataFusion (Rust) |
| **Ã‰cosystÃ¨me** | PostgreSQL-centric | Compatible data lakehouse |
| **Export** | Via COPY | Fichiers Parquet natifs |
| **MaturitÃ©** | Plus mature | Plus rÃ©cent, trÃ¨s actif |

### 3.2 Architecture pg_analytics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Architecture pg_analytics                            â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      PostgreSQL                                 â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚                  pg_analytics Extension                 â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   SQL Parser    â”‚â”€â”€â”€â–ºâ”‚  Query Rewriter â”‚            â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚  (PostgreSQL)   â”‚    â”‚   (DÃ©tecte si   â”‚            â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚                 â”‚    â”‚   analytique)   â”‚            â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                   â”‚                     â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                    â–¼                             â–¼      â”‚   â”‚   â”‚
â”‚   â”‚   â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚   â”‚
â”‚   â”‚   â”‚            â”‚ PostgreSQL  â”‚              â”‚  DataFusion â”‚ â”‚   â”‚   â”‚
â”‚   â”‚   â”‚            â”‚  Executor   â”‚              â”‚   Engine    â”‚ â”‚   â”‚   â”‚
â”‚   â”‚   â”‚            â”‚  (Row ops)  â”‚              â”‚ (Vectorized)â”‚ â”‚   â”‚   â”‚
â”‚   â”‚   â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                â”‚        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                â–¼        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                       â”‚   Apache    â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                       â”‚   Arrow     â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                       â”‚ (In-memory) â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                    â”‚
â”‚                                    â–¼                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    Stockage Parquet                             â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚   â”‚   â”‚ events.     â”‚  â”‚ metrics.    â”‚  â”‚  logs.      â”‚             â”‚   â”‚
â”‚   â”‚   â”‚ parquet     â”‚  â”‚ parquet     â”‚  â”‚  parquet    â”‚             â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â”‚   Compatible avec : Spark, DuckDB, Polars, DataBricks, etc.     â”‚   â”‚
â”‚   â”‚                                                                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Installation de pg_analytics

#### Avec Docker (ParadeDB)

```bash
# ParadeDB inclut pg_analytics + pg_search
docker run -d \
  --name paradedb \
  -e POSTGRES_PASSWORD=secret \
  -p 5432:5432 \
  paradedb/paradedb:latest

# Se connecter
psql -h localhost -U postgres
```

#### Installation Manuelle

```bash
# TÃ©lÃ©charger depuis les releases GitHub
wget https://github.com/paradedb/paradedb/releases/download/v0.x.x/pg_analytics-pg16.deb
sudo dpkg -i pg_analytics-pg16.deb

# Activer dans postgresql.conf
shared_preload_libraries = 'pg_analytics'

# RedÃ©marrer PostgreSQL
sudo systemctl restart postgresql

# CrÃ©er l'extension
psql -c "CREATE EXTENSION pg_analytics;"
```

### 3.4 Utilisation de pg_analytics

#### CrÃ©er une Table Parquet

```sql
-- Activer l'extension
CREATE EXTENSION IF NOT EXISTS pg_analytics;

-- CrÃ©er une table avec stockage Parquet
CREATE TABLE web_analytics (
    event_id UUID DEFAULT gen_random_uuid(),
    timestamp TIMESTAMP NOT NULL,
    session_id UUID,
    user_id INTEGER,
    page_url TEXT,
    referrer TEXT,
    country CHAR(2),
    device_type VARCHAR(20),
    browser VARCHAR(50),
    load_time_ms INTEGER,
    is_bounce BOOLEAN
) USING parquet;

-- InsÃ©rer des donnÃ©es
INSERT INTO web_analytics (timestamp, user_id, page_url, country, device_type, load_time_ms, is_bounce)
SELECT
    NOW() - (random() * INTERVAL '30 days'),
    (random() * 100000)::INTEGER,
    '/page/' || (random() * 100)::INTEGER,
    (ARRAY['US', 'FR', 'DE', 'UK', 'JP'])[1 + (random() * 4)::INTEGER],
    (ARRAY['mobile', 'desktop', 'tablet'])[1 + (random() * 2)::INTEGER],
    (random() * 5000)::INTEGER,
    random() > 0.7
FROM generate_series(1, 1000000);
```

#### RequÃªtes Analytiques

```sql
-- Performance par pays et appareil
SELECT
    country,
    device_type,
    COUNT(*) AS page_views,
    AVG(load_time_ms)::INTEGER AS avg_load_time,
    SUM(CASE WHEN is_bounce THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS bounce_rate
FROM web_analytics
WHERE timestamp >= NOW() - INTERVAL '7 days'
GROUP BY country, device_type
ORDER BY page_views DESC;

-- Pages les plus lentes
SELECT
    page_url,
    COUNT(*) AS views,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY load_time_ms) AS p50_ms,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY load_time_ms) AS p95_ms,
    MAX(load_time_ms) AS max_ms
FROM web_analytics
GROUP BY page_url
HAVING COUNT(*) > 100
ORDER BY p95_ms DESC
LIMIT 20;
```

### 3.5 IntÃ©gration avec le Data Lakehouse

Un avantage majeur de pg_analytics est son intÃ©gration avec l'Ã©cosystÃ¨me data lakehouse.

#### Lire des Fichiers Parquet Externes

```sql
-- CrÃ©er une table externe pointant vers S3
CREATE FOREIGN TABLE external_events ()
SERVER parquet_server
OPTIONS (
    files 's3://my-bucket/events/*.parquet'
);

-- Interroger directement les fichiers Parquet
SELECT
    event_type,
    COUNT(*),
    SUM(revenue)
FROM external_events
WHERE event_date = '2025-01-15'
GROUP BY event_type;
```

#### Exporter vers Parquet

```sql
-- Exporter une requÃªte vers un fichier Parquet
COPY (
    SELECT * FROM events
    WHERE event_time >= '2025-01-01'
) TO '/data/exports/events_jan2025.parquet'
WITH (FORMAT parquet);
```

### 3.6 pg_search : Recherche Full-Text avec pg_analytics

ParadeDB inclut aussi **pg_search**, une extension de recherche full-text ultra-rapide basÃ©e sur Tantivy (Ã©quivalent Rust de Lucene).

```sql
-- Activer pg_search
CREATE EXTENSION pg_search;

-- CrÃ©er un index BM25 pour la recherche
CREATE INDEX idx_articles_search ON articles
USING bm25 (title, content)
WITH (key_field = 'id');

-- Recherche full-text performante
SELECT
    id,
    title,
    paradedb.score(id) AS relevance
FROM articles
WHERE articles @@@ 'PostgreSQL AND performance'
ORDER BY relevance DESC
LIMIT 10;

-- Recherche hybride : full-text + filtres SQL
SELECT *
FROM articles
WHERE articles @@@ 'machine learning'
  AND published_at >= '2025-01-01'
  AND category = 'technology'
ORDER BY paradedb.score(id) DESC;
```

---

## Partie 4 : Autres Solutions Colonnaires

### 4.1 Citus Columnar

Citus inclut Ã©galement un stockage colonnaire, utile pour les workloads analytiques distribuÃ©s.

```sql
-- Avec Citus
CREATE EXTENSION citus_columnar;

CREATE TABLE analytics_events (...) USING columnar;

-- Distribuer la table colonnaire
SELECT create_distributed_table('analytics_events', 'tenant_id');
```

### 4.2 TimescaleDB Compression

TimescaleDB offre une compression colonnaire pour les sÃ©ries temporelles.

```sql
-- Activer la compression sur une hypertable
ALTER TABLE metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id',
    timescaledb.compress_orderby = 'time DESC'
);

-- Compresser les anciennes donnÃ©es
SELECT compress_chunk(c)
FROM show_chunks('metrics', older_than => INTERVAL '7 days') c;

-- Les chunks compressÃ©s utilisent le stockage colonnaire
```

### 4.3 ClickHouse avec clickhouse_fdw

Pour des besoins analytiques extrÃªmes, vous pouvez fÃ©dÃ©rer PostgreSQL avec ClickHouse.

```sql
-- Installer le FDW
CREATE EXTENSION clickhouse_fdw;

-- Configurer le serveur ClickHouse
CREATE SERVER clickhouse_server
FOREIGN DATA WRAPPER clickhouse_fdw
OPTIONS (host 'clickhouse.example.com', port '8123');

-- Mapper une table ClickHouse
CREATE FOREIGN TABLE ch_events (
    event_time DateTime,
    user_id UInt32,
    event_type String,
    revenue Float64
) SERVER clickhouse_server
OPTIONS (table_name 'events');

-- Joindre donnÃ©es PostgreSQL et ClickHouse !
SELECT
    u.name,
    COUNT(*) AS events,
    SUM(e.revenue) AS total_revenue
FROM users u
JOIN ch_events e ON u.id = e.user_id
GROUP BY u.name;
```

### 4.4 DuckDB avec pg_duckdb

DuckDB est un moteur analytique embarquÃ©. pg_duckdb l'intÃ¨gre dans PostgreSQL.

```sql
-- Installer pg_duckdb
CREATE EXTENSION pg_duckdb;

-- ExÃ©cuter des requÃªtes analytiques via DuckDB
SELECT duckdb.query($$
    SELECT
        country,
        SUM(revenue) AS total
    FROM read_parquet('s3://bucket/events/*.parquet')
    GROUP BY country
    ORDER BY total DESC
$$);
```

---

## Partie 5 : Comparaison des Solutions

### 5.1 Tableau Comparatif

| CritÃ¨re | Hydra | pg_analytics | Citus Columnar | TimescaleDB |
|---------|-------|--------------|----------------|-------------|
| **Format** | Natif | Parquet | Natif | Natif |
| **Compression** | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… |
| **Vitesse analytique** | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… |
| **IntÃ©gration PostgreSQL** | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| **Ã‰cosystÃ¨me externe** | â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜… | â˜…â˜…â˜… |
| **Distribution** | Non | Non | Oui | LimitÃ©e |
| **MaturitÃ©** | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| **Cas d'usage** | OLAP gÃ©nÃ©ral | Data lakehouse | Multi-tenant | Time series |

### 5.2 Arbre de DÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Choisir une Solution Colonnaire                      â”‚
â”‚                                                                         â”‚
â”‚   Besoin principal ?                                                    â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â”œâ”€â–º SÃ©ries temporelles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TimescaleDB            â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â”œâ”€â–º Multi-tenant Ã  grande Ã©chelle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Citus Columnar          â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â”œâ”€â–º IntÃ©gration data lakehouse (Parquet) â”€â”€â”€â–º pg_analytics            â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â””â”€â–º Analytique PostgreSQL simple â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Hydra                   â”‚
â”‚                                                                         â”‚
â”‚   ConsidÃ©rations secondaires :                                          â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â”œâ”€â–º Besoin d'exporter vers Spark/Databricks ?                         â”‚
â”‚   â”‚   â””â”€â–º pg_analytics (format Parquet natif)                           â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â”œâ”€â–º StabilitÃ© et maturitÃ© prioritaires ?                              â”‚
â”‚   â”‚   â””â”€â–º TimescaleDB ou Citus                                          â”‚
â”‚   â”‚                                                                     â”‚
â”‚   â””â”€â–º Performance pure sur agrÃ©gations ?                                â”‚
â”‚       â””â”€â–º pg_analytics ou ClickHouse (via FDW)                          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Partie 6 : Bonnes Pratiques et Patterns

### 6.1 Pattern : Hot/Warm/Cold Data

Utilisez diffÃ©rents types de stockage selon l'Ã¢ge des donnÃ©es.

```sql
-- Table partitionnÃ©e par temps
CREATE TABLE events (
    id BIGSERIAL,
    event_time TIMESTAMP NOT NULL,
    event_type VARCHAR(50),
    data JSONB
) PARTITION BY RANGE (event_time);

-- HOT : DonnÃ©es rÃ©centes en row store (Ã©critures frÃ©quentes)
CREATE TABLE events_current PARTITION OF events
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
    -- Utilise le heap standard (row)

-- WARM : DonnÃ©es du mois prÃ©cÃ©dent en columnar (lectures analytiques)
CREATE TABLE events_2024_12 PARTITION OF events
    FOR VALUES FROM ('2024-12-01') TO ('2025-01-01')
    USING columnar;

-- COLD : DonnÃ©es anciennes archivÃ©es (stockage externe)
-- â†’ Exporter vers S3 en Parquet et supprimer de PostgreSQL

-- ProcÃ©dure de rotation mensuelle
CREATE OR REPLACE PROCEDURE rotate_partitions()
LANGUAGE plpgsql AS $$
DECLARE
    old_partition TEXT;
BEGIN
    -- Convertir la partition du mois prÃ©cÃ©dent en columnar
    old_partition := 'events_' || TO_CHAR(NOW() - INTERVAL '1 month', 'YYYY_MM');
    EXECUTE format('ALTER TABLE %I SET ACCESS METHOD columnar', old_partition);

    -- CrÃ©er la nouvelle partition pour le mois suivant
    EXECUTE format(
        'CREATE TABLE events_%s PARTITION OF events FOR VALUES FROM (%L) TO (%L)',
        TO_CHAR(NOW() + INTERVAL '1 month', 'YYYY_MM'),
        DATE_TRUNC('month', NOW() + INTERVAL '1 month'),
        DATE_TRUNC('month', NOW() + INTERVAL '2 months')
    );
END;
$$;
```

### 6.2 Pattern : Tables de Faits et Dimensions

```sql
-- Dimensions : Petites tables, accÃ¨s par clÃ© â†’ Row store
CREATE TABLE dim_products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(200),
    category VARCHAR(100),
    price NUMERIC(10,2)
);  -- Row store par dÃ©faut

CREATE TABLE dim_customers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(200),
    email VARCHAR(255),
    country VARCHAR(2)
);  -- Row store par dÃ©faut

-- Faits : Grande table, agrÃ©gations â†’ Columnar
CREATE TABLE fact_sales (
    sale_id BIGSERIAL,
    sale_time TIMESTAMP NOT NULL,
    product_id INTEGER REFERENCES dim_products(id),
    customer_id INTEGER REFERENCES dim_customers(id),
    quantity INTEGER,
    amount NUMERIC(12,2),
    discount NUMERIC(5,2)
) USING columnar;

-- Les jointures fonctionnent normalement
SELECT
    p.category,
    c.country,
    SUM(f.amount) AS total_sales,
    COUNT(*) AS num_transactions
FROM fact_sales f
JOIN dim_products p ON f.product_id = p.id
JOIN dim_customers c ON f.customer_id = c.id
WHERE f.sale_time >= '2025-01-01'
GROUP BY p.category, c.country
ORDER BY total_sales DESC;
```

### 6.3 Pattern : Vues MatÃ©rialisÃ©es Colonnaires

```sql
-- PrÃ©-agrÃ©ger les donnÃ©es pour des dashboards plus rapides
CREATE MATERIALIZED VIEW daily_metrics
USING columnar AS
SELECT
    DATE_TRUNC('day', event_time) AS day,
    event_type,
    country,
    COUNT(*) AS event_count,
    COUNT(DISTINCT user_id) AS unique_users,
    SUM(revenue) AS total_revenue
FROM events
GROUP BY 1, 2, 3;

-- RafraÃ®chir pÃ©riodiquement
REFRESH MATERIALIZED VIEW CONCURRENTLY daily_metrics;

-- Index pour les requÃªtes courantes
CREATE INDEX idx_daily_metrics_day ON daily_metrics(day);

-- RequÃªtes sur la vue (ultra-rapide)
SELECT * FROM daily_metrics
WHERE day >= NOW() - INTERVAL '7 days'
ORDER BY day DESC, total_revenue DESC;
```

### 6.4 Monitoring des Tables Colonnaires

```sql
-- Statistiques sur les tables colonnaires (Hydra)
SELECT
    c.relname AS table_name,
    pg_size_pretty(pg_relation_size(c.oid)) AS size,
    s.stripe_num AS num_stripes,
    s.row_count AS total_rows,
    s.chunk_count AS num_chunks
FROM pg_class c
JOIN columnar.stripe s ON s.storage_id = columnar.get_storage_id(c.oid)
WHERE c.relam = (SELECT oid FROM pg_am WHERE amname = 'columnar');

-- Ratio de compression
SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) AS compressed_size,
    pg_size_pretty(
        (SELECT SUM(row_count) * AVG(chunk_group_row_limit) * 100
         FROM columnar.stripe
         WHERE storage_id = columnar.get_storage_id(oid))
    ) AS estimated_uncompressed
FROM pg_class
WHERE relam = (SELECT oid FROM pg_am WHERE amname = 'columnar');
```

---

## RÃ©sumÃ©

Ce chapitre a explorÃ© le stockage colonnaire dans PostgreSQL :

| Concept | Ce qu'il faut retenir |
|---------|----------------------|
| **Row vs Column** | Row pour OLTP, Column pour OLAP |
| **Compression** | 5-10Ã— rÃ©duction de taille |
| **Hydra** | Extension columnar native PostgreSQL |
| **pg_analytics** | Format Parquet, Ã©cosystÃ¨me data lakehouse |
| **HTAP** | Transactions + Analytics dans une seule base |

### Points ClÃ©s

- Le stockage colonnaire excelle pour les agrÃ©gations sur grandes tables
- Hydra est le choix simple pour rester 100% PostgreSQL
- pg_analytics ouvre PostgreSQL Ã  l'Ã©cosystÃ¨me data lakehouse
- Combinez row et columnar selon le pattern d'accÃ¨s aux donnÃ©es
- Le partitionnement temporel facilite la gestion hot/warm/cold

### L'Avenir

Le stockage colonnaire dans PostgreSQL Ã©volue rapidement :
- AmÃ©lioration continue des performances vectorisÃ©es
- Meilleure intÃ©gration avec les formats lakehouse (Iceberg, Delta Lake)
- Potentielle intÃ©gration native dans PostgreSQL core
- Convergence OLTP/OLAP vers le "HTAP" unifiÃ©

---


â­ï¸ [Les apports majeurs de PostgreSQL 18](/21-conclusion-et-perspectives/03-apports-majeurs-pg18.md)
