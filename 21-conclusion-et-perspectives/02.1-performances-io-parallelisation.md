üîù Retour au [Sommaire](/SOMMAIRE.md)

# 21.2.1 Performances I/O et Parall√©lisation

## Introduction

Les performances d'une base de donn√©es d√©pendent fondamentalement de deux facteurs : la vitesse √† laquelle elle peut lire et √©crire des donn√©es sur le disque (I/O), et sa capacit√© √† utiliser tous les c≈ìurs processeur disponibles (parall√©lisation).

PostgreSQL a fait des progr√®s consid√©rables dans ces deux domaines, particuli√®rement avec PostgreSQL 18 qui introduit un sous-syst√®me d'I/O asynchrone r√©volutionnaire. Ce chapitre explore ces avanc√©es et leur impact sur les performances.

---

## Partie 1 : Comprendre les I/O dans PostgreSQL

### 1.1 Qu'est-ce que l'I/O ?

**I/O** (Input/Output, ou Entr√©es/Sorties) d√©signe toutes les op√©rations de lecture et d'√©criture entre PostgreSQL et le stockage (disques durs, SSD, stockage r√©seau).

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Flux des Donn√©es dans PostgreSQL                     ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   Application                                                           ‚îÇ
‚îÇ       ‚îÇ                                                                 ‚îÇ
‚îÇ       ‚ñº                                                                 ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                    PostgreSQL                           ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ   Shared    ‚îÇ    ‚îÇ   Backend   ‚îÇ    ‚îÇ    WAL      ‚îÇ  ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ   Buffers   ‚îÇ    ‚îÇ   Process   ‚îÇ    ‚îÇ   Buffers   ‚îÇ  ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ             ‚îÇ                                     ‚îÇ                     ‚îÇ
‚îÇ             ‚îÇ         Op√©rations I/O              ‚îÇ                     ‚îÇ
‚îÇ             ‚ñº                                     ‚ñº                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                   Syst√®me de Fichiers                   ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ             ‚îÇ                                    ‚îÇ                      ‚îÇ
‚îÇ             ‚ñº                                    ‚ñº                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ                   Stockage Physique                     ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ              (SSD, HDD, NVMe, SAN, etc.)                ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Types d'Op√©rations I/O

PostgreSQL effectue diff√©rents types d'op√©rations I/O :

| Type | Description | Impact Performance |
|------|-------------|-------------------|
| **Lecture de donn√©es** | Charger les pages de tables et d'index | Latence des requ√™tes |
| **√âcriture de donn√©es** | Persister les modifications | Durabilit√© |
| **√âcriture WAL** | Journal des transactions | Critique pour ACID |
| **Checkpoints** | Synchronisation p√©riodique | Pics d'I/O |
| **VACUUM** | Nettoyage des lignes mortes | Maintenance |

### 1.3 Le Goulot d'√âtranglement I/O

Historiquement, l'I/O est le principal facteur limitant des performances :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Latence Typique par Composant                        ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   Registres CPU      ‚îÇ‚ñå                                    ~1 ns        ‚îÇ
‚îÇ   Cache L1           ‚îÇ‚ñà‚ñà                                   ~4 ns        ‚îÇ
‚îÇ   Cache L2           ‚îÇ‚ñà‚ñà‚ñà‚ñà                                ~10 ns        ‚îÇ
‚îÇ   Cache L3           ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            ~40 ns        ‚îÇ
‚îÇ   RAM                ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   ~100 ns        ‚îÇ
‚îÇ   SSD NVMe           ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ~20 000 ns        ‚îÇ
‚îÇ   SSD SATA           ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ~100 000 ns        ‚îÇ
‚îÇ   HDD                ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ~10 000 000 ns      ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚ûú L'√©cart entre RAM et disque est √âNORME (100x √† 100 000x)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

C'est pourquoi PostgreSQL utilise intensivement le cache m√©moire (Shared Buffers) pour minimiser les acc√®s disque.

### 1.4 I/O Synchrone vs Asynchrone

#### I/O Synchrone (Traditionnel)

En mode synchrone, PostgreSQL attend la fin de chaque op√©ration I/O avant de continuer :

```
Temps ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫

Thread 1:  [Lecture 1]----attente----[Traitement][Lecture 2]----attente----[Traitement]
                      ‚ñ≤                                     ‚ñ≤
                      ‚îÇ                                     ‚îÇ
                   CPU inactif                           CPU inactif
```

**Probl√®me** : Le CPU reste inactif pendant les op√©rations I/O.

#### I/O Asynchrone

En mode asynchrone, PostgreSQL lance plusieurs op√©rations I/O en parall√®le :

```
Temps ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫

Requ√™te I/O:   [Lancer Lecture 1]
               [Lancer Lecture 2]
               [Lancer Lecture 3]
                        ‚îÇ
                        ‚ñº
CPU:           [Traitement autres t√¢ches pendant l'attente]
                        ‚îÇ
                        ‚ñº
R√©ception:     [Donn√©es 1 pr√™tes][Donn√©es 2 pr√™tes][Donn√©es 3 pr√™tes]
```

**Avantage** : Le CPU reste occup√©, et le disque peut optimiser l'ordre des lectures.

---

## Partie 2 : PostgreSQL 18 et l'I/O Asynchrone

### 2.1 Une Avanc√©e Majeure

PostgreSQL 18 introduit un **sous-syst√®me d'I/O asynchrone** complet, repr√©sentant l'une des am√©liorations de performance les plus significatives de l'histoire du projet.

#### Gains de Performance Observ√©s

| Sc√©nario | Am√©lioration |
|----------|--------------|
| Scans s√©quentiels sur grandes tables | Jusqu'√† **2-3√ó** plus rapide |
| Lectures al√©atoires intensives | **40-60%** d'am√©lioration |
| Workloads mixtes (OLTP + Analytics) | **20-40%** d'am√©lioration |
| R√©cup√©ration apr√®s crash (Recovery) | **2√ó** plus rapide |

### 2.2 Configuration de l'I/O Asynchrone

#### Param√®tre Principal : io_method

```ini
# postgresql.conf

# M√©thode d'I/O (PostgreSQL 18+)
io_method = 'worker'    # Options : 'sync', 'worker', 'io_uring' (Linux)

# 'sync'     : I/O synchrone traditionnel (d√©faut pour compatibilit√©)
# 'worker'   : Workers d√©di√©s pour I/O asynchrone (cross-platform)
# 'io_uring' : Interface Linux moderne haute performance
```

#### Param√®tres Compl√©mentaires

```ini
# Nombre maximum d'op√©rations I/O en parall√®le
io_max_concurrency = 16

# Taille du ring buffer pour io_uring (Linux uniquement)
io_uring_ring_size = 256

# Activer le prefetch (lecture anticip√©e)
effective_io_concurrency = 200    # D√©faut : 1 (SSD recommand√© : 200)
maintenance_io_concurrency = 10   # Pour VACUUM, CREATE INDEX
```

### 2.3 Comment Fonctionne le Nouveau Sous-syst√®me

#### Architecture avec Workers I/O

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Architecture I/O Asynchrone (PG 18)                   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ   ‚îÇ  Backend 1  ‚îÇ  ‚îÇ  Backend 2  ‚îÇ  ‚îÇ  Backend 3  ‚îÇ   Processus         ‚îÇ
‚îÇ   ‚îÇ  (client)   ‚îÇ  ‚îÇ  (client)   ‚îÇ  ‚îÇ  (client)   ‚îÇ   utilisateur       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ          ‚îÇ                ‚îÇ                ‚îÇ                            ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                           ‚ñº                                             ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ                  ‚îÇ   I/O Request   ‚îÇ   File d'attente                   ‚îÇ
‚îÇ                  ‚îÇ      Queue      ‚îÇ   des requ√™tes I/O                 ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ                           ‚îÇ                                             ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ          ‚ñº                ‚ñº                ‚ñº                            ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ   ‚îÇ I/O Worker  ‚îÇ  ‚îÇ I/O Worker  ‚îÇ  ‚îÇ I/O Worker  ‚îÇ   Workers           ‚îÇ
‚îÇ   ‚îÇ     #1      ‚îÇ  ‚îÇ     #2      ‚îÇ  ‚îÇ     #3      ‚îÇ   d√©di√©s I/O        ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ          ‚îÇ                ‚îÇ                ‚îÇ                            ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                           ‚ñº                                             ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ                  ‚îÇ    Stockage     ‚îÇ                                    ‚îÇ
‚îÇ                  ‚îÇ   (SSD/NVMe)    ‚îÇ                                    ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### io_uring : L'Interface Linux Moderne

Sur Linux, PostgreSQL 18 peut utiliser **io_uring**, une interface kernel moderne (Linux 5.1+) extr√™mement performante :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         io_uring Architecture                           ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   ‚îÇ                        Espace Utilisateur                      ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ                                                                ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ   PostgreSQL                                                   ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ       ‚îÇ                                                        ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ       ‚ñº                                                        ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ  Submission Queue   ‚îÇ    ‚îÇ  Completion Queue   ‚îÇ           ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ  (requ√™tes I/O)     ‚îÇ    ‚îÇ  (r√©sultats)        ‚îÇ           ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ              ‚îÇ                          ‚îÇ                      ‚îÇ    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                  ‚îÇ    M√©moire partag√©e      ‚îÇ                           ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ                  ‚îÇ                          ‚îÇ                           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   ‚îÇ              ‚ñº                          ‚îÇ                      ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ         ‚îÇ           Kernel Linux          ‚îÇ                    ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ         ‚îÇ     (traitement optimis√©)       ‚îÇ                    ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ                        Espace Kernel                           ‚îÇ    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   Avantages io_uring :                                                  ‚îÇ
‚îÇ   ‚Ä¢ Z√©ro copie entre user/kernel                                        ‚îÇ
‚îÇ   ‚Ä¢ Batching automatique des op√©rations                                 ‚îÇ
‚îÇ   ‚Ä¢ Polling pour latence ultra-faible                                   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.4 Cas d'Usage B√©n√©ficiant de l'I/O Asynchrone

#### 1. Scans S√©quentiels sur Grandes Tables

```sql
-- Cette requ√™te b√©n√©ficie √©norm√©ment de l'I/O async
SELECT COUNT(*) FROM logs WHERE created_at > '2025-01-01';

-- PostgreSQL peut pr√©charger les pages suivantes
-- pendant le traitement des pages actuelles
```

#### 2. Index Scans avec Beaucoup de Lignes

```sql
-- Multiples lectures al√©atoires optimis√©es
SELECT * FROM commandes
WHERE client_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
ORDER BY date_commande;
```

#### 3. Op√©rations de Maintenance

```sql
-- VACUUM b√©n√©ficie de l'I/O parall√®le
VACUUM ANALYZE large_table;

-- Cr√©ation d'index plus rapide
CREATE INDEX CONCURRENTLY idx_large ON large_table(column);
```

#### 4. R√©cup√©ration apr√®s Crash

Le replay des WAL pendant la recovery est consid√©rablement acc√©l√©r√© gr√¢ce √† l'I/O asynchrone.

### 2.5 V√©rifier l'Utilisation de l'I/O Asynchrone

```sql
-- Voir la configuration actuelle
SHOW io_method;
SHOW effective_io_concurrency;

-- Statistiques I/O (PostgreSQL 18)
SELECT * FROM pg_stat_io;

-- Colonnes importantes :
-- reads        : Nombre de lectures
-- read_time    : Temps total de lecture
-- writes       : Nombre d'√©critures
-- write_time   : Temps total d'√©criture
-- extends      : Extensions de fichiers
-- fsyncs       : Synchronisations forc√©es
```

### 2.6 Recommandations de Configuration

#### Pour SSD/NVMe

```ini
# postgresql.conf optimis√© pour SSD moderne

io_method = 'io_uring'              # Si Linux 5.1+
effective_io_concurrency = 200      # SSD peuvent g√©rer beaucoup de requ√™tes
maintenance_io_concurrency = 20
random_page_cost = 1.1              # SSD : acc√®s al√©atoire ‚âà s√©quentiel
```

#### Pour HDD (Disques Traditionnels)

```ini
# Configuration pour disques m√©caniques

io_method = 'worker'
effective_io_concurrency = 2        # HDD limit√©s en I/O parall√®le
maintenance_io_concurrency = 2
random_page_cost = 4.0              # Acc√®s al√©atoire co√ªteux sur HDD
```

#### Pour Stockage Cloud/R√©seau

```ini
# Configuration pour stockage r√©seau (EBS, Azure Disk, etc.)

io_method = 'worker'
effective_io_concurrency = 100      # D√©pend de votre provisioning
maintenance_io_concurrency = 10
```

---

## Partie 3 : Parall√©lisation des Requ√™tes

### 3.1 Introduction √† la Parall√©lisation

La **parall√©lisation** permet √† PostgreSQL d'utiliser plusieurs c≈ìurs CPU pour ex√©cuter une seule requ√™te, r√©duisant drastiquement le temps d'ex√©cution sur les grandes tables.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Ex√©cution S√©quentielle vs Parall√®le                  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   S√âQUENTIEL (1 worker)                                                 ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ 100%       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ   Temps total : 100 secondes                                            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   PARALL√àLE (4 workers)                                                 ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                  ‚îÇ
‚îÇ   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ 25% (Worker 1)                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                  ‚îÇ
‚îÇ   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ 25% (Worker 2)                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                  ‚îÇ
‚îÇ   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ 25% (Worker 3)                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                  ‚îÇ
‚îÇ   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ 25% (Worker 4)                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  ‚îÇ
‚îÇ   Temps total : ~25 secondes (+ overhead coordination)                  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Configuration de la Parall√©lisation

```ini
# postgresql.conf

# Nombre maximum de workers parall√®les par requ√™te
max_parallel_workers_per_gather = 4

# Nombre total de workers parall√®les disponibles
max_parallel_workers = 8

# Nombre total de workers (inclut maintenance)
max_worker_processes = 16

# Seuil de taille de table pour consid√©rer le parall√©lisme
min_parallel_table_scan_size = 8MB

# Seuil pour les index scans parall√®les
min_parallel_index_scan_size = 512kB

# Co√ªt estim√© pour lancer un worker
parallel_setup_cost = 1000

# Co√ªt estim√© pour transf√©rer une ligne entre workers
parallel_tuple_cost = 0.1
```

### 3.3 Op√©rations Parall√©lisables

PostgreSQL peut parall√©liser de nombreuses op√©rations :

| Op√©ration | Parall√©lisable | Notes |
|-----------|---------------|-------|
| **Seq Scan** | ‚úÖ Oui | Scan s√©quentiel parall√®le |
| **Index Scan** | ‚úÖ Oui | Depuis PostgreSQL 10 |
| **Index Only Scan** | ‚úÖ Oui | Depuis PostgreSQL 10 |
| **Bitmap Heap Scan** | ‚úÖ Oui | Depuis PostgreSQL 10 |
| **Hash Join** | ‚úÖ Oui | Depuis PostgreSQL 11 |
| **Merge Join** | ‚úÖ Oui | Depuis PostgreSQL 13 |
| **Nested Loop** | ‚ö†Ô∏è Partiel | La boucle externe seulement |
| **Agr√©gation** | ‚úÖ Oui | Partial + Final Aggregate |
| **CREATE INDEX** | ‚úÖ Oui | Depuis PostgreSQL 11 |
| **VACUUM** | ‚úÖ Oui | Tables diff√©rentes en parall√®le |
| **Fonctions utilisateur** | ‚ö†Ô∏è Conditionnel | Doit √™tre marqu√©e PARALLEL SAFE |

### 3.4 Lire un Plan d'Ex√©cution Parall√®le

```sql
EXPLAIN (ANALYZE, VERBOSE)
SELECT COUNT(*) FROM large_table WHERE status = 'active';
```

**R√©sultat** :

```
Finalize Aggregate  (cost=123456.78..123456.79 rows=1 width=8)
                    (actual time=1234.567..1234.568 rows=1 loops=1)
  ->  Gather  (cost=123456.00..123456.77 rows=4 width=8)
              (actual time=1230.123..1234.500 rows=5 loops=1)
        Workers Planned: 4
        Workers Launched: 4
        ->  Partial Aggregate  (cost=122456.00..122456.01 rows=1 width=8)
                               (actual time=1200.100..1200.101 rows=1 loops=5)
              ->  Parallel Seq Scan on large_table
                                 (cost=0.00..120000.00 rows=250000 width=0)
                                 (actual time=0.050..1000.000 rows=200000 loops=5)
                    Filter: (status = 'active'::text)
                    Rows Removed by Filter: 50000
```

**D√©cryptage** :

| √âl√©ment | Signification |
|---------|---------------|
| `Gather` | Collecte les r√©sultats des workers parall√®les |
| `Workers Planned: 4` | 4 workers pr√©vus par le planificateur |
| `Workers Launched: 4` | 4 workers effectivement lanc√©s |
| `Partial Aggregate` | Agr√©gation partielle par chaque worker |
| `Finalize Aggregate` | Combinaison finale des r√©sultats |
| `loops=5` | 5 ex√©cutions (1 leader + 4 workers) |
| `Parallel Seq Scan` | Scan s√©quentiel distribu√© entre workers |

### 3.5 Fonctionnement Interne

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Ex√©cution Parall√®le d'une Requ√™te                    ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ                         Leader Process                          ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                 ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   1. Parse & Plan query                                         ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   2. D√©cide d'utiliser le parall√©lisme                          ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   3. Lance les workers via Gather                               ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                 ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                          ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ              ‚ñº               ‚ñº                  ‚ñº                       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ   ‚îÇ    Worker 1     ‚îÇ ‚îÇ    Worker 2     ‚îÇ ‚îÇ    Worker 3     ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ                 ‚îÇ ‚îÇ                 ‚îÇ ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  Scan pages     ‚îÇ ‚îÇ  Scan pages     ‚îÇ ‚îÇ  Scan pages     ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  1-1000         ‚îÇ ‚îÇ  1001-2000      ‚îÇ ‚îÇ  2001-3000      ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ                 ‚îÇ ‚îÇ                 ‚îÇ ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ   ‚îÇ  Partial Agg    ‚îÇ ‚îÇ  Partial Agg    ‚îÇ ‚îÇ  Partial Agg    ‚îÇ           ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ            ‚îÇ                   ‚îÇ                   ‚îÇ                    ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                ‚ñº                                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ                       Gather Node                               ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                 ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   - Collecte les tuples de tous les workers                     ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   - Finalize Aggregate combine les r√©sultats partiels           ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   - Retourne le r√©sultat final au client                        ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                 ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.6 Agr√©gation Parall√®le

L'agr√©gation parall√®le utilise un pattern en deux phases :

```sql
-- Requ√™te
SELECT department, COUNT(*), AVG(salary)
FROM employees
GROUP BY department;
```

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Agr√©gation Parall√®le en Deux Phases                 ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ   Phase 1 : Partial Aggregate (dans chaque worker)                     ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ   Worker 1:                    Worker 2:                               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ   ‚îÇ dept ‚îÇ count ‚îÇ sum  ‚îÇ     ‚îÇ dept ‚îÇ count ‚îÇ sum  ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ IT   ‚îÇ  100  ‚îÇ 500K ‚îÇ     ‚îÇ IT   ‚îÇ  150  ‚îÇ 750K ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ HR   ‚îÇ   50  ‚îÇ 200K ‚îÇ     ‚îÇ HR   ‚îÇ   40  ‚îÇ 160K ‚îÇ                  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ   Phase 2 : Finalize Aggregate (combine les partiels)                  ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ   ‚îÇ dept ‚îÇ count ‚îÇ avg_salary                 ‚îÇ                        ‚îÇ
‚îÇ   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                        ‚îÇ
‚îÇ   ‚îÇ IT   ‚îÇ  250  ‚îÇ (500K+750K)/(100+150)=5000 ‚îÇ                        ‚îÇ
‚îÇ   ‚îÇ HR   ‚îÇ   90  ‚îÇ (200K+160K)/(50+40)=4000   ‚îÇ                        ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.7 Jointures Parall√®les

#### Hash Join Parall√®le

```sql
SELECT c.nom, COUNT(cmd.id)
FROM clients c
JOIN commandes cmd ON c.id = cmd.client_id
GROUP BY c.nom;
```

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Parallel Hash Join                               ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   1. Build Phase (construction de la table de hash)                     ‚îÇ
‚îÇ      Tous les workers participent √† construire la hash table            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ                    Shared Hash Table                            ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                                 ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Worker 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Worker 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  Hash Table   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ Petite table        ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Worker 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  (clients)    ‚îÇ      (Build side)        ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   2. Probe Phase (recherche dans la hash table)                         ‚îÇ
‚îÇ      Chaque worker scanne une partie de la grande table                 ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   Worker 1: Scan commandes[1-10000]     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Hash Lookup      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Worker 2: Scan commandes[10001-20000] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Hash Lookup      ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Worker 3: Scan commandes[20001-30000] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Hash Lookup      ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.8 Cr√©ation d'Index Parall√®le

PostgreSQL peut utiliser plusieurs workers pour cr√©er des index B-Tree :

```sql
-- Le parall√©lisme est automatique si la table est assez grande
CREATE INDEX idx_large_column ON large_table(column);

-- Contr√¥ler le nombre de workers
SET max_parallel_maintenance_workers = 4;
CREATE INDEX idx_large_column ON large_table(column);
```

```ini
# postgresql.conf
max_parallel_maintenance_workers = 4    # Workers pour maintenance
```

### 3.9 Quand le Parall√©lisme N'est Pas Utilis√©

Le parall√©lisme peut √™tre d√©sactiv√© dans certains cas :

#### 1. Table Trop Petite

```sql
-- Si la table fait moins de min_parallel_table_scan_size (8MB d√©faut)
-- PostgreSQL n'utilisera pas le parall√©lisme
EXPLAIN SELECT * FROM small_table WHERE x > 5;
-- ‚Üí Seq Scan (pas de Parallel)
```

#### 2. Fonctions Non Parall√®les

```sql
-- Une fonction marqu√©e PARALLEL UNSAFE emp√™che le parall√©lisme
CREATE FUNCTION ma_fonction() RETURNS INTEGER AS $$
BEGIN
    -- Code qui modifie des donn√©es
    RETURN 1;
END;
$$ LANGUAGE plpgsql PARALLEL UNSAFE;  -- Bloque le parall√©lisme

-- Solution : Marquer PARALLEL SAFE si possible
CREATE FUNCTION fonction_pure(x INTEGER) RETURNS INTEGER AS $$
BEGIN
    RETURN x * 2;
END;
$$ LANGUAGE plpgsql PARALLEL SAFE;  -- Autorise le parall√©lisme
```

#### 3. Curseurs et Transactions Particuli√®res

```sql
-- Les curseurs ne peuvent pas √™tre parall√©lis√©s
DECLARE cur CURSOR FOR SELECT * FROM large_table;

-- Transaction SERIALIZABLE peut limiter le parall√©lisme
BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE;
```

#### 4. Sous-requ√™tes Corr√©l√©es

Certaines sous-requ√™tes corr√©l√©es emp√™chent le parall√©lisme de la requ√™te externe.

### 3.10 Forcer ou D√©sactiver le Parall√©lisme

```sql
-- D√©sactiver le parall√©lisme pour une session
SET max_parallel_workers_per_gather = 0;

-- Forcer plus de workers (pour tests)
SET max_parallel_workers_per_gather = 8;
SET parallel_setup_cost = 0;
SET parallel_tuple_cost = 0;

-- R√©initialiser
RESET max_parallel_workers_per_gather;
RESET parallel_setup_cost;
RESET parallel_tuple_cost;
```

---

## Partie 4 : Optimiser les Performances I/O

### 4.1 R√©duire les I/O avec le Cache

#### Shared Buffers

Le cache principal de PostgreSQL. Dimensionnez-le correctement :

```ini
# R√®gle g√©n√©rale : 25% de la RAM
shared_buffers = 4GB    # Pour un serveur avec 16GB RAM
```

```sql
-- V√©rifier le cache hit ratio
SELECT
    sum(heap_blks_read) AS disk_reads,
    sum(heap_blks_hit) AS cache_hits,
    round(
        sum(heap_blks_hit) * 100.0 /
        nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0),
        2
    ) AS cache_hit_ratio
FROM pg_statio_user_tables;
```

> **Objectif** : Cache hit ratio > 99%

#### Effective Cache Size

Indique √† PostgreSQL la m√©moire totale disponible (PostgreSQL + OS cache) :

```ini
# R√®gle g√©n√©rale : 75% de la RAM
effective_cache_size = 12GB    # Pour un serveur avec 16GB RAM
```

### 4.2 Optimiser les √âcritures

#### Configuration WAL

```ini
# Taille des segments WAL
wal_segment_size = 1GB    # D√©faut : 16MB (changeable √† initdb)

# Taille maximale avant checkpoint
max_wal_size = 4GB

# Niveau de compression
wal_compression = on      # R√©duit le volume de WAL
```

#### Checkpoints

```ini
# Intervalle entre checkpoints
checkpoint_timeout = 15min    # D√©faut : 5min

# √âtaler les √©critures sur la dur√©e du checkpoint
checkpoint_completion_target = 0.9    # 90% de la p√©riode
```

### 4.3 Surveiller les I/O

```sql
-- Statistiques I/O par table (PostgreSQL 18)
SELECT
    schemaname,
    relname,
    heap_blks_read,
    heap_blks_hit,
    idx_blks_read,
    idx_blks_hit
FROM pg_statio_user_tables
ORDER BY heap_blks_read DESC
LIMIT 10;

-- Tables avec le plus de lectures disque
SELECT
    schemaname || '.' || relname AS table,
    pg_size_pretty(pg_relation_size(relid)) AS size,
    seq_scan,
    seq_tup_read,
    idx_scan,
    n_live_tup
FROM pg_stat_user_tables
WHERE seq_tup_read > 100000
ORDER BY seq_tup_read DESC;
```

### 4.4 Bonnes Pratiques I/O

1. **Utilisez des SSD/NVMe** : L'am√©lioration est dramatique
2. **S√©parez WAL et donn√©es** : Sur des disques diff√©rents si possible
3. **Activez les checksums** : Protection contre la corruption (d√©faut PG18)
4. **Configurez le filesystem** :
   - ext4 ou XFS recommand√©s
   - Options de montage : `noatime,nodiratime`

```bash
# Exemple de montage optimis√©
mount -o noatime,nodiratime /dev/sda1 /var/lib/postgresql
```

---

## Partie 5 : Bonnes Pratiques et Recommandations

### 5.1 Checklist de Configuration Performance

```ini
# postgresql.conf - Configuration Performance

# === M√©moire ===
shared_buffers = 4GB                    # 25% RAM
effective_cache_size = 12GB             # 75% RAM
work_mem = 64MB                         # Attention au nombre de connexions
maintenance_work_mem = 1GB

# === I/O (PostgreSQL 18) ===
io_method = 'io_uring'                  # Linux 5.1+ uniquement
effective_io_concurrency = 200          # Pour SSD
random_page_cost = 1.1                  # SSD

# === Parall√©lisme ===
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4
max_worker_processes = 16

# === WAL ===
wal_compression = on
max_wal_size = 4GB
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9

# === Autovacuum ===
autovacuum_max_workers = 4
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05
```

### 5.2 Diagnostic des Probl√®mes de Performance

```sql
-- 1. Requ√™tes en attente d'I/O
SELECT
    pid,
    wait_event_type,
    wait_event,
    state,
    query
FROM pg_stat_activity
WHERE wait_event_type = 'IO';

-- 2. Tables n√©cessitant le plus d'I/O
SELECT
    relname,
    heap_blks_read AS reads,
    heap_blks_hit AS hits,
    round(heap_blks_hit * 100.0 / nullif(heap_blks_hit + heap_blks_read, 0), 2) AS hit_ratio
FROM pg_statio_user_tables
WHERE heap_blks_read > 0
ORDER BY heap_blks_read DESC
LIMIT 10;

-- 3. Requ√™tes n'utilisant pas le parall√©lisme
-- (utiliser pg_stat_statements et v√©rifier les plans)
```

### 5.3 √âvolutions Futures

Le domaine des performances I/O continue d'√©voluer rapidement :

| Tendance | Description | Horizon |
|----------|-------------|---------|
| **Direct I/O** | Bypasser le cache OS pour plus de contr√¥le | PostgreSQL 19+ |
| **Storage Class Memory** | M√©moire persistante ultra-rapide (Intel Optane) | En cours |
| **Parallel Write** | √âcritures parall√®les pour WAL | En recherche |
| **Vectorized Execution** | Traitement par lots de tuples | Extensions |
| **Columnar Storage** | Stockage colonnaire natif | Extensions (Hydra) |

---

## R√©sum√©

Ce chapitre a couvert les performances I/O et la parall√©lisation dans PostgreSQL :

| Concept | Ce qu'il faut retenir |
|---------|----------------------|
| **I/O Synchrone** | Traditionnel, une op√©ration √† la fois |
| **I/O Asynchrone** | PostgreSQL 18, jusqu'√† 3√ó plus rapide |
| **io_uring** | Interface Linux moderne ultra-performante |
| **Parall√©lisation** | Plusieurs workers pour une requ√™te |
| **Gather** | N≈ìud qui collecte les r√©sultats parall√®les |

### Points Cl√©s

- PostgreSQL 18 introduit l'I/O asynchrone, une am√©lioration majeure
- Configurez `io_method` et `effective_io_concurrency` pour votre stockage
- La parall√©lisation peut r√©duire le temps d'ex√©cution proportionnellement au nombre de workers
- Surveillez le cache hit ratio et les statistiques I/O
- Les SSD/NVMe modernes tirent pleinement parti de ces optimisations

### Pour Aller Plus Loin

- Testez diff√©rentes configurations sur votre workload
- Utilisez `EXPLAIN (ANALYZE, BUFFERS)` pour comprendre l'I/O de vos requ√™tes
- Surveillez pg_stat_io (PostgreSQL 18) pour les m√©triques d√©taill√©es
- Consid√©rez le partitionnement pour les tr√®s grandes tables


‚è≠Ô∏è [IA et Machine Learning int√©gr√©s](/21-conclusion-et-perspectives/02.2-ia-machine-learning.md)
