ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 18.6.3. Cas d'Usage : RAG, Semantic Search

## Introduction

Vous avez probablement dÃ©jÃ  utilisÃ© des moteurs de recherche traditionnels qui cherchent des **mots-clÃ©s exacts**. Mais que se passe-t-il si vous tapez "comment rÃ©parer une fuite d'eau" et que les meilleurs rÃ©sultats utilisent les termes "colmater canalisation" ou "urgence plomberie" ? Un moteur classique les manquera.

C'est lÃ  qu'interviennent deux technologies rÃ©volutionnaires rendues possibles par **pgvector** :

1. **Semantic Search (Recherche SÃ©mantique)** : Comprendre le *sens* de votre recherche, pas seulement les mots
2. **RAG (Retrieval-Augmented Generation)** : Permettre aux IA (comme ChatGPT) d'accÃ©der Ã  vos donnÃ©es privÃ©es pour des rÃ©ponses prÃ©cises

Ce chapitre explique ces deux cas d'usage majeurs qui transforment PostgreSQL en moteur de recherche intelligent.

### Analogie : Le bibliothÃ©caire expert

**Recherche traditionnelle (mots-clÃ©s)** :
```
Vous : "Je cherche un livre sur les dauphins"
BibliothÃ©caire robot : *Scan des titres*
                       "Voici les livres avec 'dauphin' dans le titre"
                       âŒ Manque les livres sur "cÃ©tacÃ©s" ou "mammifÃ¨res marins"
```

**Semantic Search (recherche sÃ©mantique)** :
```
Vous : "Je cherche un livre sur les dauphins"
BibliothÃ©caire expert : *Comprend le sujet*
                        "Voici des livres sur les dauphins, mais aussi
                         sur les cÃ©tacÃ©s, la vie marine, et l'intelligence animale"
                        âœ… Comprend l'intention et les concepts liÃ©s
```

**RAG (ChatGPT + vos donnÃ©es)** :
```
Vous : "Quelle est notre politique de retour ?"
ChatGPT seul : "Je ne connais pas votre entreprise spÃ©cifique..."

ChatGPT + RAG : 1. Cherche dans votre base de connaissances interne
                2. Trouve les documents pertinents
                3. "Selon votre politique (v2.3), les retours sont
                   acceptÃ©s sous 30 jours avec facture..."
                âœ… RÃ©ponse prÃ©cise basÃ©e sur VOS donnÃ©es
```

---

## Partie 1 : Semantic Search (Recherche SÃ©mantique)

### Qu'est-ce que la Recherche SÃ©mantique ?

La **recherche sÃ©mantique** est la capacitÃ© de trouver des documents en comprenant leur **signification** plutÃ´t qu'en cherchant des mots-clÃ©s exacts.

**Concept clÃ©** : Deux phrases peuvent avoir des mots diffÃ©rents mais le mÃªme sens.

```
"J'adore les chats"
â‰ˆ "J'aime beaucoup les fÃ©lins"
â‰ˆ "Les chats sont mes animaux prÃ©fÃ©rÃ©s"

Ces trois phrases sont sÃ©mantiquement similaires,
mÃªme si elles ne partagent que peu de mots.
```

### DiffÃ©rence avec Recherche Traditionnelle

#### Recherche par mots-clÃ©s (Full-Text Search)

```sql
-- PostgreSQL Full-Text Search classique
SELECT * FROM articles
WHERE to_tsvector('french', contenu) @@ to_tsquery('french', 'Python & programmation');
```

**Avantages** :
- âœ… TrÃ¨s rapide
- âœ… Exact (trouve les mots spÃ©cifiÃ©s)

**Limitations** :
- âŒ Synonymes non compris ("coder" vs "programmer")
- âŒ Variations linguistiques ("programmation" vs "dÃ©veloppement logiciel")
- âŒ Concepts liÃ©s non dÃ©tectÃ©s ("Python" ne trouve pas "langage interprÃ©tÃ©")

#### Recherche sÃ©mantique (pgvector)

```sql
-- Recherche sÃ©mantique avec pgvector
SELECT * FROM articles
ORDER BY embedding <=> query_embedding
LIMIT 10;
```

**Avantages** :
- âœ… Comprend les synonymes
- âœ… Capture les concepts liÃ©s
- âœ… Fonctionne entre langues (multilingue)
- âœ… Comprend l'intention de recherche

**Limitations** :
- âš ï¸ NÃ©cessite gÃ©nÃ©ration d'embeddings (API/modÃ¨le)
- âš ï¸ LÃ©gÃ¨rement plus lent que full-text
- âš ï¸ Moins prÃ©cis pour recherche exacte de mots spÃ©cifiques

### Tableau comparatif

| CritÃ¨re | Full-Text Search | Semantic Search |
|---------|------------------|-----------------|
| **RequÃªte** | "voiture Ã©lectrique" | "voiture Ã©lectrique" |
| **Trouve** | Documents avec "voiture" ET "Ã©lectrique" | Concepts : vÃ©hicule Ã©lectrique, Tesla, mobilitÃ© verte, auto Ã©cologique |
| **Synonymes** | âŒ Non (sauf dictionnaire) | âœ… Oui (automatique) |
| **Fautes de frappe** | âš ï¸ Avec fuzzy search | âœ… Robuste |
| **Multilingue** | âŒ Une langue par index | âœ… Fonctionne entre langues |
| **Vitesse** | ğŸŸ¢ğŸŸ¢ğŸŸ¢ Ultra-rapide | ğŸŸ¢ğŸŸ¢ Rapide |
| **Setup** | ğŸŸ¢ Simple (natif PG) | ğŸŸ¡ NÃ©cessite modÃ¨le embeddings |

### Architecture de Semantic Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. INDEXATION (Une fois, au dÃ©but)                         â”‚
â”‚                                                             â”‚
â”‚  Documents texte                                            â”‚
â”‚  "PostgreSQL est une base..."                               â”‚
â”‚  "Python est un langage..."                                 â”‚
â”‚  "Docker permet de..."                                      â”‚
â”‚           â†“                                                 â”‚
â”‚  ModÃ¨le d'embeddings (OpenAI, Sentence-BERT, etc.)          â”‚
â”‚           â†“                                                 â”‚
â”‚  Vecteurs                                                   â”‚
â”‚  [0.023, -0.154, 0.387, ...]                                â”‚
â”‚  [0.891, 0.234, -0.567, ...]                                â”‚
â”‚  [-0.123, 0.456, 0.789, ...]                                â”‚
â”‚           â†“                                                 â”‚
â”‚  PostgreSQL + pgvector                                      â”‚
â”‚  INSERT INTO docs (texte, embedding) VALUES ...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. RECHERCHE (Ã€ chaque requÃªte utilisateur)                â”‚
â”‚                                                             â”‚
â”‚  RequÃªte utilisateur                                        â”‚
â”‚  "comment installer PostgreSQL"                             â”‚
â”‚           â†“                                                 â”‚
â”‚  MÃªme modÃ¨le d'embeddings                                   â”‚
â”‚           â†“                                                 â”‚
â”‚  Vecteur de requÃªte                                         â”‚
â”‚  [0.045, -0.167, 0.392, ...]                                â”‚
â”‚           â†“                                                 â”‚
â”‚  pgvector : Recherche de similaritÃ©                         â”‚
â”‚  SELECT * FROM docs                                         â”‚
â”‚  ORDER BY embedding <=> query_vector                        â”‚
â”‚  LIMIT 10                                                   â”‚
â”‚           â†“                                                 â”‚
â”‚  RÃ©sultats pertinents (mÃªme sans mots exacts)               â”‚
â”‚  1. "Installation de PostgreSQL sur Linux"                  â”‚
â”‚  2. "Guide de dÃ©marrage PostgreSQL"                         â”‚
â”‚  3. "Configurer PostgreSQL aprÃ¨s installation"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation ComplÃ¨te

#### Ã‰tape 1 : PrÃ©parer la base de donnÃ©es

```sql
-- Activer pgvector
CREATE EXTENSION vector;

-- CrÃ©er la table de documents
CREATE TABLE articles (
    id SERIAL PRIMARY KEY,
    titre TEXT NOT NULL,
    contenu TEXT NOT NULL,
    categorie TEXT,
    auteur TEXT,
    date_publication DATE DEFAULT CURRENT_DATE,
    embedding vector(1536),  -- OpenAI text-embedding-3-small
    created_at TIMESTAMP DEFAULT NOW()
);

-- CrÃ©er un index HNSW pour recherche rapide
CREATE INDEX idx_articles_embedding
ON articles
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

#### Ã‰tape 2 : GÃ©nÃ©rer et stocker les embeddings (Python)

```python
import openai
import psycopg2
from psycopg2.extras import execute_values

# Configuration
openai.api_key = "votre_clÃ©_openai"
conn = psycopg2.connect("postgresql://user:pass@localhost/dbname")

def generer_embedding(texte, model="text-embedding-3-small"):
    """GÃ©nÃ¨re un embedding pour un texte donnÃ©"""
    # Nettoyer le texte (optionnel)
    texte = texte.replace("\n", " ").strip()

    # Appel API OpenAI
    response = openai.embeddings.create(
        input=texte,
        model=model
    )

    return response.data[0].embedding

def indexer_article(titre, contenu, categorie=None, auteur=None):
    """Indexe un article avec son embedding"""

    # Combiner titre et contenu pour l'embedding
    texte_complet = f"{titre}\n\n{contenu}"

    # GÃ©nÃ©rer l'embedding
    embedding = generer_embedding(texte_complet)

    # InsÃ©rer dans PostgreSQL
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO articles (titre, contenu, categorie, auteur, embedding)
            VALUES (%s, %s, %s, %s, %s)
            RETURNING id
            """,
            (titre, contenu, categorie, auteur, embedding)
        )
        article_id = cur.fetchone()[0]

    conn.commit()
    return article_id

# Exemple d'utilisation
article_id = indexer_article(
    titre="Introduction Ã  PostgreSQL",
    contenu="PostgreSQL est un systÃ¨me de gestion de base de donnÃ©es relationnel...",
    categorie="Database",
    auteur="Jean Dupont"
)

print(f"Article indexÃ© avec l'ID : {article_id}")
```

#### Ã‰tape 3 : Fonction de recherche sÃ©mantique

```python
def recherche_semantique(requete, limit=10, seuil_similarite=None):
    """
    Recherche sÃ©mantique dans les articles

    Args:
        requete: Question ou mots-clÃ©s de recherche
        limit: Nombre de rÃ©sultats Ã  retourner
        seuil_similarite: SimilaritÃ© minimale (0-1), optionnel

    Returns:
        Liste de tuples (id, titre, contenu, similarite)
    """

    # GÃ©nÃ©rer l'embedding de la requÃªte
    query_embedding = generer_embedding(requete)

    # Construire la requÃªte SQL
    sql = """
        SELECT
            id,
            titre,
            contenu,
            categorie,
            auteur,
            1 - (embedding <=> %s::vector) AS similarite
        FROM articles
    """

    params = [query_embedding]

    # Ajouter filtre de similaritÃ© si spÃ©cifiÃ©
    if seuil_similarite:
        sql += " WHERE 1 - (embedding <=> %s::vector) >= %s"
        params.append(query_embedding)
        params.append(seuil_similarite)

    sql += """
        ORDER BY embedding <=> %s::vector
        LIMIT %s
    """
    params.extend([query_embedding, limit])

    # ExÃ©cuter la requÃªte
    with conn.cursor() as cur:
        cur.execute(sql, params)
        resultats = cur.fetchall()

    return resultats

# Exemple d'utilisation
resultats = recherche_semantique(
    requete="comment optimiser les performances d'une base de donnÃ©es",
    limit=5,
    seuil_similarite=0.7  # Seulement rÃ©sultats >70% similaires
)

for id, titre, contenu, categorie, auteur, similarite in resultats:
    print(f"\nğŸ“„ {titre} (SimilaritÃ©: {similarite:.2%})")
    print(f"   Auteur: {auteur} | CatÃ©gorie: {categorie}")
    print(f"   AperÃ§u: {contenu[:150]}...")
```

**RÃ©sultat exemple** :

```
ğŸ“„ Optimisation des Index PostgreSQL (SimilaritÃ©: 94.2%)
   Auteur: Marie Martin | CatÃ©gorie: Performance
   AperÃ§u: Les index sont essentiels pour accÃ©lÃ©rer les requÃªtes...

ğŸ“„ Tuning de PostgreSQL pour Production (SimilaritÃ©: 91.7%)
   Auteur: Pierre Durand | CatÃ©gorie: Database
   AperÃ§u: Configuration optimale de shared_buffers, work_mem...

ğŸ“„ Analyse des RequÃªtes Lentes (SimilaritÃ©: 88.3%)
   Auteur: Sophie Lambert | CatÃ©gorie: Troubleshooting
   AperÃ§u: Utiliser EXPLAIN ANALYZE pour identifier les goulots...
```

### Cas d'Usage de Semantic Search

#### 1. Documentation intelligente

**ProblÃ¨me** : Les utilisateurs posent des questions avec leurs propres mots

```python
# L'utilisateur demande
recherche = "erreur connexion refusÃ©e postgresql"

# Le systÃ¨me trouve (mÃªme sans ces mots exacts)
# - "Impossible de se connecter au serveur PostgreSQL"
# - "RÃ©soudre les problÃ¨mes de connexion"
# - "Configuration pg_hba.conf pour accÃ¨s distant"
```

**Avantage** : RÃ©duit drastiquement le support client en trouvant les bonnes rÃ©ponses.

#### 2. Recherche dans une base de connaissances

```sql
-- CrÃ©er une fonction PostgreSQL pour recherche directe
CREATE OR REPLACE FUNCTION rechercher_kb(
    p_requete TEXT,
    p_limit INTEGER DEFAULT 5
)
RETURNS TABLE (
    titre TEXT,
    contenu TEXT,
    categorie TEXT,
    pertinence NUMERIC
) AS $$
DECLARE
    v_query_embedding vector(1536);
BEGIN
    -- En production, gÃ©nÃ©rer l'embedding via extension pl/python
    -- ou rÃ©cupÃ©rer depuis l'application

    RETURN QUERY
    SELECT
        a.titre,
        LEFT(a.contenu, 300) AS contenu,
        a.categorie,
        ROUND((1 - (a.embedding <=> v_query_embedding))::numeric, 3) AS pertinence
    FROM articles a
    ORDER BY a.embedding <=> v_query_embedding
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

-- Utilisation (si embedding prÃ©-calculÃ©)
SELECT * FROM rechercher_kb('configuration ssl postgresql');
```

#### 3. SystÃ¨me de tickets / Support

```python
def classifier_ticket(description_ticket):
    """
    Classe automatiquement un ticket de support
    en trouvant les articles de KB les plus similaires
    """

    # Rechercher articles similaires
    resultats = recherche_semantique(description_ticket, limit=3)

    # Extraire les catÃ©gories
    categories = [r[3] for r in resultats]  # categorie
    similarites = [r[5] for r in resultats]  # similarite

    # CatÃ©gorie la plus probable (pondÃ©rÃ©e par similaritÃ©)
    from collections import Counter
    categorie_ponderee = Counter()
    for cat, sim in zip(categories, similarites):
        categorie_ponderee[cat] += sim

    categorie_suggeree = categorie_ponderee.most_common(1)[0][0]
    confiance = categorie_ponderee[categorie_suggeree] / sum(similarites)

    return {
        'categorie': categorie_suggeree,
        'confiance': confiance,
        'articles_similaires': resultats[:3]
    }

# Exemple
ticket = "Impossible d'insÃ©rer des donnÃ©es, erreur de contrainte foreign key"
classification = classifier_ticket(ticket)

print(f"CatÃ©gorie suggÃ©rÃ©e: {classification['categorie']}")
print(f"Confiance: {classification['confiance']:.1%}")
```

#### 4. DÃ©tection de contenu dupliquÃ©

```python
def detecter_doublons(seuil=0.95):
    """Trouve les articles trÃ¨s similaires (possibles doublons)"""

    with conn.cursor() as cur:
        cur.execute("""
            SELECT
                a1.id AS id1,
                a1.titre AS titre1,
                a2.id AS id2,
                a2.titre AS titre2,
                1 - (a1.embedding <=> a2.embedding) AS similarite
            FROM articles a1
            CROSS JOIN articles a2
            WHERE a1.id < a2.id  -- Ã‰viter duplicatas (a1-a2 et a2-a1)
              AND 1 - (a1.embedding <=> a2.embedding) >= %s
            ORDER BY similarite DESC
        """, (seuil,))

        doublons = cur.fetchall()

    return doublons

# Trouver doublons
doublons = detecter_doublons(seuil=0.95)
for id1, titre1, id2, titre2, sim in doublons:
    print(f"âš ï¸  Doublon potentiel ({sim:.1%} similaire):")
    print(f"    #{id1}: {titre1}")
    print(f"    #{id2}: {titre2}\n")
```

#### 5. Recommandation de contenu

```python
def recommander_articles(article_id, limit=5):
    """Recommande des articles similaires Ã  un article donnÃ©"""

    with conn.cursor() as cur:
        # RÃ©cupÃ©rer l'embedding de l'article de rÃ©fÃ©rence
        cur.execute(
            "SELECT embedding FROM articles WHERE id = %s",
            (article_id,)
        )
        ref_embedding = cur.fetchone()[0]

        # Trouver articles similaires
        cur.execute("""
            SELECT
                id,
                titre,
                categorie,
                1 - (embedding <=> %s::vector) AS similarite
            FROM articles
            WHERE id != %s
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (ref_embedding, article_id, ref_embedding, limit))

        recommandations = cur.fetchall()

    return recommandations

# Afficher recommandations
print("Vous avez aimÃ© 'Introduction Ã  PostgreSQL' ?")
print("Vous aimerez aussi :\n")

reco = recommander_articles(article_id=123, limit=3)
for id, titre, categorie, sim in reco:
    print(f"  ğŸ“š {titre} ({sim:.1%} similaire)")
```

---

## Partie 2 : RAG (Retrieval-Augmented Generation)

### Qu'est-ce que RAG ?

**RAG** = Retrieval-Augmented Generation (GÃ©nÃ©ration AugmentÃ©e par RÃ©cupÃ©ration)

**DÃ©finition simple** : Permettre Ã  un modÃ¨le d'IA (comme ChatGPT) d'accÃ©der Ã  des informations externes **avant** de gÃ©nÃ©rer sa rÃ©ponse.

### Le ProblÃ¨me sans RAG

**ChatGPT seul** :

```
User: Quelle est notre politique de congÃ©s pour 2025 ?

ChatGPT: Je n'ai pas accÃ¨s aux politiques internes de votre entreprise.
         Je ne peux pas vous aider avec des informations spÃ©cifiques
         Ã  votre organisation.
```

**Limitations** :
- âŒ Pas d'accÃ¨s aux donnÃ©es privÃ©es
- âŒ Connaissances limitÃ©es Ã  sa date d'entraÃ®nement
- âŒ Hallucinations possibles (invente des rÃ©ponses)

### La Solution avec RAG

**ChatGPT + RAG** :

```
1. User: "Quelle est notre politique de congÃ©s pour 2025 ?"

2. SystÃ¨me RAG:
   - GÃ©nÃ¨re embedding de la question
   - Recherche dans la base de docs internes (pgvector)
   - Trouve : "Politique RH 2025 - CongÃ©s.pdf"

3. Contexte rÃ©cupÃ©rÃ©:
   "Politique de congÃ©s 2025 : Chaque employÃ© bÃ©nÃ©ficie de 25 jours
    de congÃ©s payÃ©s par an, plus 11 jours fÃ©riÃ©s..."

4. Prompt Ã  ChatGPT:
   Contexte: [Document ci-dessus]
   Question: Quelle est notre politique de congÃ©s pour 2025 ?

5. ChatGPT (avec contexte):
   "Selon votre politique RH 2025, les employÃ©s bÃ©nÃ©ficient de
    25 jours de congÃ©s payÃ©s annuels, auxquels s'ajoutent 11 jours
    fÃ©riÃ©s. Les congÃ©s peuvent Ãªtre fractionnÃ©s..."
```

**Avantages** :
- âœ… RÃ©ponses basÃ©es sur VOS donnÃ©es
- âœ… Toujours Ã  jour (base de donnÃ©es en temps rÃ©el)
- âœ… Moins d'hallucinations (contexte factuel)
- âœ… CitabilitÃ© (sources identifiables)

### Architecture RAG ComplÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1 : INDEXATION (Une fois)                             â”‚
â”‚                                                              â”‚
â”‚  ğŸ“„ Documents sources                                        â”‚
â”‚  - PDFs, Word, Markdown, HTML                                â”‚
â”‚  - Documentation, politiques, manuels                        â”‚
â”‚  - Articles, FAQs, wikis                                     â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ“ Extraction de texte                                      â”‚
â”‚  - PyPDF2, python-docx, BeautifulSoup                        â”‚
â”‚           â†“                                                  â”‚
â”‚  âœ‚ï¸  Chunking (dÃ©coupage en morceaux)                        â”‚
â”‚  - Diviser en paragraphes de 500-1000 tokens                 â”‚
â”‚  - Conserver contexte et mÃ©tadonnÃ©es                         â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ§® GÃ©nÃ©ration d'embeddings                                  â”‚
â”‚  - OpenAI, Cohere, ou modÃ¨le local                           â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ’¾ Stockage PostgreSQL + pgvector                           â”‚
â”‚  INSERT INTO knowledge_base (chunk, metadata, embedding)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2 : RÃ‰CUPÃ‰RATION (Ã€ chaque requÃªte)                   â”‚
â”‚                                                              â”‚
â”‚  ğŸ’¬ Question utilisateur                                     â”‚
â”‚  "Comment configurer SSL sur PostgreSQL ?"                   â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ§® Embedding de la question                                 â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ” Recherche de similaritÃ© (pgvector)                       â”‚
â”‚  SELECT * FROM knowledge_base                                â”‚
â”‚  ORDER BY embedding <=> question_embedding                   â”‚
â”‚  LIMIT 3-5                                                   â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ“š Top K documents pertinents rÃ©cupÃ©rÃ©s                     â”‚
â”‚  - Chunk 1: "Configuration SSL PostgreSQL..."                â”‚
â”‚  - Chunk 2: "Certificats et pg_hba.conf..."                  â”‚
â”‚  - Chunk 3: "ParamÃ¨tres postgresql.conf SSL..."              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3 : GÃ‰NÃ‰RATION (ChatGPT)                              â”‚
â”‚                                                              â”‚
â”‚  ğŸ“‹ Construction du prompt augmentÃ©                          â”‚
â”‚  ---                                                         â”‚
â”‚  Contexte:                                                   â”‚
â”‚  [Chunk 1]                                                   â”‚
â”‚  [Chunk 2]                                                   â”‚
â”‚  [Chunk 3]                                                   â”‚
â”‚                                                              â”‚
â”‚  Question: Comment configurer SSL sur PostgreSQL ?           â”‚
â”‚  ---                                                         â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ¤– ChatGPT gÃ©nÃ¨re la rÃ©ponse                                â”‚
â”‚  - BasÃ©e sur le contexte fourni                              â”‚
â”‚  - Cite les sources                                          â”‚
â”‚           â†“                                                  â”‚
â”‚  ğŸ’¬ RÃ©ponse finale Ã  l'utilisateur                           â”‚
â”‚  "Pour configurer SSL sur PostgreSQL (source: doc_ssl.pdf):  â”‚
â”‚   1. Modifier postgresql.conf : ssl = on                     â”‚
â”‚   2. Placer les certificats dans data/..."                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation ComplÃ¨te d'un RAG

#### Ã‰tape 1 : SchÃ©ma de base de donnÃ©es

```sql
-- Table pour stocker les chunks de documents
CREATE TABLE knowledge_base (
    id SERIAL PRIMARY KEY,
    document_id INTEGER,  -- ID du document source
    document_titre TEXT,
    document_source TEXT,  -- Chemin fichier, URL, etc.
    chunk_index INTEGER,   -- Position du chunk dans le document
    chunk_text TEXT NOT NULL,
    chunk_tokens INTEGER,  -- Nombre de tokens
    embedding vector(1536),
    metadata JSONB,  -- MÃ©tadonnÃ©es flexibles
    created_at TIMESTAMP DEFAULT NOW()
);

-- Index pour recherche rapide
CREATE INDEX idx_kb_embedding
ON knowledge_base
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Index sur mÃ©tadonnÃ©es pour filtres
CREATE INDEX idx_kb_metadata ON knowledge_base USING gin (metadata);

-- Table pour tracker les documents sources
CREATE TABLE documents_source (
    id SERIAL PRIMARY KEY,
    titre TEXT NOT NULL,
    source_path TEXT NOT NULL,
    type_mime TEXT,
    taille_bytes BIGINT,
    date_ajout TIMESTAMP DEFAULT NOW(),
    derniere_indexation TIMESTAMP,
    statut TEXT DEFAULT 'indexed',  -- indexed, processing, error
    metadata JSONB
);
```

#### Ã‰tape 2 : Chunking et Indexation

```python
import tiktoken
from typing import List, Dict

def compter_tokens(texte: str, model="gpt-3.5-turbo") -> int:
    """Compte le nombre de tokens dans un texte"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(texte))

def decouper_en_chunks(
    texte: str,
    taille_chunk: int = 500,
    overlap: int = 50
) -> List[Dict]:
    """
    DÃ©coupe un texte en chunks avec overlap

    Args:
        texte: Texte Ã  dÃ©couper
        taille_chunk: Nombre de tokens par chunk
        overlap: Nombre de tokens de chevauchement entre chunks

    Returns:
        Liste de dictionnaires contenant le texte et mÃ©tadonnÃ©es
    """
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    tokens = encoding.encode(texte)

    chunks = []
    start = 0
    chunk_index = 0

    while start < len(tokens):
        # Extraire un chunk
        end = min(start + taille_chunk, len(tokens))
        chunk_tokens = tokens[start:end]
        chunk_text = encoding.decode(chunk_tokens)

        chunks.append({
            'text': chunk_text,
            'tokens': len(chunk_tokens),
            'chunk_index': chunk_index,
            'start_token': start,
            'end_token': end
        })

        chunk_index += 1
        start += taille_chunk - overlap  # Avancer avec overlap

    return chunks

def indexer_document(
    titre: str,
    texte: str,
    source: str,
    metadata: Dict = None
) -> int:
    """Indexe un document complet dans la knowledge base"""

    # 1. Enregistrer le document source
    with conn.cursor() as cur:
        cur.execute("""
            INSERT INTO documents_source
            (titre, source_path, metadata, statut)
            VALUES (%s, %s, %s, 'processing')
            RETURNING id
        """, (titre, source, metadata))
        doc_id = cur.fetchone()[0]
    conn.commit()

    try:
        # 2. DÃ©couper en chunks
        chunks = decouper_en_chunks(texte, taille_chunk=500, overlap=50)

        # 3. GÃ©nÃ©rer embeddings et insÃ©rer
        for chunk in chunks:
            embedding = generer_embedding(chunk['text'])

            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO knowledge_base
                    (document_id, document_titre, document_source,
                     chunk_index, chunk_text, chunk_tokens, embedding, metadata)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    doc_id,
                    titre,
                    source,
                    chunk['chunk_index'],
                    chunk['text'],
                    chunk['tokens'],
                    embedding,
                    metadata
                ))

        conn.commit()

        # 4. Mettre Ã  jour le statut
        with conn.cursor() as cur:
            cur.execute("""
                UPDATE documents_source
                SET statut = 'indexed',
                    derniere_indexation = NOW()
                WHERE id = %s
            """, (doc_id,))
        conn.commit()

        print(f"âœ… Document '{titre}' indexÃ© : {len(chunks)} chunks")
        return doc_id

    except Exception as e:
        # En cas d'erreur, marquer comme erreur
        with conn.cursor() as cur:
            cur.execute("""
                UPDATE documents_source
                SET statut = 'error'
                WHERE id = %s
            """, (doc_id,))
        conn.commit()
        raise e

# Exemple d'utilisation
texte_doc = """
PostgreSQL est un systÃ¨me de gestion de base de donnÃ©es relationnel...
[Long texte de documentation]
"""

doc_id = indexer_document(
    titre="Guide PostgreSQL",
    texte=texte_doc,
    source="/docs/postgresql_guide.pdf",
    metadata={
        'auteur': 'Ã‰quipe Technique',
        'version': '16.0',
        'categorie': 'Database',
        'langue': 'fr'
    }
)
```

#### Ã‰tape 3 : Fonction de RÃ©cupÃ©ration (Retrieval)

```python
def recuperer_contexte(
    question: str,
    k: int = 3,
    seuil_similarite: float = 0.7,
    filtres_metadata: Dict = None
) -> List[Dict]:
    """
    RÃ©cupÃ¨re les chunks les plus pertinents pour une question

    Args:
        question: Question de l'utilisateur
        k: Nombre de chunks Ã  rÃ©cupÃ©rer
        seuil_similarite: SimilaritÃ© minimale (0-1)
        filtres_metadata: Filtres optionnels (ex: {'categorie': 'Database'})

    Returns:
        Liste de chunks avec mÃ©tadonnÃ©es
    """

    # GÃ©nÃ©rer embedding de la question
    question_embedding = generer_embedding(question)

    # Construire la requÃªte SQL
    sql = """
        SELECT
            id,
            document_titre,
            document_source,
            chunk_text,
            chunk_index,
            metadata,
            1 - (embedding <=> %s::vector) AS similarite
        FROM knowledge_base
        WHERE 1 - (embedding <=> %s::vector) >= %s
    """

    params = [question_embedding, question_embedding, seuil_similarite]

    # Ajouter filtres sur mÃ©tadonnÃ©es
    if filtres_metadata:
        for key, value in filtres_metadata.items():
            sql += f" AND metadata->>{key!r} = %s"
            params.append(value)

    sql += """
        ORDER BY embedding <=> %s::vector
        LIMIT %s
    """
    params.extend([question_embedding, k])

    # ExÃ©cuter
    with conn.cursor() as cur:
        cur.execute(sql, params)
        resultats = cur.fetchall()

    # Formater les rÃ©sultats
    contexte = []
    for row in resultats:
        contexte.append({
            'id': row[0],
            'document': row[1],
            'source': row[2],
            'texte': row[3],
            'chunk_index': row[4],
            'metadata': row[5],
            'similarite': row[6]
        })

    return contexte
```

#### Ã‰tape 4 : GÃ©nÃ©ration avec LLM (ChatGPT)

```python
def generer_reponse_rag(
    question: str,
    k: int = 3,
    modele: str = "gpt-4"
) -> Dict:
    """
    Pipeline RAG complet : RÃ©cupÃ©ration + GÃ©nÃ©ration

    Returns:
        Dict avec rÃ©ponse, sources et mÃ©tadonnÃ©es
    """

    # 1. RETRIEVAL : RÃ©cupÃ©rer le contexte pertinent
    contexte = recuperer_contexte(question, k=k)

    if not contexte:
        return {
            'reponse': "Je n'ai pas trouvÃ© d'information pertinente dans la base de connaissances.",
            'sources': [],
            'confiance': 0.0
        }

    # 2. AUGMENTATION : Construire le prompt avec contexte
    contexte_texte = "\n\n---\n\n".join([
        f"Source: {c['document']} (SimilaritÃ©: {c['similarite']:.1%})\n{c['texte']}"
        for c in contexte
    ])

    prompt = f"""Vous Ãªtes un assistant qui rÃ©pond aux questions en utilisant UNIQUEMENT le contexte fourni ci-dessous. Si la rÃ©ponse n'est pas dans le contexte, dites-le clairement.

Contexte:
{contexte_texte}

Question: {question}

RÃ©ponse (basÃ©e sur le contexte ci-dessus):"""

    # 3. GENERATION : Appeler ChatGPT
    response = openai.chat.completions.create(
        model=modele,
        messages=[
            {
                "role": "system",
                "content": "Tu es un assistant qui rÃ©pond uniquement en te basant sur le contexte fourni. Tu cites tes sources."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        temperature=0.3,  # RÃ©ponses plus factuelles
        max_tokens=500
    )

    reponse_text = response.choices[0].message.content

    # 4. Formater le rÃ©sultat avec sources
    sources_uniques = list({c['document'] for c in contexte})
    similarite_moyenne = sum(c['similarite'] for c in contexte) / len(contexte)

    return {
        'reponse': reponse_text,
        'sources': sources_uniques,
        'contexte': contexte,
        'confiance': similarite_moyenne,
        'nb_chunks_utilises': len(contexte)
    }

# Exemple d'utilisation
resultat = generer_reponse_rag(
    question="Comment optimiser les performances de PostgreSQL ?",
    k=3
)

print(f"ğŸ¤– RÃ©ponse:\n{resultat['reponse']}\n")
print(f"ğŸ“š Sources:")
for source in resultat['sources']:
    print(f"  - {source}")
print(f"\nğŸ“Š Confiance: {resultat['confiance']:.1%}")
```

**RÃ©sultat exemple** :

```
ğŸ¤– RÃ©ponse:
Pour optimiser les performances de PostgreSQL, voici les recommandations principales basÃ©es sur la documentation :

1. Configuration de la mÃ©moire :
   - Ajuster shared_buffers Ã  25% de la RAM disponible
   - Configurer work_mem selon la complexitÃ© des requÃªtes (4-16MB)
   - Augmenter effective_cache_size pour reflÃ©ter la RAM systÃ¨me

2. Indexation :
   - CrÃ©er des index B-Tree sur les colonnes frÃ©quemment filtrÃ©es
   - Utiliser EXPLAIN ANALYZE pour identifier les scans sÃ©quentiels coÃ»teux
   - ConsidÃ©rer les index partiels pour les filtres rÃ©currents

3. Maintenance rÃ©guliÃ¨re :
   - Activer autovacuum pour rÃ©cupÃ©ration d'espace
   - ExÃ©cuter ANALYZE aprÃ¨s modifications massives
   - Planifier VACUUM FULL en pÃ©riode creuse si nÃ©cessaire

ğŸ“š Sources:
  - Guide PostgreSQL - Optimisation
  - Manuel d'Administration PostgreSQL 16
  - Best Practices Production

ğŸ“Š Confiance: 92.3%
```

### Interface Utilisateur (Chatbot)

```python
def chatbot_rag_interactif():
    """Interface en ligne de commande pour chatbot RAG"""

    print("ğŸ¤– Chatbot RAG - Base de Connaissances")
    print("=" * 50)
    print("Tapez 'quit' pour quitter\n")

    while True:
        # Question utilisateur
        question = input("\nğŸ‘¤ Vous: ").strip()

        if question.lower() in ['quit', 'exit', 'quitter']:
            print("Au revoir ! ğŸ‘‹")
            break

        if not question:
            continue

        # GÃ©nÃ©rer rÃ©ponse
        print("\nğŸ” Recherche dans la base de connaissances...")
        resultat = generer_reponse_rag(question, k=3)

        # Afficher rÃ©ponse
        print(f"\nğŸ¤– Assistant: {resultat['reponse']}")

        # Afficher sources (optionnel)
        if resultat['sources']:
            print(f"\nğŸ“š Sources ({resultat['confiance']:.0%} de confiance):")
            for i, source in enumerate(resultat['sources'], 1):
                print(f"  {i}. {source}")

# Lancer le chatbot
if __name__ == "__main__":
    chatbot_rag_interactif()
```

### Cas d'Usage de RAG

#### 1. Support Client Intelligent

**Contexte** : Centre d'aide avec des milliers d'articles

```python
# Indexer toute la base de connaissances
for article in articles_support:
    indexer_document(
        titre=article['titre'],
        texte=article['contenu'],
        source=article['url'],
        metadata={
            'categorie': article['categorie'],
            'produit': article['produit'],
            'langue': 'fr'
        }
    )

# Chatbot de support
def support_client(question):
    resultat = generer_reponse_rag(
        question,
        k=5,  # Plus de contexte pour support
        modele="gpt-4"
    )

    # Ajouter liens vers articles complets
    reponse_enrichie = resultat['reponse']
    reponse_enrichie += "\n\nğŸ“– Pour en savoir plus:"
    for chunk in resultat['contexte']:
        reponse_enrichie += f"\n- {chunk['source']}"

    return reponse_enrichie
```

#### 2. Assistant Documentation Technique

**Contexte** : Documentation interne d'entreprise (onboarding, process, etc.)

```python
# Indexer documentation avec mÃ©tadonnÃ©es riches
indexer_document(
    titre="Processus de DÃ©ploiement Production",
    texte=process_deploiement_text,
    source="docs/deploiement.md",
    metadata={
        'department': 'DevOps',
        'acces': 'tech_team',
        'version': '2.1',
        'date': '2025-01-15'
    }
)

# Recherche avec filtres
def assistant_doc(question, departement=None):
    filtres = {'department': departement} if departement else None
    return generer_reponse_rag(question, k=3, filtres_metadata=filtres)

# Exemple
reponse = assistant_doc(
    "Comment dÃ©ployer en production ?",
    departement="DevOps"
)
```

#### 3. Analyse de Documents Juridiques / Contrats

**Contexte** : Recherche dans des contrats, lois, rÃ©glementations

```python
# Indexer documents juridiques
for contrat in contrats:
    indexer_document(
        titre=f"Contrat {contrat['numero']}",
        texte=contrat['texte_complet'],
        source=contrat['fichier_path'],
        metadata={
            'type': 'contrat',
            'partie': contrat['client'],
            'date_signature': contrat['date'],
            'statut': contrat['statut']
        }
    )

# Recherche spÃ©cifique
def analyser_clause(question, type_contrat=None):
    filtres = {'type': type_contrat} if type_contrat else None
    return generer_reponse_rag(question, k=5, filtres_metadata=filtres)

# Exemple
analyse = analyser_clause(
    "Quelles sont les clauses de rÃ©siliation ?",
    type_contrat="contrat"
)
```

#### 4. Assistant de Recherche Scientifique

**Contexte** : Base de papers, articles scientifiques

```python
# Indexer articles scientifiques
for paper in scientific_papers:
    # Extraire abstract + introduction + conclusion
    texte_important = f"{paper['abstract']}\n\n{paper['introduction']}\n\n{paper['conclusion']}"

    indexer_document(
        titre=paper['titre'],
        texte=texte_important,
        source=paper['doi'],
        metadata={
            'auteurs': paper['auteurs'],
            'journal': paper['journal'],
            'annee': paper['annee'],
            'domaine': paper['domaine'],
            'citations': paper['nb_citations']
        }
    )

# Recherche avec synthÃ¨se
def recherche_litterature(sujet):
    resultat = generer_reponse_rag(
        f"SynthÃ©tise les avancÃ©es rÃ©centes sur : {sujet}",
        k=10,  # Plusieurs papers
        modele="gpt-4"
    )
    return resultat
```

---

## Optimisations AvancÃ©es

### 1. Hybrid Search (CombinÃ© Full-Text + Semantic)

```sql
-- CrÃ©er index full-text en plus de pgvector
ALTER TABLE knowledge_base ADD COLUMN texte_tsvector tsvector;

UPDATE knowledge_base
SET texte_tsvector = to_tsvector('french', chunk_text);

CREATE INDEX idx_kb_fulltext ON knowledge_base USING gin(texte_tsvector);

-- Fonction de recherche hybride
CREATE OR REPLACE FUNCTION recherche_hybride(
    p_query TEXT,
    p_query_embedding vector(1536),
    p_limit INTEGER DEFAULT 10,
    p_weight_semantic FLOAT DEFAULT 0.7,
    p_weight_fulltext FLOAT DEFAULT 0.3
)
RETURNS TABLE (
    id INTEGER,
    chunk_text TEXT,
    score_final FLOAT
) AS $$
BEGIN
    RETURN QUERY
    WITH semantic AS (
        SELECT
            kb.id,
            kb.chunk_text,
            (1 - (kb.embedding <=> p_query_embedding)) AS score_semantic
        FROM knowledge_base kb
        ORDER BY kb.embedding <=> p_query_embedding
        LIMIT p_limit * 2
    ),
    fulltext AS (
        SELECT
            kb.id,
            kb.chunk_text,
            ts_rank(kb.texte_tsvector, to_tsquery('french', p_query)) AS score_fulltext
        FROM knowledge_base kb
        WHERE kb.texte_tsvector @@ to_tsquery('french', p_query)
        ORDER BY score_fulltext DESC
        LIMIT p_limit * 2
    ),
    combined AS (
        SELECT
            COALESCE(s.id, f.id) AS id,
            COALESCE(s.chunk_text, f.chunk_text) AS chunk_text,
            COALESCE(s.score_semantic, 0) * p_weight_semantic +
            COALESCE(f.score_fulltext, 0) * p_weight_fulltext AS score_final
        FROM semantic s
        FULL OUTER JOIN fulltext f ON s.id = f.id
    )
    SELECT
        combined.id,
        combined.chunk_text,
        combined.score_final
    FROM combined
    ORDER BY score_final DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;
```

**Avantage** : Combine prÃ©cision sÃ©mantique + correspondance mots-clÃ©s exacts

### 2. Re-ranking avec Cross-Encoder

```python
from sentence_transformers import CrossEncoder

# Charger un modÃ¨le de re-ranking
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def recherche_avec_reranking(question, k_initial=20, k_final=3):
    """
    1. RÃ©cupÃ¨re k_initial candidats avec recherche sÃ©mantique (rapide)
    2. Re-rank avec modÃ¨le plus prÃ©cis (lent mais prÃ©cis)
    3. Retourne top k_final
    """

    # Ã‰tape 1 : RÃ©cupÃ©ration rapide
    candidats = recuperer_contexte(question, k=k_initial)

    # Ã‰tape 2 : Re-ranking prÃ©cis
    pairs = [[question, c['texte']] for c in candidats]
    scores = reranker.predict(pairs)

    # Ajouter scores et trier
    for i, candidat in enumerate(candidats):
        candidat['rerank_score'] = float(scores[i])

    candidats_tries = sorted(
        candidats,
        key=lambda x: x['rerank_score'],
        reverse=True
    )

    # Ã‰tape 3 : Retourner top k_final
    return candidats_tries[:k_final]
```

### 3. MÃ©tadonnÃ©es et Filtres AvancÃ©s

```python
def recherche_avec_filtres_avances(
    question,
    date_min=None,
    date_max=None,
    auteur=None,
    categorie=None,
    langue='fr'
):
    """Recherche avec filtres temporels et catÃ©goriels"""

    question_embedding = generer_embedding(question)

    sql = """
        SELECT
            id, document_titre, chunk_text,
            metadata,
            1 - (embedding <=> %s::vector) AS similarite
        FROM knowledge_base
        WHERE metadata->>'langue' = %s
    """

    params = [question_embedding, langue]

    # Filtres optionnels
    if date_min:
        sql += " AND (metadata->>'date')::date >= %s"
        params.append(date_min)

    if date_max:
        sql += " AND (metadata->>'date')::date <= %s"
        params.append(date_max)

    if auteur:
        sql += " AND metadata->>'auteur' = %s"
        params.append(auteur)

    if categorie:
        sql += " AND metadata->>'categorie' = %s"
        params.append(categorie)

    sql += """
        ORDER BY embedding <=> %s::vector
        LIMIT 5
    """
    params.append(question_embedding)

    with conn.cursor() as cur:
        cur.execute(sql, params)
        return cur.fetchall()
```

### 4. Caching des Embeddings

```python
import hashlib
import json

# Table de cache
"""
CREATE TABLE embedding_cache (
    query_hash TEXT PRIMARY KEY,
    query_text TEXT NOT NULL,
    embedding vector(1536) NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_cache_created ON embedding_cache(created_at);
"""

def generer_embedding_avec_cache(texte):
    """GÃ©nÃ¨re ou rÃ©cupÃ¨re un embedding depuis le cache"""

    # Hash du texte pour clÃ© de cache
    query_hash = hashlib.md5(texte.encode()).hexdigest()

    # VÃ©rifier cache
    with conn.cursor() as cur:
        cur.execute(
            "SELECT embedding FROM embedding_cache WHERE query_hash = %s",
            (query_hash,)
        )
        result = cur.fetchone()

        if result:
            # Cache hit
            return result[0]

    # Cache miss : gÃ©nÃ©rer
    embedding = generer_embedding(texte)

    # Stocker en cache
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO embedding_cache (query_hash, query_text, embedding)
            VALUES (%s, %s, %s)
            ON CONFLICT (query_hash) DO NOTHING
            """,
            (query_hash, texte, embedding)
        )
    conn.commit()

    return embedding
```

### 5. Streaming des RÃ©ponses

```python
def generer_reponse_rag_stream(question, k=3):
    """RAG avec streaming de la rÃ©ponse pour UX temps rÃ©el"""

    # RÃ©cupÃ©ration (rapide)
    contexte = recuperer_contexte(question, k=k)

    if not contexte:
        yield "Je n'ai pas trouvÃ© d'information pertinente."
        return

    # Construire prompt
    contexte_texte = "\n\n".join([c['texte'] for c in contexte])
    prompt = f"Contexte:\n{contexte_texte}\n\nQuestion: {question}\n\nRÃ©ponse:"

    # Streaming de ChatGPT
    stream = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Tu rÃ©ponds en te basant sur le contexte."},
            {"role": "user", "content": prompt}
        ],
        stream=True,
        temperature=0.3
    )

    # Yield chaque chunk
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# Utilisation (ex: avec FastAPI)
from fastapi.responses import StreamingResponse

@app.get("/chat")
async def chat_stream(question: str):
    return StreamingResponse(
        generer_reponse_rag_stream(question),
        media_type="text/plain"
    )
```

---

## Bonnes Pratiques

### 1. Chunking Optimal

**Taille de chunk** :

| Type de contenu | Taille recommandÃ©e | Raison |
|-----------------|-------------------|--------|
| FAQ / Questions courtes | 200-300 tokens | Une question = un chunk |
| Documentation technique | 500-800 tokens | Un concept = un chunk |
| Articles longs | 800-1000 tokens | Un paragraphe/section |
| Livres / ThÃ¨ses | 1000-1500 tokens | Une sous-section |

**Overlap** :
- RecommandÃ© : 10-20% de la taille du chunk
- Ã‰vite de couper des phrases/concepts au milieu

### 2. QualitÃ© des Embeddings

**Choix du modÃ¨le** :

| ModÃ¨le | Dimensions | CoÃ»t | QualitÃ© | Usage |
|--------|-----------|------|---------|-------|
| OpenAI text-embedding-3-small | 1536 | $0.02/1M tokens | â­â­â­â­ | Production (recommandÃ©) |
| OpenAI text-embedding-3-large | 3072 | $0.13/1M tokens | â­â­â­â­â­ | Haute prÃ©cision |
| Sentence-BERT (local) | 384-768 | Gratuit | â­â­â­ | Budget limitÃ©, privacy |
| Cohere Embed v3 | 1024 | $0.10/1M tokens | â­â­â­â­ | Alternative |

**ConsidÃ©rations** :
- MÃªme modÃ¨le pour indexation ET recherche
- ModÃ¨les multilingues si contenu multi-langues
- ModÃ¨les locaux pour donnÃ©es sensibles

### 3. Nombre de Chunks (k)

| k | Tokens contexte | Cas d'usage |
|---|----------------|-------------|
| 1-2 | ~500-1000 | Questions simples, FAQ |
| 3-5 | ~1500-4000 | Standard (recommandÃ©) |
| 5-10 | ~4000-10000 | Sujets complexes, comparaison |
| 10+ | 10000+ | SynthÃ¨se large, recherche exhaustive |

**Attention** : Limites de tokens des modÃ¨les LLM
- GPT-3.5-turbo : 16K tokens
- GPT-4 : 128K tokens
- GPT-4-turbo : 128K tokens

### 4. Prompt Engineering pour RAG

**Structure du prompt** :

```python
PROMPT_TEMPLATE = """Tu es un assistant expert qui rÃ©pond aux questions en utilisant UNIQUEMENT le contexte fourni.

RÃˆGLES IMPORTANTES :
1. Base ta rÃ©ponse UNIQUEMENT sur le contexte ci-dessous
2. Si la rÃ©ponse n'est pas dans le contexte, dis "Je ne trouve pas cette information dans les documents fournis"
3. Cite tes sources en mentionnant les documents
4. Sois prÃ©cis et factuel
5. Si tu n'es pas sÃ»r, dis-le

CONTEXTE :
{contexte}

QUESTION : {question}

RÃ‰PONSE (basÃ©e sur le contexte ci-dessus) :"""
```

### 5. Ã‰valuation de la QualitÃ©

```python
def evaluer_qualite_rag(questions_test, reponses_attendues):
    """Ã‰value la qualitÃ© d'un systÃ¨me RAG"""

    metriques = {
        'recall': [],      # Trouve-t-il les bonnes infos ?
        'precision': [],   # Les infos sont-elles pertinentes ?
        'latence': []      # Temps de rÃ©ponse
    }

    for question, reponse_attendue in zip(questions_test, reponses_attendues):
        import time
        start = time.time()

        # GÃ©nÃ©rer rÃ©ponse
        resultat = generer_reponse_rag(question)
        latence = time.time() - start

        # Ã‰valuer (simplifiÃ©, utiliser des mÃ©triques plus sophistiquÃ©es en prod)
        contexte_recupere = resultat['contexte']
        reponse_generee = resultat['reponse']

        # Recall : les bons chunks sont-ils rÃ©cupÃ©rÃ©s ?
        # Precision : les chunks rÃ©cupÃ©rÃ©s sont-ils pertinents ?
        # (NÃ©cessite ground truth des chunks pertinents)

        metriques['latence'].append(latence)

    return {
        'latence_moyenne': sum(metriques['latence']) / len(metriques['latence']),
        'latence_p95': sorted(metriques['latence'])[int(0.95 * len(metriques['latence']))],
    }
```

---

## Comparaison des Approches

### RAG vs Fine-tuning

| CritÃ¨re | RAG | Fine-tuning LLM |
|---------|-----|----------------|
| **CoÃ»t** | ğŸŸ¢ Faible (API embeddings + LLM) | ğŸ”´ Ã‰levÃ© (entraÃ®nement GPU) |
| **Mise Ã  jour donnÃ©es** | ğŸŸ¢ Temps rÃ©el (ajouter docs) | ğŸ”´ RÃ©entraÃ®nement complet |
| **DonnÃ©es sensibles** | ğŸŸ¢ Restent en DB privÃ©e | ğŸ”´ IntÃ©grÃ©es au modÃ¨le |
| **Transparence** | ğŸŸ¢ Sources citables | ğŸŸ¡ BoÃ®te noire |
| **PrÃ©cision** | ğŸŸ¢ Excellente (si bons docs) | ğŸŸ¢ Excellente |
| **Latence** | ğŸŸ¡ ~1-3s (recherche + gÃ©nÃ©ration) | ğŸŸ¢ ~0.5s (gÃ©nÃ©ration seule) |
| **Cas d'usage** | FAQ, docs, support | Ton, style, domaine spÃ©cialisÃ© |

**Recommandation** : RAG pour la plupart des cas, fine-tuning pour personnalisation style/ton.

### RAG vs Embeddings seuls (sans LLM)

| CritÃ¨re | RAG (Embeddings + LLM) | Embeddings seuls |
|---------|----------------------|-----------------|
| **RÃ©ponse** | âœ… Texte gÃ©nÃ©rÃ©, synthÃ¨se | âŒ Chunks bruts |
| **ComprÃ©hension** | âœ… Reformulation, clarification | âŒ Texte source tel quel |
| **Citations** | âœ… Sources + explication | âœ… Sources uniquement |
| **CoÃ»t** | ğŸŸ¡ Moyen ($0.002/requÃªte) | ğŸŸ¢ Faible ($0.0001/requÃªte) |
| **Latence** | ğŸŸ¡ 1-3s | ğŸŸ¢ 0.1-0.5s |
| **Cas d'usage** | Chatbot conversationnel | Recherche documentaire pure |

**Recommandation** :
- Embeddings seuls pour recherche rapide sans gÃ©nÃ©ration
- RAG pour interface conversationnelle avec synthÃ¨se

---

## Checklist de Production

### Avant dÃ©ploiement

âœ… **DonnÃ©es** :
- [ ] Documents nettoyÃ©s et formatÃ©s
- [ ] Chunking optimisÃ© (taille + overlap)
- [ ] MÃ©tadonnÃ©es enrichies
- [ ] Index pgvector crÃ©Ã©s (HNSW)

âœ… **Performance** :
- [ ] Temps de recherche < 500ms
- [ ] Temps de gÃ©nÃ©ration < 3s
- [ ] Cache des embeddings activÃ©
- [ ] Connection pooling configurÃ©

âœ… **QualitÃ©** :
- [ ] Tests sur dataset d'Ã©valuation
- [ ] Recall > 90%
- [ ] Prompt optimisÃ© pour LLM
- [ ] Gestion des cas "pas de rÃ©ponse"

âœ… **SÃ©curitÃ©** :
- [ ] Authentification API LLM
- [ ] Rate limiting
- [ ] Filtrage contenu sensible
- [ ] Logs des requÃªtes (anonymisÃ©s)

### Monitoring continu

âœ… **MÃ©triques** :
- [ ] Latence p50, p95, p99
- [ ] Taux de rÃ©ussite (rÃ©ponse vs "pas de rÃ©ponse")
- [ ] CoÃ»t API embeddings/LLM
- [ ] Utilisation CPU/RAM/Disque

âœ… **QualitÃ©** :
- [ ] Feedback utilisateurs (ğŸ‘/ğŸ‘)
- [ ] Revue manuelle Ã©chantillon
- [ ] A/B testing des prompts

---

## Conclusion

### Points clÃ©s Ã  retenir

âœ… **Semantic Search** : Comprend le sens, pas seulement les mots-clÃ©s
- Utilise embeddings + pgvector
- Trouve synonymes et concepts liÃ©s
- IdÃ©al pour : documentation, FAQ, recherche intelligente

âœ… **RAG** : ChatGPT + vos donnÃ©es privÃ©es
- Architecture : RÃ©cupÃ©ration (pgvector) â†’ Augmentation (prompt) â†’ GÃ©nÃ©ration (LLM)
- Chunking optimal : 500-800 tokens avec overlap
- k = 3-5 chunks pour la plupart des cas

âœ… **Avantages RAG** :
- DonnÃ©es toujours Ã  jour (temps rÃ©el)
- Transparence (sources citables)
- CoÃ»t raisonnable vs fine-tuning
- Pas de hallucinations sur vos donnÃ©es

âš ï¸ **ConsidÃ©rations** :
- QualitÃ© des embeddings cruciale
- Chunking impacte directement qualitÃ©
- Monitoring et Ã©valuation continues nÃ©cessaires

ğŸ¯ **RAG + pgvector = Chatbot intelligent sur VOS donnÃ©es !** ğŸš€

### Prochaines Ã©tapes

Pour aller plus loin :
1. **ImplÃ©menter** : CrÃ©er un POC avec vos documents
2. **ExpÃ©rimenter** : Tester diffÃ©rents chunking/k/prompts
3. **Optimiser** : Hybrid search, re-ranking, caching
4. **DÃ©ployer** : FastAPI + Streamlit pour interface

### Ressources

- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [pgvector Examples](https://github.com/pgvector/pgvector-python)
- [RAG Best Practices](https://www.anthropic.com/index/retrieval-augmented-generation-rag)
- Livre : *Building LLM Applications* (O'Reilly)

---


â­ï¸ [Autres extensions notables](/18-extensions-et-integrations/07-autres-extensions.md)
