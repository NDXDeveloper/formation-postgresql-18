ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 18.6.1. Recherche de SimilaritÃ© (cosine, L2)

## Introduction

Imaginez que vous voulez trouver des produits "similaires" Ã  un produit donnÃ©, ou des documents qui "ressemblent" Ã  une recherche, ou encore recommander des films qu'un utilisateur pourrait aimer. Comment faire ?

La **recherche de similaritÃ©** (similarity search) est la capacitÃ© de trouver des Ã©lÃ©ments "proches" ou "similaires" Ã  un Ã©lÃ©ment de rÃ©fÃ©rence, en se basant sur leurs caractÃ©ristiques.

Avec **pgvector**, PostgreSQL peut stocker et rechercher des **vecteurs** (reprÃ©sentations mathÃ©matiques d'objets) et calculer leur similaritÃ© ultra-rapidement.

### Analogie : La bibliothÃ¨que magique

Imaginez une bibliothÃ¨que oÃ¹ chaque livre est reprÃ©sentÃ© par sa "position" dans un espace imaginaire :
- Les livres de science-fiction sont regroupÃ©s dans une zone
- Les romans d'amour dans une autre zone
- Les livres techniques dans encore une autre

Quand vous demandez "trouve-moi des livres comme celui-ci", le bibliothÃ©caire mesure la **distance** entre votre livre et tous les autres, puis vous donne les plus proches.

**pgvector** fait exactement Ã§a, mais avec des mathÃ©matiques prÃ©cises et Ã  grande vitesse ! ğŸš€

---

## Qu'est-ce qu'un Vecteur ?

### DÃ©finition simple

Un **vecteur** est une liste ordonnÃ©e de nombres qui reprÃ©sente un objet dans un espace mathÃ©matique.

**Exemples** :

```
Vecteur 2D (2 dimensions) : [3.5, 2.1]
Vecteur 3D (3 dimensions) : [1.0, 4.2, -2.5]
Vecteur 1536D (1536 dimensions) : [0.023, -0.891, 0.445, ..., 0.234]
                                    â†‘ Typique pour OpenAI embeddings
```

### Visualisation (2D et 3D)

**Vecteurs 2D** (faciles Ã  visualiser) :

```
        Y
        â†‘
      5 |           â€¢ (4, 4)
      4 |         /
      3 |       /
      2 |     â€¢ (2, 2)
      1 |   /
      0 +--+--+--+--+--â†’ X
        0  1  2  3  4  5

Point A = [2, 2]
Point B = [4, 4]

Ces deux points sont "proches" dans l'espace 2D.
```

**Vecteurs 3D** :

```
      Z
      â†‘
      |     â€¢ (3, 4, 2)
      |    /|
      |   / |
      |  /  â€¢ (1, 2, 1)
      | /   |
      +-----+----â†’ Y
     /
    /
   â†“
   X
```

**En rÃ©alitÃ©, nous travaillons souvent avec des vecteurs Ã  100, 768, 1536 dimensions** (impossibles Ã  visualiser, mais le principe est le mÃªme).

### Embeddings : De texte Ã  vecteur

Un **embedding** est la transformation d'un objet (texte, image, son) en vecteur numÃ©rique.

**Exemple avec du texte** :

```
Phrase : "J'aime les chats"
         â†“ ModÃ¨le d'embedding (par ex. OpenAI text-embedding-3-small)
Vecteur : [0.023, -0.154, 0.387, ..., 0.892]  (1536 dimensions)
```

**Magie** : Des phrases similaires auront des vecteurs proches !

```
"J'aime les chats"        â†’ [0.023, -0.154, 0.387, ...]
"J'adore les fÃ©lins"      â†’ [0.029, -0.148, 0.391, ...]  â† Proche !
"Le temps est nuageux"    â†’ [-0.892, 0.445, -0.234, ...]  â† Ã‰loignÃ©
```

---

## Extension pgvector

### Qu'est-ce que pgvector ?

**pgvector** est une extension PostgreSQL open-source qui ajoute :
- Un type de donnÃ©es `vector` pour stocker des vecteurs
- Des opÃ©rateurs de calcul de distance/similaritÃ©
- Des index spÃ©cialisÃ©s pour recherche rapide

**DÃ©veloppÃ©e par** : Ankane (Andrew Kane)
**Licence** : Open-source (PostgreSQL License)
**PopularitÃ©** : UtilisÃ©e par Supabase, Neon, et des milliers d'applications IA

### Installation

#### MÃ©thode 1 : Via package (recommandÃ©)

```bash
# Ubuntu/Debian
sudo apt-get install postgresql-16-pgvector

# macOS (Homebrew)
brew install pgvector

# Ou depuis les sources
git clone https://github.com/pgvector/pgvector.git
cd pgvector
make
sudo make install
```

#### MÃ©thode 2 : Extension activÃ©e par dÃ©faut

Sur des services managÃ©s comme **Supabase**, **Neon**, **AWS RDS** (certaines versions), pgvector est souvent prÃ©-installÃ©.

### Activer l'extension

```sql
-- Dans votre base de donnÃ©es
CREATE EXTENSION vector;
```

**VÃ©rification** :

```sql
SELECT * FROM pg_available_extensions WHERE name = 'vector';
```

RÃ©sultat :
```
  name   | default_version | installed_version |     comment
---------+-----------------+-------------------+------------------
 vector  | 0.7.0           | 0.7.0             | vector data type
```

### Type de donnÃ©es vector

Une fois l'extension activÃ©e, vous avez accÃ¨s au type `vector(n)` oÃ¹ `n` est le nombre de dimensions.

```sql
-- CrÃ©er une table avec un vecteur
CREATE TABLE produits (
    id SERIAL PRIMARY KEY,
    nom TEXT,
    embedding vector(1536)  -- Vecteur de 1536 dimensions
);
```

**Insertion** :

```sql
INSERT INTO produits (nom, embedding) VALUES
    ('Ordinateur portable', '[0.1, 0.2, 0.3, ..., 0.9]'),  -- 1536 valeurs
    ('Souris sans fil', '[0.15, 0.25, 0.35, ..., 0.85]');
```

---

## MÃ©triques de Distance et SimilaritÃ©

### Concept : Distance vs SimilaritÃ©

**Distance** : Mesure de l'Ã©loignement entre deux vecteurs
- Plus la distance est **petite**, plus les vecteurs sont **proches/similaires**
- Exemple : Distance = 0.1 (trÃ¨s proches), Distance = 5.0 (trÃ¨s Ã©loignÃ©s)

**SimilaritÃ©** : Mesure de la ressemblance entre deux vecteurs
- Plus la similaritÃ© est **Ã©levÃ©e**, plus les vecteurs sont **similaires**
- Exemple : SimilaritÃ© = 0.95 (trÃ¨s similaires), SimilaritÃ© = 0.2 (peu similaires)

### Les 3 principales mÃ©triques

pgvector supporte trois mÃ©triques principales :

| MÃ©trique | OpÃ©rateur pgvector | Type | Plage | Usage principal |
|----------|-------------------|------|-------|----------------|
| **Distance L2 (euclidienne)** | `<->` | Distance | [0, âˆ) | GÃ©nÃ©ral, images |
| **Distance cosine** | `<=>` | Distance | [0, 2] | Texte, embeddings |
| **Produit scalaire (inner product)** | `<#>` | SimilaritÃ© | (-âˆ, âˆ) | Recommandation |

---

## 1. Distance L2 (Euclidienne)

### DÃ©finition

La **distance euclidienne** (ou distance L2) est la distance "en ligne droite" entre deux points dans l'espace.

**Formule mathÃ©matique** :

```
Pour deux vecteurs A = [aâ‚, aâ‚‚, ..., aâ‚™] et B = [bâ‚, bâ‚‚, ..., bâ‚™]

Distance L2(A, B) = âˆš[(aâ‚-bâ‚)Â² + (aâ‚‚-bâ‚‚)Â² + ... + (aâ‚™-bâ‚™)Â²]
```

### Visualisation 2D

```
        Y
        â†‘
      5 |           â€¢ B (4, 4)
      4 |         /|
      3 |       / |
      2 |     /   |
      1 |   â€¢ A   |
        |  (2,1)  |
      0 +--+------+----â†’ X
        0  2      4

Distance = âˆš[(4-2)Â² + (4-1)Â²]
        = âˆš[2Â² + 3Â²]
        = âˆš[4 + 9]
        = âˆš13
        â‰ˆ 3.61
```

### PropriÃ©tÃ©s

âœ… **Intuitive** : Correspond Ã  notre notion de "distance physique"

âœ… **SymÃ©trique** : `distance(A, B) = distance(B, A)`

âœ… **Triangle inequality** : `distance(A, C) â‰¤ distance(A, B) + distance(B, C)`

âš ï¸ **Sensible Ã  la magnitude** : Les vecteurs de grande magnitude ont plus d'influence

### Utilisation avec pgvector

**OpÃ©rateur** : `<->`

```sql
-- Trouver les 5 produits les plus proches du produit #1
SELECT
    id,
    nom,
    embedding <-> (SELECT embedding FROM produits WHERE id = 1) AS distance
FROM produits
WHERE id != 1  -- Exclure le produit lui-mÃªme
ORDER BY distance ASC
LIMIT 5;
```

**RÃ©sultat exemple** :

```
 id |        nom         | distance
----+--------------------+----------
  3 | Clavier mÃ©canique  |   0.234
  7 | Ã‰cran 27 pouces    |   0.456
 12 | Webcam HD          |   0.589
  5 | Hub USB            |   0.634
 18 | Casque audio       |   0.712
```

### Exemple complet

```sql
-- CrÃ©er une table de films avec embeddings
CREATE TABLE films (
    id SERIAL PRIMARY KEY,
    titre TEXT,
    genre TEXT,
    embedding vector(128)  -- 128 dimensions pour l'exemple
);

-- InsÃ©rer des films (embeddings gÃ©nÃ©rÃ©s par un modÃ¨le IA)
INSERT INTO films (titre, genre, embedding) VALUES
    ('Inception', 'Sci-Fi', '[0.1, 0.8, 0.3, ..., 0.5]'),
    ('Interstellar', 'Sci-Fi', '[0.12, 0.79, 0.31, ..., 0.52]'),
    ('Le Seigneur des Anneaux', 'Fantasy', '[0.9, 0.2, 0.1, ..., 0.3]'),
    ('Harry Potter', 'Fantasy', '[0.88, 0.21, 0.12, ..., 0.31]'),
    ('Titanic', 'Romance', '[0.2, 0.1, 0.9, ..., 0.8]');

-- Rechercher les films similaires Ã  "Inception"
SELECT
    titre,
    genre,
    embedding <-> (SELECT embedding FROM films WHERE titre = 'Inception') AS distance
FROM films
WHERE titre != 'Inception'
ORDER BY distance ASC
LIMIT 3;
```

**RÃ©sultat** :

```
       titre        |  genre  | distance
--------------------+---------+----------
 Interstellar       | Sci-Fi  |   0.087   â† TrÃ¨s proche (mÃªme genre)
 Le Seigneur...     | Fantasy |   1.234
 Harry Potter       | Fantasy |   1.289
```

### Cas d'usage de L2

âœ… **Images** : Recherche d'images similaires (embeddings CNN)

âœ… **DonnÃ©es numÃ©riques** : Comparaison de profils utilisateurs avec features numÃ©riques

âœ… **GÃ©ospatial** : Recherche de points gÃ©ographiques proches (coordonnÃ©es)

âœ… **GÃ©nÃ©ral** : Quand la magnitude des vecteurs est importante

---

## 2. Distance Cosine

### DÃ©finition

La **distance cosine** mesure l'angle entre deux vecteurs, **indÃ©pendamment de leur magnitude**.

**Concept clÃ©** : Deux vecteurs pointant dans la mÃªme direction sont considÃ©rÃ©s comme similaires, mÃªme si l'un est "plus long" que l'autre.

### Visualisation 2D

```
        Y
        â†‘
      8 |         â€¢ B [4, 8]
      7 |        /
      6 |       /
      5 |      /
      4 |     /   â€¢ A [2, 4]
      3 |    /   /
      2 |   /   /
      1 |  /   /
      0 +--+--+----------â†’ X
        0  2  4

A = [2, 4]
B = [4, 8] = 2 Ã— A

Angle entre A et B = 0Â° (mÃªme direction)
â†’ SimilaritÃ© cosine = 1.0 (parfaitement similaires)
â†’ Distance cosine = 0.0
```

**MÃªme direction, magnitude diffÃ©rente = similaires pour cosine !**

### Formule mathÃ©matique

**SimilaritÃ© cosine** :

```
similarity_cosine(A, B) = (A Â· B) / (||A|| Ã— ||B||)

OÃ¹ :
- A Â· B = produit scalaire (somme des produits des composantes)
- ||A|| = norme (longueur) du vecteur A
- RÃ©sultat : -1 (opposÃ©s) Ã  +1 (identiques)
```

**Distance cosine** (utilisÃ©e par pgvector) :

```
distance_cosine(A, B) = 1 - similarity_cosine(A, B)

RÃ©sultat : 0 (identiques) Ã  2 (opposÃ©s)
```

### Exemple de calcul

**Vecteurs** :
- A = [1, 2, 3]
- B = [2, 4, 6] (= 2 Ã— A)

**Calcul** :

```
1. Produit scalaire : A Â· B = (1Ã—2) + (2Ã—4) + (3Ã—6) = 2 + 8 + 18 = 28

2. Normes :
   ||A|| = âˆš(1Â² + 2Â² + 3Â²) = âˆš14 â‰ˆ 3.742
   ||B|| = âˆš(2Â² + 4Â² + 6Â²) = âˆš56 â‰ˆ 7.483

3. SimilaritÃ© cosine :
   cos(Î¸) = 28 / (3.742 Ã— 7.483) = 28 / 28 = 1.0

4. Distance cosine :
   distance = 1 - 1.0 = 0.0  â† Parfaitement similaires
```

### PropriÃ©tÃ©s

âœ… **IndÃ©pendant de la magnitude** : Seule la direction compte

âœ… **NormalisÃ©** : Toujours entre 0 et 2

âœ… **IdÃ©al pour texte** : Les embeddings de texte bÃ©nÃ©ficient de cette propriÃ©tÃ©

âš ï¸ **Moins intuitif** : La notion de "distance" est moins Ã©vidente

### Utilisation avec pgvector

**OpÃ©rateur** : `<=>`

```sql
-- Recherche de similaritÃ© sÃ©mantique de documents
SELECT
    id,
    titre,
    contenu,
    embedding <=> (SELECT embedding FROM documents WHERE id = 1) AS distance_cosine
FROM documents
WHERE id != 1
ORDER BY distance_cosine ASC
LIMIT 5;
```

### Exemple complet : Recherche sÃ©mantique

```sql
-- CrÃ©er une table de documents avec embeddings
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    titre TEXT,
    contenu TEXT,
    embedding vector(1536)  -- OpenAI text-embedding-3-small
);

-- Fonction pour gÃ©nÃ©rer embedding (fictive, en rÃ©alitÃ© via API OpenAI)
-- En pratique, vous utiliseriez l'API OpenAI ou un modÃ¨le local

-- InsÃ©rer des documents
INSERT INTO documents (titre, contenu, embedding) VALUES
    (
        'Python pour dÃ©butants',
        'Apprendre Python est facile. Ce langage est idÃ©al pour commencer la programmation.',
        '[0.023, -0.154, 0.387, ..., 0.892]'  -- Embedding rÃ©el via OpenAI
    ),
    (
        'Introduction Ã  Python',
        'Python est un langage de programmation accessible aux dÃ©butants.',
        '[0.029, -0.148, 0.391, ..., 0.885]'  -- Similaire au premier
    ),
    (
        'Recette de cookies',
        'Pour faire des cookies, mÃ©langer beurre, sucre et farine.',
        '[-0.892, 0.445, -0.234, ..., 0.123]'  -- TrÃ¨s diffÃ©rent
    );

-- RequÃªte : "Trouver des documents similaires Ã  une recherche"
-- Supposons que nous avons l'embedding de la recherche "tutoriel programmation Python"

WITH recherche AS (
    SELECT '[0.025, -0.152, 0.389, ..., 0.888]'::vector(1536) AS query_embedding
)
SELECT
    d.id,
    d.titre,
    LEFT(d.contenu, 80) AS apercu,
    d.embedding <=> recherche.query_embedding AS distance
FROM documents d, recherche
ORDER BY distance ASC
LIMIT 3;
```

**RÃ©sultat** :

```
 id |         titre          |                      apercu                       | distance
----+------------------------+---------------------------------------------------+----------
  1 | Python pour dÃ©butants  | Apprendre Python est facile. Ce langage est...    |   0.012
  2 | Introduction Ã  Python  | Python est un langage de programmation access... |   0.015
  3 | Recette de cookies     | Pour faire des cookies, mÃ©langer beurre...        |   0.892
```

Les deux premiers documents (sur Python) sont trÃ¨s proches sÃ©mantiquement de la recherche !

### Pourquoi cosine pour le texte ?

**ProblÃ¨me avec L2 sur du texte** :

```
Document A : "chat"
  â†’ Embedding : [0.5, 0.3]
  â†’ Norme : 0.58

Document B : "chat chat chat chat chat"  (5 fois "chat")
  â†’ Embedding : [2.5, 1.5]  (5Ã— plus grand)
  â†’ Norme : 2.92

Distance L2 : TrÃ¨s grande (Ã  cause de la magnitude)
â†’ ConsidÃ©rÃ©s comme DIFFÃ‰RENTS

Distance Cosine : Proche de 0 (mÃªme direction)
â†’ ConsidÃ©rÃ©s comme SIMILAIRES âœ…
```

**Conclusion** : Pour le texte, la frÃ©quence des mots (magnitude) ne devrait pas influencer la similaritÃ© sÃ©mantique.

### Cas d'usage de cosine

âœ… **Recherche sÃ©mantique** : Documents, articles, questions-rÃ©ponses

âœ… **Embeddings de texte** : OpenAI, Sentence Transformers, etc.

âœ… **Recommandation** : Profils utilisateurs, prÃ©fÃ©rences

âœ… **Classification** : Trouver la catÃ©gorie la plus proche

âœ… **RAG (Retrieval-Augmented Generation)** : RÃ©cupÃ©ration de contexte pour LLM

---

## 3. Produit Scalaire (Inner Product)

### DÃ©finition

Le **produit scalaire** (dot product ou inner product) mesure Ã  quel point deux vecteurs "pointent dans la mÃªme direction" ET ont une grande magnitude.

**Formule** :

```
A Â· B = aâ‚Ã—bâ‚ + aâ‚‚Ã—bâ‚‚ + ... + aâ‚™Ã—bâ‚™
```

### DiffÃ©rence avec cosine

- **Cosine** : Ignore la magnitude (normalise)
- **Inner product** : Prend en compte la magnitude

**Analogie** :
- Cosine : "Ces deux personnes ont les mÃªmes goÃ»ts ?"
- Inner product : "Ces deux personnes ont les mÃªmes goÃ»ts ET sont trÃ¨s passionnÃ©es ?"

### Utilisation avec pgvector

**OpÃ©rateur** : `<#>`

**Note** : pgvector retourne `-1 * inner_product` (nÃ©gation) pour que l'ordre ASC donne les plus similaires.

```sql
-- Recommandation de produits
SELECT
    id,
    nom,
    embedding <#> '[0.1, 0.2, 0.3, ..., 0.9]'::vector AS neg_inner_product
FROM produits
ORDER BY neg_inner_product ASC  -- Les plus nÃ©gatifs = plus grands inner products = plus similaires
LIMIT 10;
```

### Cas d'usage

âœ… **SystÃ¨mes de recommandation** : Combinaison de prÃ©fÃ©rences + intensitÃ©

âœ… **Ranking** : Classer par pertinence avec poids

âš ï¸ Moins utilisÃ© que L2 et cosine pour la recherche de similaritÃ© standard

---

## Comparaison des MÃ©triques

### Tableau rÃ©capitulatif

| CritÃ¨re | L2 (Euclidienne) | Cosine | Inner Product |
|---------|------------------|--------|---------------|
| **OpÃ©rateur pgvector** | `<->` | `<=>` | `<#>` |
| **SensibilitÃ© magnitude** | âœ… Oui | âŒ Non | âœ… Oui |
| **Plage de valeurs** | [0, âˆ) | [0, 2] | (-âˆ, âˆ) |
| **NormalisÃ©** | âŒ Non | âœ… Oui | âŒ Non |
| **IdÃ©al pour texte** | âš ï¸ Moyen | âœ…âœ… Excellent | âš ï¸ Moyen |
| **IdÃ©al pour images** | âœ…âœ… Excellent | âœ… Bon | âš ï¸ Moyen |
| **RapiditÃ© calcul** | ğŸŸ¡ Moyen | ğŸŸ¢ Rapide | ğŸŸ¢ Rapide |
| **Usage principal** | GÃ©nÃ©ral | Embeddings texte | Recommandation |

### Choix de la mÃ©trique : Arbre de dÃ©cision

```
Vous travaillez avec des embeddings de texte ?
â”‚
â”œâ”€ Oui â†’ Utilisez COSINE (<=>)
â”‚         âœ… OpenAI, Sentence Transformers, etc.
â”‚
â””â”€ Non
    â”‚
    â”œâ”€ Images / CNN embeddings ?
    â”‚   â”‚
    â”‚   â””â”€ Oui â†’ Utilisez L2 (<->)
    â”‚
    â””â”€ SystÃ¨me de recommandation avec poids ?
        â”‚
        â””â”€ Oui â†’ ConsidÃ©rez INNER PRODUCT (<#>)
             Sinon â†’ Utilisez L2 (<->)
```

### Benchmark comparatif

**ScÃ©nario** : 1 million de vecteurs de 1536 dimensions

| MÃ©trique | Temps de recherche | PrÃ©cision | Cas d'usage |
|----------|-------------------|-----------|-------------|
| **L2** | 45 ms | Haute | Images, donnÃ©es numÃ©riques |
| **Cosine** | 38 ms | Haute | Texte, sÃ©mantique |
| **Inner Product** | 35 ms | Moyenne | Recommandation avec scores |

---

## Exemples Pratiques Complets

### Exemple 1 : Recherche sÃ©mantique de produits

```sql
-- CrÃ©er une table de produits e-commerce
CREATE TABLE produits_ecommerce (
    id SERIAL PRIMARY KEY,
    nom TEXT NOT NULL,
    description TEXT,
    categorie TEXT,
    prix NUMERIC(10, 2),
    embedding vector(1536)  -- Embedding de la description
);

-- InsÃ©rer des produits (embeddings gÃ©nÃ©rÃ©s via OpenAI API)
INSERT INTO produits_ecommerce (nom, description, categorie, prix, embedding) VALUES
    (
        'MacBook Pro 16"',
        'Ordinateur portable haute performance pour professionnels crÃ©atifs',
        'Informatique',
        2499.99,
        '[0.023, -0.154, 0.387, ..., 0.892]'
    ),
    (
        'Dell XPS 15',
        'PC portable puissant pour dÃ©veloppeurs et designers',
        'Informatique',
        1899.99,
        '[0.029, -0.148, 0.391, ..., 0.885]'
    ),
    (
        'Chaise ergonomique',
        'SiÃ¨ge de bureau confortable pour longues sessions de travail',
        'Mobilier',
        349.99,
        '[-0.234, 0.567, -0.123, ..., 0.445]'
    );

-- Fonction de recherche (supposons que nous avons l'embedding de la recherche)
-- En production, vous appelleriez l'API OpenAI pour obtenir l'embedding

-- Recherche : "ordinateur portable pour dÃ©veloppeur"
WITH recherche AS (
    SELECT '[0.025, -0.152, 0.389, ..., 0.888]'::vector(1536) AS query_emb
)
SELECT
    p.id,
    p.nom,
    p.description,
    p.categorie,
    p.prix,
    (p.embedding <=> recherche.query_emb) AS distance_cosine,
    (1 - (p.embedding <=> recherche.query_emb)) AS similarite_cosine
FROM produits_ecommerce p, recherche
ORDER BY distance_cosine ASC
LIMIT 5;
```

**RÃ©sultat** :

```
 id |      nom       |                description                 | categorie  |  prix   | distance_cosine | similarite_cosine
----+----------------+--------------------------------------------+------------+---------+-----------------+-------------------
  2 | Dell XPS 15    | PC portable puissant pour dÃ©veloppeurs...  | Informatique| 1899.99 |     0.015      |      0.985
  1 | MacBook Pro 16"| Ordinateur portable haute performance...   | Informatique| 2499.99 |     0.023      |      0.977
  3 | Chaise ergo... | SiÃ¨ge de bureau confortable...             | Mobilier   |  349.99 |     0.892      |      0.108
```

Les ordinateurs portables sont identifiÃ©s comme trÃ¨s similaires Ã  la recherche !

### Exemple 2 : SystÃ¨me de questions-rÃ©ponses (FAQ)

```sql
-- Base de connaissances FAQ
CREATE TABLE faq (
    id SERIAL PRIMARY KEY,
    question TEXT NOT NULL,
    reponse TEXT NOT NULL,
    categorie TEXT,
    embedding vector(1536)
);

-- InsÃ©rer des questions FAQ
INSERT INTO faq (question, reponse, categorie, embedding) VALUES
    (
        'Comment rÃ©initialiser mon mot de passe ?',
        'Cliquez sur "Mot de passe oubliÃ©" sur la page de connexion, puis suivez les instructions par email.',
        'Compte',
        '[0.123, -0.456, 0.789, ..., 0.234]'
    ),
    (
        'J''ai oubliÃ© mon mot de passe, que faire ?',
        'Utilisez la fonctionnalitÃ© de rÃ©cupÃ©ration de mot de passe sur la page de login.',
        'Compte',
        '[0.125, -0.452, 0.791, ..., 0.238]'  -- TrÃ¨s similaire Ã  la question prÃ©cÃ©dente
    ),
    (
        'Quels sont les modes de paiement acceptÃ©s ?',
        'Nous acceptons les cartes bancaires (Visa, Mastercard), PayPal et virement bancaire.',
        'Paiement',
        '[-0.892, 0.445, -0.234, ..., 0.567]'
    );

-- Fonction de recherche dans la FAQ
CREATE OR REPLACE FUNCTION rechercher_faq(
    question_utilisateur TEXT,
    limite INTEGER DEFAULT 3
)
RETURNS TABLE (
    question TEXT,
    reponse TEXT,
    categorie TEXT,
    pertinence NUMERIC
) AS $$
DECLARE
    query_embedding vector(1536);
BEGIN
    -- En production, appel API pour obtenir l'embedding
    -- Pour cet exemple, simulons un embedding
    query_embedding := '[0.124, -0.454, 0.790, ..., 0.236]'::vector(1536);

    RETURN QUERY
    SELECT
        f.question,
        f.reponse,
        f.categorie,
        ROUND((1 - (f.embedding <=> query_embedding))::numeric, 3) AS pertinence
    FROM faq f
    ORDER BY f.embedding <=> query_embedding ASC
    LIMIT limite;
END;
$$ LANGUAGE plpgsql;

-- Utilisation
SELECT * FROM rechercher_faq('mot de passe perdu');
```

**RÃ©sultat** :

```
              question              |                     reponse                      | categorie | pertinence
------------------------------------+--------------------------------------------------+-----------+------------
 J'ai oubliÃ© mon mot de passe...   | Utilisez la fonctionnalitÃ© de rÃ©cupÃ©ration...   | Compte    |   0.987
 Comment rÃ©initialiser mon mot...  | Cliquez sur "Mot de passe oubliÃ©" sur...        | Compte    |   0.982
 Quels sont les modes de paiement..| Nous acceptons les cartes bancaires...          | Paiement  |   0.123
```

Les deux premiÃ¨res questions sont identifiÃ©es comme trÃ¨s pertinentes !

### Exemple 3 : DÃ©tection de doublons

```sql
-- DÃ©tecter des articles en double (mÃªme contenu, titres diffÃ©rents)
WITH doublons AS (
    SELECT
        a1.id AS id1,
        a1.titre AS titre1,
        a2.id AS id2,
        a2.titre AS titre2,
        a1.embedding <=> a2.embedding AS distance
    FROM articles a1
    CROSS JOIN articles a2
    WHERE a1.id < a2.id  -- Ã‰viter les duplicatas (a1-a2 et a2-a1)
      AND a1.embedding <=> a2.embedding < 0.05  -- Seuil de similaritÃ© trÃ¨s Ã©levÃ©
)
SELECT * FROM doublons
ORDER BY distance ASC;
```

**RÃ©sultat** :

```
 id1 |         titre1          | id2 |         titre2          | distance
-----+-------------------------+-----+-------------------------+----------
  12 | PostgreSQL Guide        |  34 | Guide complet PostgreSQL|   0.012
  45 | Python Tips             |  78 | Astuces Python          |   0.018
  89 | Docker Introduction     | 123 | Intro Ã  Docker          |   0.023
```

Articles potentiellement en double dÃ©tectÃ©s !

### Exemple 4 : Recommandation de contenu

```sql
-- Recommander des articles Ã  un utilisateur basÃ© sur son historique
CREATE TABLE lectures_utilisateur (
    utilisateur_id INTEGER,
    article_id INTEGER,
    date_lecture TIMESTAMP DEFAULT NOW()
);

-- Recommander des articles similaires Ã  ceux qu'un utilisateur a lus
WITH articles_lus AS (
    SELECT a.embedding
    FROM lectures_utilisateur lu
    INNER JOIN articles a ON lu.article_id = a.id
    WHERE lu.utilisateur_id = 42
      AND lu.date_lecture > NOW() - INTERVAL '30 days'
),
vecteur_moyen AS (
    -- Calculer le vecteur moyen des prÃ©fÃ©rences de l'utilisateur
    SELECT AVG(embedding) AS profil_utilisateur
    FROM articles_lus
)
SELECT
    a.id,
    a.titre,
    a.auteur,
    a.embedding <=> vm.profil_utilisateur AS distance
FROM articles a, vecteur_moyen vm
WHERE a.id NOT IN (
    SELECT article_id FROM lectures_utilisateur WHERE utilisateur_id = 42
)
ORDER BY distance ASC
LIMIT 10;
```

**RÃ©sultat** : Les 10 articles les plus pertinents pour l'utilisateur 42.

---

## Optimisation des Performances

### 1. Sans index : Scan complet (lent)

```sql
-- Recherche sans index
SELECT * FROM documents
ORDER BY embedding <=> '[0.1, 0.2, ..., 0.9]'::vector(1536)
LIMIT 10;

-- EXPLAIN montre : Seq Scan sur documents
-- Temps : 15 secondes pour 1M de vecteurs
```

**ProblÃ¨me** : PostgreSQL doit calculer la distance avec TOUS les vecteurs.

### 2. Index IVFFlat (rapide)

```sql
-- CrÃ©er un index IVFFlat
CREATE INDEX idx_documents_embedding_ivfflat
ON documents
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- Nombre de clusters

-- AprÃ¨s crÃ©ation de l'index
SET ivfflat.probes = 10;  -- Nombre de clusters Ã  rechercher

-- MÃªme recherche
SELECT * FROM documents
ORDER BY embedding <=> '[0.1, 0.2, ..., 0.9]'::vector(1536)
LIMIT 10;

-- EXPLAIN montre : Index Scan using idx_documents_embedding_ivfflat
-- Temps : 0.05 secondes pour 1M de vecteurs
```

**AmÃ©lioration** : 300Ã— plus rapide ! ğŸš€

### 3. Index HNSW (le plus rapide)

```sql
-- CrÃ©er un index HNSW (Hierarchical Navigable Small World)
CREATE INDEX idx_documents_embedding_hnsw
ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- ParamÃ¨tres de requÃªte
SET hnsw.ef_search = 40;

-- Recherche ultra-rapide
SELECT * FROM documents
ORDER BY embedding <=> '[0.1, 0.2, ..., 0.9]'::vector(1536)
LIMIT 10;

-- Temps : 0.01 secondes pour 1M de vecteurs
```

**AmÃ©lioration** : 1500Ã— plus rapide ! ğŸš€ğŸš€

### Comparaison des index

| MÃ©thode | Temps recherche (1M vecteurs) | PrÃ©cision | Espace disque | Construction |
|---------|------------------------------|-----------|---------------|--------------|
| **Sans index** | 15 s | 100% | 0 | ImmÃ©diat |
| **IVFFlat** | 0.05 s | ~95% | Moyen | Rapide |
| **HNSW** | 0.01 s | ~98% | Ã‰levÃ© | Lent |

**Recommandation** :
- **HNSW** pour production (meilleur compromis vitesse/prÃ©cision)
- **IVFFlat** si contraintes d'espace ou construction rapide nÃ©cessaire

---

## Cas d'Usage RÃ©els

### 1. Recherche sÃ©mantique (Semantic Search)

**Application** : Moteur de recherche intelligent

```sql
-- Utilisateur recherche : "comment rÃ©parer une fuite d'eau"
-- Le systÃ¨me trouve des articles mÃªme s'ils utilisent des termes diffÃ©rents :
-- - "RÃ©paration de plomberie"
-- - "Colmater une canalisation"
-- - "Urgence fuites domestiques"
```

**Avantage** : Comprend l'intention, pas seulement les mots-clÃ©s exacts.

### 2. RAG (Retrieval-Augmented Generation)

**Application** : Chatbots intelligents avec ChatGPT

**Workflow** :

```
1. Utilisateur : "Comment fonctionne la rÃ©plication PostgreSQL ?"
   â†“
2. GÃ©nÃ©rer embedding de la question (OpenAI API)
   â†“
3. Rechercher documents similaires dans pgvector
   SELECT * FROM documentation
   ORDER BY embedding <=> question_embedding
   LIMIT 3
   â†“
4. Contexte rÃ©cupÃ©rÃ© + question â†’ ChatGPT
   â†“
5. RÃ©ponse prÃ©cise et contextualisÃ©e
```

**Exemple en production** : Supabase Docs Search, Notion AI, etc.

### 3. Recommandation de produits

**Application** : E-commerce

```sql
-- "Les clients qui ont aimÃ© X ont aussi aimÃ©..."
SELECT p2.nom, p2.prix
FROM produits p1
CROSS JOIN produits p2
WHERE p1.id = 123  -- Produit actuel
  AND p2.id != 123
ORDER BY p1.embedding <=> p2.embedding ASC
LIMIT 5;
```

### 4. DÃ©tection de plagiat

**Application** : VÃ©rification d'originalitÃ© de contenu

```sql
-- Trouver des textes trÃ¨s similaires (potentiel plagiat)
SELECT
    t1.titre,
    t2.titre,
    1 - (t1.embedding <=> t2.embedding) AS similarite
FROM textes t1
CROSS JOIN textes t2
WHERE t1.id < t2.id
  AND t1.embedding <=> t2.embedding < 0.1  -- > 90% similaire
ORDER BY similarite DESC;
```

### 5. Classification automatique

**Application** : CatÃ©goriser des tickets de support

```sql
-- CrÃ©er des embeddings de catÃ©gories de rÃ©fÃ©rence
CREATE TABLE categories_support (
    categorie TEXT PRIMARY KEY,
    description TEXT,
    embedding vector(1536)
);

-- Classifier un nouveau ticket
WITH nouveau_ticket AS (
    SELECT '[0.123, -0.456, ..., 0.789]'::vector(1536) AS ticket_embedding
)
SELECT
    c.categorie,
    c.description,
    1 - (c.embedding <=> nt.ticket_embedding) AS confiance
FROM categories_support c, nouveau_ticket nt
ORDER BY confiance DESC
LIMIT 1;
```

**RÃ©sultat** : "Facturation" (confiance : 0.94)

### 6. Image similaire (Computer Vision)

**Application** : Recherche d'images par similaritÃ© visuelle

```sql
-- Stocker des embeddings d'images (gÃ©nÃ©rÃ©s via CNN : ResNet, VGG, etc.)
CREATE TABLE images (
    id SERIAL PRIMARY KEY,
    url TEXT,
    description TEXT,
    embedding vector(512)  -- ResNet-50 gÃ©nÃ¨re des vecteurs de 512D
);

-- Recherche d'images similaires
SELECT
    i2.url,
    i2.description,
    i1.embedding <-> i2.embedding AS distance
FROM images i1
CROSS JOIN images i2
WHERE i1.id = 456  -- Image de rÃ©fÃ©rence
  AND i2.id != 456
ORDER BY distance ASC
LIMIT 10;
```

---

## IntÃ©gration avec l'IA

### Workflow typique avec OpenAI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. GÃ©nÃ©rer embeddings (Application)                         â”‚
â”‚     - Python, Node.js, etc.                                  â”‚
â”‚     - Appel API OpenAI text-embedding-3-small                â”‚
â”‚     - RÃ©sultat : vecteur de 1536 dimensions                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“ INSERT INTO documents (texte, embedding)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Stocker dans PostgreSQL (pgvector)                       â”‚
â”‚     - Colonne type vector(1536)                              â”‚
â”‚     - Index HNSW pour recherche rapide                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“ Question utilisateur
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Rechercher (Application)                                 â”‚
â”‚     - GÃ©nÃ©rer embedding de la question (OpenAI)              â”‚
â”‚     - SELECT ... ORDER BY embedding <=> question_emb         â”‚
â”‚     - RÃ©cupÃ©rer top K documents                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“ Contexte + Question
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. GÃ©nÃ©rer rÃ©ponse (ChatGPT)                                â”‚
â”‚     - Prompt : Contexte + Question                           â”‚
â”‚     - API OpenAI Chat Completions                            â”‚
â”‚     - RÃ©ponse finale Ã  l'utilisateur                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemple de code Python

```python
import openai
import psycopg2
from psycopg2.extras import execute_values

# Configuration
openai.api_key = "votre_clÃ©_api"
conn = psycopg2.connect("postgresql://user:pass@localhost/dbname")

# 1. GÃ©nÃ©rer embedding d'un texte
def get_embedding(text, model="text-embedding-3-small"):
    response = openai.embeddings.create(
        input=text,
        model=model
    )
    return response.data[0].embedding

# 2. Stocker dans PostgreSQL
def ajouter_document(titre, contenu):
    embedding = get_embedding(contenu)

    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO documents (titre, contenu, embedding)
            VALUES (%s, %s, %s)
            """,
            (titre, contenu, embedding)
        )
    conn.commit()

# 3. Rechercher des documents similaires
def rechercher(question, limit=3):
    question_emb = get_embedding(question)

    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT
                titre,
                contenu,
                1 - (embedding <=> %s::vector) AS similarite
            FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT %s
            """,
            (question_emb, question_emb, limit)
        )
        return cur.fetchall()

# 4. RAG : RÃ©cupÃ©ration + GÃ©nÃ©ration
def chatbot_rag(question):
    # RÃ©cupÃ©rer contexte pertinent
    documents = rechercher(question, limit=3)

    # Construire le contexte
    contexte = "\n\n".join([doc[1] for doc in documents])

    # GÃ©nÃ©rer rÃ©ponse avec ChatGPT
    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Tu es un assistant qui rÃ©pond en utilisant le contexte fourni."},
            {"role": "user", "content": f"Contexte:\n{contexte}\n\nQuestion: {question}"}
        ]
    )

    return response.choices[0].message.content

# Utilisation
reponse = chatbot_rag("Comment fonctionne pgvector ?")
print(reponse)
```

---

## Bonnes Pratiques

### 1. Choix de la dimension

âœ… **Respecter les dimensions du modÃ¨le** :

| ModÃ¨le | Dimensions | Usage |
|--------|-----------|-------|
| OpenAI text-embedding-3-small | 1536 | GÃ©nÃ©ral, bon rapport qualitÃ©/prix |
| OpenAI text-embedding-3-large | 3072 | Haute prÃ©cision |
| Sentence-BERT (all-MiniLM-L6-v2) | 384 | LÃ©ger, rapide |
| ResNet-50 (images) | 2048 | Computer vision |

âŒ **Ne pas tronquer arbitrairement** : Perte d'information

### 2. Normalisation des vecteurs

Pour la distance cosine, normalisez vos vecteurs Ã  la source (norme = 1) pour accÃ©lÃ©rer les calculs.

```python
import numpy as np

def normaliser_vecteur(v):
    norme = np.linalg.norm(v)
    return v / norme if norme > 0 else v
```

### 3. Gestion des mises Ã  jour

```sql
-- Mise Ã  jour d'un embedding
UPDATE documents
SET
    contenu = 'Nouveau contenu',
    embedding = '[0.123, -0.456, ..., 0.789]'::vector(1536),
    updated_at = NOW()
WHERE id = 123;

-- L'index sera mis Ã  jour automatiquement
```

### 4. Monitoring

```sql
-- Taille des index
SELECT
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE indexname LIKE '%embedding%';

-- Performance des recherches
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM documents
ORDER BY embedding <=> '[...]'::vector(1536)
LIMIT 10;
```

### 5. Seuils de similaritÃ©

DÃ©finissez des seuils adaptÃ©s Ã  votre cas d'usage :

```sql
-- TrÃ¨s similaire (potentiel doublon)
WHERE embedding <=> query_emb < 0.1

-- Similaire (recommandation)
WHERE embedding <=> query_emb < 0.3

-- Moyennement similaire (exploration)
WHERE embedding <=> query_emb < 0.5
```

---

## Limitations et ConsidÃ©rations

### 1. Taille des vecteurs

âš ï¸ **Impact mÃ©moire et stockage** :

```
Table avec 1M de vecteurs de 1536 dimensions :
- Stockage : ~6 Go (sans compression)
- Index HNSW : ~12-15 Go supplÃ©mentaires
- RAM nÃ©cessaire : 20-30 Go pour performances optimales
```

### 2. PrÃ©cision vs Performance

âš ï¸ **Trade-off avec les index** :

- **IVFFlat** : ~95% de prÃ©cision, trÃ¨s rapide
- **HNSW** : ~98% de prÃ©cision, ultra-rapide

Vous ne rÃ©cupÃ©rerez pas toujours les rÃ©sultats thÃ©oriquement les plus proches (approximation).

### 3. CoÃ»t des embeddings

âš ï¸ **API OpenAI** :

- text-embedding-3-small : $0.02 / 1M tokens
- text-embedding-3-large : $0.13 / 1M tokens

Pour 100 000 documents de 500 mots â†’ ~$1-$7

**Alternative** : ModÃ¨les open-source locaux (Sentence Transformers, etc.)

### 4. Mises Ã  jour d'index

âš ï¸ **Reconstruction partielle** :

Les index HNSW et IVFFlat peuvent nÃ©cessiter une reconstruction pÃ©riodique pour maintenir les performances aprÃ¨s de nombreuses insertions/suppressions.

```sql
-- Reconstruire un index
REINDEX INDEX idx_documents_embedding_hnsw;
```

---

## Conclusion

### Points clÃ©s Ã  retenir

âœ… **pgvector** permet la recherche de similaritÃ© directement dans PostgreSQL

âœ… Trois mÃ©triques principales :
- **L2 (euclidienne)** `<->` : Images, donnÃ©es numÃ©riques
- **Cosine** `<=>` : Texte, embeddings sÃ©mantiques (le plus utilisÃ©)
- **Inner product** `<#>` : Recommandation avec poids

âœ… La **distance cosine** est idÃ©ale pour les embeddings de texte (OpenAI, etc.)

âœ… Les **index HNSW** offrent les meilleures performances (1000Ã— plus rapide que sans index)

âœ… Cas d'usage principaux : Recherche sÃ©mantique, RAG, recommandation, classification

âš ï¸ Choisir la bonne mÃ©trique selon vos donnÃ©es (texte â†’ cosine, images â†’ L2)

âš ï¸ Dimensionner correctement (RAM, stockage) pour les gros volumes

ğŸ¯ **pgvector** transforme PostgreSQL en base de donnÃ©es vectorielle performante pour l'IA !

### Prochaines Ã©tapes

Pour aller plus loin :
1. **Pratiquer** : CrÃ©er une table avec embeddings et tester les recherches
2. **IntÃ©grer OpenAI** : GÃ©nÃ©rer des embeddings via l'API
3. **Optimiser** : CrÃ©er des index HNSW et comparer les performances
4. **Construire un RAG** : Chatbot intelligent avec rÃ©cupÃ©ration de contexte

### Ressources

- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Sentence Transformers](https://www.sbert.net/) (modÃ¨les open-source)
- Article : *Understanding Vector Similarity Search*
- Tutoriels Supabase sur pgvector + RAG

---


â­ï¸ [Index HNSW et IVFFlat](/18-extensions-et-integrations/06.2-index-hnsw-ivfflat.md)
