ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 20bis.2.4 â€” Debezium pour Streaming d'Ã‰vÃ©nements

## Introduction

Dans le chapitre prÃ©cÃ©dent, nous avons dÃ©couvert les concepts du Change Data Capture (CDC) et du Logical Decoding. Nous avons briÃ¨vement prÃ©sentÃ© Debezium comme solution de rÃ©fÃ©rence. Ce chapitre approfondit Debezium : son architecture, sa configuration avancÃ©e, les transformations de messages, le monitoring, et les patterns de production.

**Debezium** est une plateforme open-source de Change Data Capture distribuÃ©e. Elle capture les modifications de bases de donnÃ©es en temps rÃ©el et les diffuse sous forme de flux d'Ã©vÃ©nements vers Apache Kafka. C'est aujourd'hui le standard de facto pour le CDC dans les architectures Ã©vÃ©nementielles.

Ã€ la fin de ce chapitre, vous saurez dÃ©ployer, configurer et opÃ©rer Debezium en production avec PostgreSQL.

---

## Pourquoi Debezium ?

### Les DÃ©fis du CDC en Production

ImplÃ©menter le CDC "Ã  la main" pose plusieurs dÃ©fis :

| DÃ©fi | ComplexitÃ© |
|------|------------|
| Gestion des connexions au WAL | Reconnexion, timeouts, heartbeats |
| Snapshots initiaux | Capturer l'Ã©tat existant avant le streaming |
| Ã‰volution des schÃ©mas | GÃ©rer les ALTER TABLE sans interruption |
| Garanties de livraison | At-least-once, ordering, exactly-once |
| Monitoring | MÃ©triques, alertes, lag tracking |
| ScalabilitÃ© | RÃ©partition de charge, haute disponibilitÃ© |

### Ce que Debezium Apporte

Debezium rÃ©sout tous ces problÃ¨mes avec une solution Ã©prouvÃ©e :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FonctionnalitÃ©s Debezium                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  âœ… Connecteurs pour 10+ bases de donnÃ©es                               â”‚
â”‚  âœ… Snapshots automatiques et incrÃ©mentaux                              â”‚
â”‚  âœ… Gestion des schÃ©mas avec Schema Registry                            â”‚
â”‚  âœ… Transformations de messages (SMT)                                   â”‚
â”‚  âœ… MÃ©triques JMX/Prometheus                                            â”‚
â”‚  âœ… TolÃ©rance aux pannes et reprise automatique                         â”‚
â”‚  âœ… IntÃ©gration native avec Kafka Connect                               â”‚
â”‚  âœ… Documentation exhaustive et communautÃ© active                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Architecture de Debezium

### Vue d'Ensemble

Debezium s'exÃ©cute comme un ensemble de **connecteurs** au sein de **Kafka Connect**, la plateforme d'intÃ©gration d'Apache Kafka.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      Kafka Connect Cluster                       â”‚  â”‚
â”‚  â”‚                                                                  â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚   â”‚  Worker 1   â”‚  â”‚  Worker 2   â”‚  â”‚  Worker 3   â”‚              â”‚  â”‚
â”‚  â”‚   â”‚             â”‚  â”‚             â”‚  â”‚             â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â”‚Debezium â”‚ â”‚  â”‚ â”‚Debezium â”‚ â”‚  â”‚ â”‚  Other  â”‚ â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â”‚Postgres â”‚ â”‚  â”‚ â”‚ MySQL   â”‚ â”‚  â”‚ â”‚Connectorâ”‚ â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â”‚Connectorâ”‚ â”‚  â”‚ â”‚Connectorâ”‚ â”‚  â”‚ â”‚         â”‚ â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚          â”‚                â”‚                                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                â”‚                                         â”‚
â”‚             â–¼                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        Apache Kafka                              â”‚  â”‚
â”‚  â”‚                                                                  â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚   â”‚   Topic:    â”‚  â”‚   Topic:    â”‚  â”‚   Topic:    â”‚              â”‚  â”‚
â”‚  â”‚   â”‚  postgres.  â”‚  â”‚   mysql.    â”‚  â”‚   schema    â”‚              â”‚  â”‚
â”‚  â”‚   â”‚   public.   â”‚  â”‚   mydb.     â”‚  â”‚   changes   â”‚              â”‚  â”‚
â”‚  â”‚   â”‚  customers  â”‚  â”‚   orders    â”‚  â”‚             â”‚              â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚                                                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                        â”‚
â”‚        â–²                          â–²                                    â”‚
â”‚        â”‚                          â”‚                                    â”‚
â”‚   Logical                    Binlog                                    â”‚
â”‚   Replication                Replication                               â”‚
â”‚        â”‚                          â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚PostgreSQL â”‚              â”‚   MySQL   â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les Composants

| Composant | RÃ´le |
|-----------|------|
| **Kafka Connect** | Framework d'intÃ©gration qui hÃ©berge les connecteurs |
| **Debezium Connector** | Plugin qui capture les changements d'une base spÃ©cifique |
| **Source Task** | Thread qui lit le WAL et produit des messages |
| **Kafka** | Bus de messages qui stocke et distribue les Ã©vÃ©nements |
| **Schema Registry** | Stocke les schÃ©mas Avro/JSON (optionnel mais recommandÃ©) |

### Modes de DÃ©ploiement

#### Mode Standalone (DÃ©veloppement)

Un seul worker Kafka Connect :

```bash
# Lancer Connect en mode standalone
connect-standalone.sh \
    connect-standalone.properties \
    postgres-connector.properties
```

#### Mode DistribuÃ© (Production)

Cluster de workers pour la haute disponibilitÃ© :

```bash
# Chaque worker rejoint le cluster
connect-distributed.sh connect-distributed.properties
```

Le cluster distribue automatiquement les tÃ¢ches entre les workers. Si un worker tombe, ses tÃ¢ches sont redistribuÃ©es.

---

## Installation ComplÃ¨te

### PrÃ©requis

- PostgreSQL 10+ avec `wal_level = logical`
- Apache Kafka 2.x ou 3.x
- Java 11+
- Docker (optionnel mais recommandÃ©)

### Option 1 : Avec Docker Compose (RecommandÃ©)

CrÃ©ez un fichier `docker-compose.yml` complet :

```yaml
version: '3.8'

services:
  # ZooKeeper pour Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5

  # Apache Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: kafka-broker-api-versions --bootstrap-server kafka:29092
      interval: 10s
      timeout: 5s
      retries: 5

  # Schema Registry (optionnel mais recommandÃ©)
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  # PostgreSQL source
  postgres:
    image: postgres:16
    hostname: postgres
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: sourcedb
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "max_wal_senders=10"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  # Debezium Connect
  connect:
    image: debezium/connect:2.4
    hostname: connect
    container_name: connect
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: debezium-connect-cluster
      CONFIG_STORAGE_TOPIC: _connect_configs
      OFFSET_STORAGE_TOPIC: _connect_offsets
      STATUS_STORAGE_TOPIC: _connect_statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      # Pour Avro avec Schema Registry :
      # KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      # KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      # VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      # VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    healthcheck:
      test: curl -f http://localhost:8083/connectors || exit 1
      interval: 10s
      timeout: 5s
      retries: 10

  # Interface Web pour Kafka (optionnel)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
      - connect
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: debezium
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://connect:8083

volumes:
  postgres_data:
```

Script d'initialisation `init.sql` :

```sql
-- CrÃ©er un utilisateur dÃ©diÃ© au CDC
CREATE USER debezium WITH REPLICATION LOGIN PASSWORD 'dbz_password';

-- Base de donnÃ©es exemple
CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INTEGER REFERENCES customers(id),
    total NUMERIC(10,2) NOT NULL,
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE order_items (
    id SERIAL PRIMARY KEY,
    order_id INTEGER REFERENCES orders(id),
    product_name VARCHAR(255),
    quantity INTEGER,
    unit_price NUMERIC(10,2)
);

-- Configurer REPLICA IDENTITY pour capturer les valeurs "before"
ALTER TABLE customers REPLICA IDENTITY FULL;
ALTER TABLE orders REPLICA IDENTITY FULL;
ALTER TABLE order_items REPLICA IDENTITY FULL;

-- Donner les droits Ã  debezium
GRANT SELECT ON ALL TABLES IN SCHEMA public TO debezium;
GRANT USAGE ON SCHEMA public TO debezium;

-- CrÃ©er la publication pour Debezium
CREATE PUBLICATION dbz_publication FOR ALL TABLES;

-- InsÃ©rer des donnÃ©es initiales
INSERT INTO customers (name, email) VALUES
    ('Alice Martin', 'alice@example.com'),
    ('Bob Johnson', 'bob@example.com'),
    ('Charlie Brown', 'charlie@example.com');

INSERT INTO orders (customer_id, total, status) VALUES
    (1, 150.00, 'completed'),
    (1, 75.50, 'pending'),
    (2, 200.00, 'shipped');
```

DÃ©marrer l'environnement :

```bash
docker-compose up -d

# VÃ©rifier que tout est dÃ©marrÃ©
docker-compose ps

# Voir les logs de Connect
docker-compose logs -f connect
```

### Option 2 : Installation Manuelle

```bash
# TÃ©lÃ©charger Kafka
wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
tar -xzf kafka_2.13-3.6.0.tgz
cd kafka_2.13-3.6.0

# TÃ©lÃ©charger le connecteur Debezium PostgreSQL
mkdir -p plugins/debezium-postgres
wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/2.4.0.Final/debezium-connector-postgres-2.4.0.Final-plugin.tar.gz
tar -xzf debezium-connector-postgres-2.4.0.Final-plugin.tar.gz -C plugins/debezium-postgres

# Configurer Kafka Connect
cat >> config/connect-distributed.properties << EOF
plugin.path=/path/to/kafka/plugins
EOF

# DÃ©marrer ZooKeeper
bin/zookeeper-server-start.sh config/zookeeper.properties &

# DÃ©marrer Kafka
bin/kafka-server-start.sh config/server.properties &

# DÃ©marrer Kafka Connect
bin/connect-distributed.sh config/connect-distributed.properties &
```

---

## Configuration du Connecteur PostgreSQL

### Enregistrement via l'API REST

Debezium se configure via l'API REST de Kafka Connect :

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "postgres-source-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",

      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "debezium",
      "database.password": "dbz_password",
      "database.dbname": "sourcedb",

      "topic.prefix": "dbserver1",

      "plugin.name": "pgoutput",
      "slot.name": "debezium_slot",
      "publication.name": "dbz_publication",

      "table.include.list": "public.customers,public.orders,public.order_items",

      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "key.converter.schemas.enable": "false",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "true",

      "snapshot.mode": "initial",

      "heartbeat.interval.ms": "10000",
      "heartbeat.action.query": "UPDATE public.heartbeat SET ts = NOW()",

      "transforms": "unwrap",
      "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
      "transforms.unwrap.drop.tombstones": "false",
      "transforms.unwrap.delete.handling.mode": "rewrite"
    }
  }'
```

### VÃ©rifier l'Ã‰tat du Connecteur

```bash
# Liste des connecteurs
curl http://localhost:8083/connectors

# Ã‰tat d'un connecteur
curl http://localhost:8083/connectors/postgres-source-connector/status | jq

# Configuration actuelle
curl http://localhost:8083/connectors/postgres-source-connector/config | jq
```

RÃ©ponse typique d'un connecteur sain :

```json
{
  "name": "postgres-source-connector",
  "connector": {
    "state": "RUNNING",
    "worker_id": "connect:8083"
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "connect:8083"
    }
  ],
  "type": "source"
}
```

### ParamÃ¨tres de Configuration Essentiels

#### Connexion Ã  la Base

| ParamÃ¨tre | Description |
|-----------|-------------|
| `database.hostname` | HÃ´te PostgreSQL |
| `database.port` | Port (dÃ©faut: 5432) |
| `database.user` | Utilisateur avec droits REPLICATION |
| `database.password` | Mot de passe |
| `database.dbname` | Nom de la base de donnÃ©es |

#### RÃ©plication Logique

| ParamÃ¨tre | Description |
|-----------|-------------|
| `plugin.name` | `pgoutput` (recommandÃ©), `wal2json`, `decoderbufs` |
| `slot.name` | Nom du slot de rÃ©plication |
| `publication.name` | Nom de la publication PostgreSQL |
| `publication.autocreate.mode` | `all_tables`, `filtered`, `disabled` |

#### SÃ©lection des Tables

| ParamÃ¨tre | Description |
|-----------|-------------|
| `table.include.list` | Tables Ã  inclure (regex) |
| `table.exclude.list` | Tables Ã  exclure |
| `column.include.list` | Colonnes Ã  inclure |
| `column.exclude.list` | Colonnes Ã  exclure (donnÃ©es sensibles) |
| `schema.include.list` | SchÃ©mas Ã  inclure |

#### Topics Kafka

| ParamÃ¨tre | Description |
|-----------|-------------|
| `topic.prefix` | PrÃ©fixe des topics (ex: `dbserver1`) |
| `topic.naming.strategy` | StratÃ©gie de nommage des topics |
| `topic.creation.enable` | CrÃ©er les topics automatiquement |

---

## Modes de Snapshot

Le **snapshot** est la phase initiale oÃ¹ Debezium capture l'Ã©tat actuel des tables avant de passer au streaming continu.

### Les DiffÃ©rents Modes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Modes de Snapshot                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  initial          â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º      â”‚
â”‚                   â•‘ Snapshot  â•‘         Streaming continu               â”‚
â”‚                   â•šâ•â•â•â•â•â•â•â•â•â•â•â•                                         â”‚
â”‚                                                                         â”‚
â”‚  always           â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º      â”‚
â”‚  (Ã  chaque        â•‘ Snapshot  â•‘         Streaming continu               â”‚
â”‚   redÃ©marrage)    â•šâ•â•â•â•â•â•â•â•â•â•â•â•                                         â”‚
â”‚                                                                         â”‚
â”‚  never            â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º       â”‚
â”‚                              Streaming uniquement                       â”‚
â”‚                              (depuis le LSN actuel)                     â”‚
â”‚                                                                         â”‚
â”‚  initial_only     â•â•â•â•â•â•â•â•â•â•â•â•â•—                                         â”‚
â”‚                   â•‘ Snapshot  â•‘  (ArrÃªt aprÃ¨s snapshot)                 â”‚
â”‚                   â•šâ•â•â•â•â•â•â•â•â•â•â•â•                                         â”‚
â”‚                                                                         â”‚
â”‚  when_needed      â•â•â•â•â•â•â•â•¦â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º       â”‚
â”‚                   (Snap  â•‘    â•‘  Streaming                              â”‚
â”‚                   si pas â•‘    â•‘                                         â”‚
â”‚                   d'offset)   â•                                         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Mode | Comportement |
|------|-------------|
| `initial` | Snapshot au premier dÃ©marrage, puis streaming |
| `always` | Snapshot Ã  chaque redÃ©marrage du connecteur |
| `never` | Pas de snapshot, streaming depuis la position actuelle |
| `initial_only` | Snapshot uniquement, pas de streaming (migration batch) |
| `when_needed` | Snapshot si pas d'offset stockÃ© ou slot invalide |
| `exported` | Utilise une transaction d'export pour le snapshot |
| `custom` | Classe personnalisÃ©e pour le snapshot |

### Configuration du Snapshot

```json
{
  "snapshot.mode": "initial",
  "snapshot.include.collection.list": "public.customers,public.orders",
  "snapshot.select.statement.overrides": "public.large_table",
  "snapshot.select.statement.overrides.public.large_table": "SELECT * FROM public.large_table WHERE created_at > '2024-01-01'",
  "snapshot.max.threads": "4",
  "snapshot.fetch.size": "10000",
  "snapshot.lock.timeout.ms": "10000"
}
```

### Snapshot IncrÃ©mental (Ad-hoc)

Depuis Debezium 1.6, vous pouvez dÃ©clencher un snapshot Ã  la demande via des signaux :

```sql
-- CrÃ©er la table de signaux
CREATE TABLE debezium_signal (
    id VARCHAR(42) PRIMARY KEY,
    type VARCHAR(32) NOT NULL,
    data VARCHAR(2048) NULL
);

-- Configurer le connecteur pour Ã©couter les signaux
-- "signal.data.collection": "public.debezium_signal"

-- DÃ©clencher un snapshot incrÃ©mental
INSERT INTO debezium_signal (id, type, data)
VALUES (
    'ad-hoc-1',
    'execute-snapshot',
    '{"data-collections": ["public.customers"], "type": "incremental"}'
);
```

---

## Transformations de Messages (SMT)

Les **Single Message Transforms (SMT)** permettent de modifier les messages avant leur envoi Ã  Kafka.

### Transformations Courantes

#### 1. ExtractNewRecordState (Unwrap)

Simplifie la structure du message en extrayant uniquement l'Ã©tat "after" :

```json
// AVANT (structure Debezium complÃ¨te)
{
  "schema": { ... },
  "payload": {
    "before": null,
    "after": { "id": 1, "name": "Alice" },
    "source": { ... },
    "op": "c",
    "ts_ms": 1234567890
  }
}

// APRÃˆS (avec ExtractNewRecordState)
{
  "id": 1,
  "name": "Alice"
}
```

Configuration :

```json
{
  "transforms": "unwrap",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.unwrap.drop.tombstones": "false",
  "transforms.unwrap.delete.handling.mode": "rewrite",
  "transforms.unwrap.add.fields": "op,source.ts_ms"
}
```

#### 2. Filtrage par Contenu

```json
{
  "transforms": "filter",
  "transforms.filter.type": "io.debezium.transforms.Filter",
  "transforms.filter.language": "jsr223.groovy",
  "transforms.filter.condition": "value.after.status == 'active'"
}
```

#### 3. Routage vers Topics DiffÃ©rents

Router les messages vers diffÃ©rents topics selon le contenu :

```json
{
  "transforms": "route",
  "transforms.route.type": "io.debezium.transforms.ByLogicalTableRouter",
  "transforms.route.topic.regex": "(.*)customers(.*)",
  "transforms.route.topic.replacement": "$1users$2"
}
```

#### 4. Masquage de DonnÃ©es Sensibles

```json
{
  "transforms": "mask",
  "transforms.mask.type": "org.apache.kafka.connect.transforms.MaskField$Value",
  "transforms.mask.fields": "email,phone,ssn",
  "transforms.mask.replacement": "***MASKED***"
}
```

#### 5. Ajout de MÃ©tadonnÃ©es

```json
{
  "transforms": "addMetadata",
  "transforms.addMetadata.type": "org.apache.kafka.connect.transforms.InsertField$Value",
  "transforms.addMetadata.static.field": "environment",
  "transforms.addMetadata.static.value": "production"
}
```

### ChaÃ®ner Plusieurs Transformations

```json
{
  "transforms": "unwrap,filter,route,addField",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.filter.type": "io.debezium.transforms.Filter",
  "transforms.filter.language": "jsr223.groovy",
  "transforms.filter.condition": "value.status != 'deleted'",
  "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
  "transforms.route.regex": "dbserver1.public.(.*)",
  "transforms.route.replacement": "prod.$1",
  "transforms.addField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
  "transforms.addField.timestamp.field": "processed_at"
}
```

---

## Gestion des SchÃ©mas

### Le ProblÃ¨me de l'Ã‰volution des SchÃ©mas

Quand vous modifiez la structure d'une table (ALTER TABLE), les consommateurs doivent pouvoir gÃ©rer les anciens ET nouveaux formats de messages.

```
Jour 1: { "id": 1, "name": "Alice" }
Jour 2: ALTER TABLE ADD COLUMN email VARCHAR(255)
Jour 2: { "id": 2, "name": "Bob", "email": "bob@example.com" }

Le consommateur doit gÃ©rer les deux formats !
```

### Schema Registry

Le **Schema Registry** (Confluent ou Apicurio) stocke les schÃ©mas et garantit la compatibilitÃ© :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  Debezium â”€â”€â”€â”€â”€â”€â–º Schema Registry â—„â”€â”€â”€â”€â”€â”€ Consommateurs                 â”‚
â”‚     â”‚                   â”‚                        â”‚                      â”‚
â”‚     â”‚ Enregistre        â”‚ Stocke les             â”‚ RÃ©cupÃ¨re             â”‚
â”‚     â”‚ le schÃ©ma         â”‚ schÃ©mas                â”‚ le schÃ©ma            â”‚
â”‚     â–¼                   â”‚                        â–¼                      â”‚
â”‚  Message Kafka          â”‚                 DÃ©sÃ©rialise                   â”‚
â”‚  (donnÃ©es + ID schÃ©ma)  â”‚                 correctement                  â”‚
â”‚                         â”‚                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration avec Avro

```json
{
  "key.converter": "io.confluent.connect.avro.AvroConverter",
  "key.converter.schema.registry.url": "http://schema-registry:8081",
  "value.converter": "io.confluent.connect.avro.AvroConverter",
  "value.converter.schema.registry.url": "http://schema-registry:8081",
  "value.converter.schemas.enable": "true"
}
```

### Modes de CompatibilitÃ©

| Mode | RÃ¨gle |
|------|-------|
| `BACKWARD` | Nouveau schÃ©ma peut lire anciennes donnÃ©es |
| `FORWARD` | Ancien schÃ©ma peut lire nouvelles donnÃ©es |
| `FULL` | Backward + Forward |
| `NONE` | Pas de vÃ©rification |

```bash
# Configurer la compatibilitÃ© pour un sujet
curl -X PUT http://localhost:8081/config/dbserver1.public.customers-value \
  -H "Content-Type: application/json" \
  -d '{"compatibility": "BACKWARD"}'
```

---

## Monitoring et OpÃ©rations

### MÃ©triques JMX

Debezium expose des mÃ©triques via JMX :

```bash
# Activer JMX dans le connecteur
KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote \
  -Dcom.sun.management.jmxremote.port=9010 \
  -Dcom.sun.management.jmxremote.authenticate=false \
  -Dcom.sun.management.jmxremote.ssl=false"
```

### MÃ©triques Importantes

| MÃ©trique | Description |
|----------|-------------|
| `MilliSecondsBehindSource` | Latence par rapport Ã  la source |
| `TotalNumberOfEventsSeen` | Nombre total d'Ã©vÃ©nements capturÃ©s |
| `NumberOfEventsFiltered` | Ã‰vÃ©nements filtrÃ©s |
| `QueueTotalCapacity` | CapacitÃ© de la queue interne |
| `QueueRemainingCapacity` | CapacitÃ© restante |
| `LastEvent` | Timestamp du dernier Ã©vÃ©nement |
| `SnapshotCompleted` | Snapshot terminÃ© (true/false) |
| `SnapshotDurationInSeconds` | DurÃ©e du snapshot |

### Prometheus et Grafana

Utilisez JMX Exporter pour exposer les mÃ©triques Ã  Prometheus :

```yaml
# prometheus-jmx-config.yml
rules:
  - pattern: "debezium.postgres<type=connector-metrics, context=(.+), server=(.+)><>(.+): (.+)"
    name: "debezium_postgres_$3"
    labels:
      context: "$1"
      server: "$2"
    type: GAUGE

  - pattern: "debezium.postgres<type=connector-metrics, context=streaming, server=(.+)><>MilliSecondsBehindSource"
    name: "debezium_streaming_lag_ms"
    labels:
      server: "$1"
    type: GAUGE
```

### Dashboard Grafana

MÃ©triques clÃ©s Ã  surveiller :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Dashboard Debezium                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Lag (ms)       â”‚  â”‚  Events/sec      â”‚  â”‚  Queue Usage     â”‚       â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚       â”‚
â”‚  â”‚     245 ms       â”‚  â”‚     1,234        â”‚  â”‚     45%          â”‚       â”‚
â”‚  â”‚     â–‚â–ƒâ–…â–†â–‡        â”‚  â”‚     â–ƒâ–…â–†â–‡â–…â–ƒ       â”‚  â”‚     â–ƒâ–ƒâ–ƒâ–ƒâ–…â–†       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Errors         â”‚  â”‚  Snapshot Status â”‚  â”‚  Slot Lag        â”‚       â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚       â”‚
â”‚  â”‚       0          â”‚  â”‚   Completed âœ“    â”‚  â”‚     12 MB        â”‚       â”‚
â”‚  â”‚     â–â–â–â–â–        â”‚  â”‚                  â”‚  â”‚     â–‚â–‚â–‚â–ƒâ–ƒâ–‚       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Alertes RecommandÃ©es

```yaml
# Prometheus alerting rules
groups:
  - name: debezium
    rules:
      - alert: DebeziumHighLag
        expr: debezium_streaming_lag_ms > 60000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Debezium lag > 60 seconds"

      - alert: DebeziumConnectorDown
        expr: debezium_connector_status != 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Debezium connector is not running"

      - alert: DebeziumSlotLagHigh
        expr: pg_replication_slots_lag_bytes > 1073741824
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL replication slot lag > 1GB"
```

---

## OpÃ©rations Courantes

### Mettre Ã  Jour la Configuration

```bash
# Mettre Ã  jour un connecteur
curl -X PUT http://localhost:8083/connectors/postgres-source-connector/config \
  -H "Content-Type: application/json" \
  -d '{
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "table.include.list": "public.customers,public.orders,public.new_table",
    ... autres paramÃ¨tres ...
  }'
```

### Pause et Reprise

```bash
# Mettre en pause
curl -X PUT http://localhost:8083/connectors/postgres-source-connector/pause

# Reprendre
curl -X PUT http://localhost:8083/connectors/postgres-source-connector/resume
```

### RedÃ©marrer un Connecteur

```bash
# RedÃ©marrer le connecteur
curl -X POST http://localhost:8083/connectors/postgres-source-connector/restart

# RedÃ©marrer une tÃ¢che spÃ©cifique
curl -X POST http://localhost:8083/connectors/postgres-source-connector/tasks/0/restart
```

### Supprimer un Connecteur

```bash
# Supprimer le connecteur
curl -X DELETE http://localhost:8083/connectors/postgres-source-connector

# IMPORTANT : Nettoyer aussi le slot PostgreSQL
psql -c "SELECT pg_drop_replication_slot('debezium_slot');"
```

---

## Patterns de Production

### Pattern 1 : Outbox Pattern avec Debezium

Le pattern Outbox garantit la cohÃ©rence entre les modifications de base et la publication d'Ã©vÃ©nements :

```sql
-- Table Outbox
CREATE TABLE outbox (
    id UUID PRIMARY KEY,
    aggregate_type VARCHAR(255) NOT NULL,
    aggregate_id VARCHAR(255) NOT NULL,
    type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- La transaction mÃ©tier Ã©crit dans outbox
BEGIN;
    INSERT INTO orders (customer_id, total) VALUES (1, 100.00);
    INSERT INTO outbox (id, aggregate_type, aggregate_id, type, payload)
    VALUES (
        gen_random_uuid(),
        'Order',
        '12345',
        'OrderCreated',
        '{"order_id": "12345", "customer_id": 1, "total": 100.00}'
    );
COMMIT;
```

Configuration Debezium pour Outbox :

```json
{
  "table.include.list": "public.outbox",
  "transforms": "outbox",
  "transforms.outbox.type": "io.debezium.transforms.outbox.EventRouter",
  "transforms.outbox.table.fields.additional.placement": "type:header:eventType",
  "transforms.outbox.route.by.field": "aggregate_type",
  "transforms.outbox.route.topic.replacement": "events.${routedByValue}"
}
```

### Pattern 2 : Multi-Tenant

```json
{
  "transforms": "addTenant,route",
  "transforms.addTenant.type": "org.apache.kafka.connect.transforms.InsertField$Value",
  "transforms.addTenant.static.field": "tenant_id",
  "transforms.addTenant.static.value": "tenant_123",
  "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
  "transforms.route.regex": "dbserver1.public.(.*)",
  "transforms.route.replacement": "tenant_123.$1"
}
```

### Pattern 3 : RÃ©plication Cross-Region

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Region EU     â”‚         â”‚   Region US     â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚  PostgreSQL â”€â”€â”€â”€â”¼â”€â”€ CDC â”€â”€â”¼â”€â”€â–º PostgreSQL   â”‚
â”‚  (Primary)      â”‚  Kafka  â”‚    (Replica)    â”‚
â”‚                 â”‚         â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Troubleshooting

### ProblÃ¨me : Le Connecteur ne DÃ©marre Pas

```bash
# VÃ©rifier les logs
docker-compose logs connect | grep -i error

# Causes courantes :
# - pg_hba.conf n'autorise pas la connexion
# - wal_level != logical
# - Utilisateur sans droits REPLICATION
```

### ProblÃ¨me : Lag Croissant

```sql
-- VÃ©rifier le lag du slot
SELECT
    slot_name,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS lag
FROM pg_replication_slots
WHERE slot_name = 'debezium_slot';

-- Solutions :
-- - Augmenter les ressources du worker Connect
-- - RÃ©duire le nombre de tables capturÃ©es
-- - Optimiser les transformations
```

### ProblÃ¨me : Slot Perdu (Slot Already Exists)

```sql
-- Le slot existe mais le connecteur ne peut pas s'y connecter
-- Supprimer et recrÃ©er

SELECT pg_drop_replication_slot('debezium_slot');

-- Puis redÃ©marrer le connecteur
```

### ProblÃ¨me : Messages DupliquÃ©s

Debezium garantit at-least-once delivery. Les consommateurs doivent Ãªtre **idempotents** :

```python
def process_event(event):
    event_id = f"{event['source']['lsn']}_{event['source']['txId']}"

    if redis.exists(f"processed:{event_id}"):
        return  # DÃ©jÃ  traitÃ©

    # Traiter l'Ã©vÃ©nement
    do_work(event)

    # Marquer comme traitÃ© (avec TTL)
    redis.setex(f"processed:{event_id}", 86400, "1")
```

---

## Conclusion

**Debezium** transforme PostgreSQL en source d'Ã©vÃ©nements robuste pour les architectures modernes. Ses points forts :

- **FiabilitÃ©** : BasÃ© sur le WAL, garantie at-least-once
- **FlexibilitÃ©** : SMT pour transformer les messages Ã  la volÃ©e
- **ObservabilitÃ©** : MÃ©triques complÃ¨tes pour le monitoring
- **Ã‰cosystÃ¨me** : IntÃ©gration native avec Kafka et Schema Registry
- **CommunautÃ©** : Open-source actif avec support Red Hat

En production, Debezium permet de construire des pipelines CDC qui synchronisent des tÃ©raoctets de donnÃ©es avec une latence de l'ordre de la seconde, sans modifier les applications sources.

---

## Points ClÃ©s Ã  Retenir

- **Debezium** : Plateforme CDC open-source basÃ©e sur Kafka Connect
- **Modes de snapshot** : `initial`, `never`, `always` selon le cas d'usage
- **SMT** : Transformations pour simplifier, filtrer ou enrichir les messages
- **Schema Registry** : Gestion de l'Ã©volution des schÃ©mas
- **Monitoring** : MÃ©triques JMX, alertes sur le lag et les erreurs
- **Outbox Pattern** : Garantir la cohÃ©rence transactionnelle des Ã©vÃ©nements
- **Idempotence** : Les consommateurs doivent gÃ©rer les doublons
- **OpÃ©rations** : API REST pour gÃ©rer les connecteurs (pause, restart, update)

---


â­ï¸ [PostgreSQL en architecture serverless](/20bis-postgresql-et-architectures-modernes/03-postgresql-serverless.md)
