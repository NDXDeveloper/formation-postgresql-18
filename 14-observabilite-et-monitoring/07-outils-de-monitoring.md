ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.7. Outils de monitoring

## Introduction

Imaginez que vous conduisez une voiture sans tableau de bord : pas de compteur de vitesse, pas de jauge d'essence, pas de voyant moteur. Vous pourriez rouler pendant un moment, mais tÃ´t ou tard, vous seriez confrontÃ© Ã  un problÃ¨me que vous n'avez pas vu venir. **Une base de donnÃ©es sans monitoring, c'est exactement la mÃªme chose.**

Le monitoring (surveillance en franÃ§ais) est l'art de **garder un Å“il constant sur votre base de donnÃ©es PostgreSQL** pour :
- ğŸ” DÃ©tecter les problÃ¨mes avant qu'ils ne deviennent critiques
- ğŸ“ˆ Comprendre comment votre base Ã©volue dans le temps
- âš¡ Identifier les goulots d'Ã©tranglement de performance
- ğŸ›¡ï¸ Anticiper les besoins en ressources
- ğŸš¨ ÃŠtre alertÃ© immÃ©diatement en cas d'incident

Dans ce chapitre, nous allons explorer les **outils essentiels** pour surveiller PostgreSQL efficacement, des solutions open-source aux plateformes cloud natives.

---

## Pourquoi le monitoring est-il crucial ?

### Le coÃ»t de l'ignorance

**Sans monitoring, vous Ãªtes aveugle :**

```
ScÃ©nario typique sans monitoring :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
09:00 âœ "Tout va bien" (en apparence)
10:00 âœ La base ralentit (personne ne le voit)
11:00 âœ Le disque se remplit (toujours invisible)
12:00 âœ ğŸ’¥ CRASH : Disque plein, base inaccessible
12:01 âœ Panique gÃ©nÃ©rale, clients impactÃ©s
12:05 âœ "Depuis quand c'est comme Ã§a ?"
12:10 âœ "On aurait dÃ» voir Ã§a venir..."
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RÃ©sultat : 2 heures de downtime, perte de revenus
```

**Avec monitoring, vous Ãªtes proactif :**

```
ScÃ©nario avec monitoring :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
09:00 âœ Dashboard montre tout en vert
10:00 âœ Alerte : "Disque Ã  70%, tendance inquiÃ©tante"
10:05 âœ Investigation : logs volumineux trouvÃ©s
10:15 âœ Action : nettoyage automatisÃ© dÃ©clenchÃ©
10:20 âœ RÃ©solu : espace disque revenu Ã  45%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RÃ©sultat : ZÃ©ro downtime, problÃ¨me anticipÃ©
```

### Les bÃ©nÃ©fices concrets du monitoring

#### 1. DÃ©tection prÃ©coce des problÃ¨mes

**ProblÃ¨mes dÃ©tectables** :
- ğŸ’¾ Espace disque qui se remplit progressivement
- ğŸ”¥ CPU qui monte anormalement
- ğŸŒ RequÃªtes qui ralentissent avec le temps
- ğŸ”’ Verrous (locks) qui s'accumulent
- ğŸ“Š Tables qui gonflent (bloat)

**Exemple** : Votre monitoring dÃ©tecte que l'espace disque passe de 50% Ã  85% en une semaine. Vous avez le temps d'investiguer et d'agir **avant** d'atteindre 100%.

#### 2. Optimisation des performances

**Questions auxquelles le monitoring rÃ©pond** :
- Quelles sont mes requÃªtes les plus lentes ?
- Quelles tables sont les plus sollicitÃ©es ?
- Mon cache est-il bien dimensionnÃ© ?
- Mes index sont-ils utilisÃ©s ?

**Impact** : Identifier qu'une seule requÃªte lente peut amÃ©liorer les performances globales de 50%.

#### 3. Planification de capacitÃ©

**Tendances visibles** :
```
Janvier  : 100 connexions/jour, 50 GB de donnÃ©es
FÃ©vrier  : 120 connexions/jour, 60 GB de donnÃ©es
Mars     : 145 connexions/jour, 72 GB de donnÃ©es

â†’ Projection : Juin = 200 connexions, 100 GB
â†’ Action : Planifier l'upgrade du serveur en Mai
```

**BÃ©nÃ©fice** : Anticiper les besoins plutÃ´t que rÃ©agir dans l'urgence.

#### 4. ConformitÃ© et audit

**Exigences** :
- Tracer qui accÃ¨de Ã  quoi
- Prouver la disponibilitÃ© (SLA)
- DÃ©montrer les performances
- Justifier les investissements infrastructure

**Exemple** : "Notre SLA garantit 99.9% de uptime" â†’ Le monitoring fournit la preuve.

#### 5. RÃ©duction du MTTR (Mean Time To Repair)

**Sans monitoring** :
```
Incident â†’ DÃ©tection â†’ Investigation â†’ Reproduction â†’ RÃ©solution
           30 min      2 heures        1 heure      30 min
           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Total : 4 heures
```

**Avec monitoring** :
```
Incident â†’ Alerte immÃ©diate â†’ Dashboard historique â†’ Action ciblÃ©e
           < 1 min              5 min                15 min
           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Total : 20 minutes
```

**RÃ©sultat** : 12Ã— plus rapide Ã  rÃ©soudre.

---

## Les dimensions du monitoring PostgreSQL

Le monitoring d'une base de donnÃ©es n'est pas unidimensionnel. Il faut surveiller plusieurs aspects en parallÃ¨le.

### 1. MÃ©triques systÃ¨me (Infrastructure)

**Que surveille-t-on ?**
- ğŸ–¥ï¸ **CPU** : Utilisation, load average, processus actifs
- ğŸ’¾ **MÃ©moire** : RAM utilisÃ©e, swap, cache
- ğŸ’¿ **Disque** : Espace libre, IOPS, latence, throughput
- ğŸŒ **RÃ©seau** : Bande passante, latence, paquets perdus

**Pourquoi c'est important ?**

Un problÃ¨me PostgreSQL est souvent **causÃ© par** un problÃ¨me systÃ¨me :
- CPU Ã  100% â†’ RequÃªtes lentes ou non optimisÃ©es
- Disque saturÃ© â†’ Checkpoints lents, requÃªtes bloquÃ©es
- MÃ©moire insuffisante â†’ Swap, performances dÃ©gradÃ©es
- RÃ©seau lent â†’ RÃ©plication en retard

**Exemple** :
```
SymptÃ´me : RequÃªtes lentes depuis 1h
Monitoring systÃ¨me montre : I/O disque Ã  100%
Cause racine : Checkpoint trop frÃ©quents (max_wal_size trop petit)
Solution : Augmenter max_wal_size
```

### 2. MÃ©triques PostgreSQL (Base de donnÃ©es)

**Que surveille-t-on ?**
- ğŸ”Œ **Connexions** : Nombre actif, idle, max_connections
- ğŸ“ **Transactions** : Commits, rollbacks, taux
- ğŸ” **RequÃªtes** : Temps d'exÃ©cution, requÃªtes lentes
- ğŸ’¾ **Cache** : Hit ratio, lectures disque vs cache
- ğŸ”„ **RÃ©plication** : Lag, Ã©tat des replicas
- ğŸ§¹ **Maintenance** : VACUUM, ANALYZE, bloat

**Pourquoi c'est important ?**

Ces mÃ©triques reflÃ¨tent directement la **santÃ© de votre base** :
- Connexions saturÃ©es â†’ Application ne peut plus se connecter
- Cache hit ratio faible â†’ Trop de lectures disque (lent)
- RÃ©plication en retard â†’ DonnÃ©es obsolÃ¨tes sur les replicas

**Exemple** :
```
SymptÃ´me : Application qui timeout
Monitoring PostgreSQL montre : Connexions = 200/200 (saturÃ©)
Cause racine : Connection pooling mal configurÃ©
Solution : DÃ©ployer PgBouncer
```

### 3. Logs (Ã‰vÃ©nements)

**Que surveille-t-on ?**
- âŒ **Erreurs** : Deadlocks, violations de contraintes
- ğŸŒ **RequÃªtes lentes** : duration > seuil
- ğŸ” **Connexions** : Tentatives Ã©chouÃ©es, authentification
- ğŸ”§ **Changements** : DDL (CREATE, ALTER, DROP)
- ğŸš¨ **Alertes** : Checkpoints, VACUUM, wraparound

**Pourquoi c'est important ?**

Les logs racontent **l'histoire** de ce qui s'est passÃ© :
- Deadlock dÃ©tectÃ© â†’ Conflit de verrous entre transactions
- Connexion refusÃ©e â†’ Tentative d'intrusion ?
- RequÃªte lente loggÃ©e â†’ Candidat Ã  l'optimisation

**Exemple** :
```
Log : "ERROR: deadlock detected"
Log : "DETAIL: Process 12345 waits for ShareLock on transaction 67890"
â†’ Investigation : Deux transactions se bloquent mutuellement
â†’ Solution : Revoir l'ordre des opÃ©rations dans le code
```

### 4. MÃ©triques applicatives (APM)

**Que surveille-t-on ?**
- â±ï¸ **Latence** : Temps de rÃ©ponse des API
- ğŸ“Š **Throughput** : RequÃªtes par seconde
- ğŸ¯ **Taux d'erreur** : 4xx, 5xx
- ğŸ”— **DÃ©pendances** : Appels entre services

**Pourquoi c'est important ?**

La base de donnÃ©es ne vit pas en isolation. Comprendre **l'impact applicatif** est crucial :
- RequÃªte PostgreSQL lente â†’ API timeout
- Cache applicatif inefficace â†’ Surcharge PostgreSQL
- Pic de trafic â†’ Saturation des connexions

**Exemple** :
```
APM montre : Endpoint /api/orders Ã  5s (normalement 200ms)
Trace distribuÃ©e : 4.8s passÃ©es dans PostgreSQL
Monitoring PostgreSQL : RequÃªte sans index sur orders.date
Solution : CREATE INDEX idx_orders_date ON orders(date)
```

---

## Les catÃ©gories d'outils de monitoring

Il existe de nombreux outils pour monitorer PostgreSQL. Ils peuvent Ãªtre classÃ©s en plusieurs catÃ©gories selon leur approche et leur objectif.

### 1. Outils d'analyse de logs

**Principe** : Analyser les fichiers de logs PostgreSQL pour en extraire des insights.

**CaractÃ©ristiques** :
- ğŸ“„ Analyse a posteriori (aprÃ¨s que les Ã©vÃ©nements se sont produits)
- ğŸ¨ GÃ©nÃ¨rent des rapports visuels (HTML, PDF)
- ğŸ” Excellent pour l'audit et le troubleshooting
- ğŸ’¾ Pas d'impact sur la base (analyse hors ligne)

**Outil principal** : **pgBadger**

**Cas d'usage** :
- Analyser les performances de la semaine derniÃ¨re
- Identifier les requÃªtes qui ont posÃ© problÃ¨me
- Auditer les tentatives de connexion
- Comprendre les patterns d'utilisation

**Avantages** :
- âœ… Gratuit et open-source
- âœ… TrÃ¨s dÃ©taillÃ©
- âœ… Facile Ã  utiliser (un seul binaire)
- âœ… GÃ©nÃ¨re de beaux rapports HTML

**Limites** :
- â±ï¸ Pas de temps rÃ©el (analyse post-mortem)
- ğŸ“Š NÃ©cessite que PostgreSQL logue suffisamment d'informations
- ğŸ”„ Doit Ãªtre exÃ©cutÃ© manuellement ou via cron

### 2. Extensions PostgreSQL internes

**Principe** : Extensions installÃ©es dans PostgreSQL qui collectent des mÃ©triques systÃ¨me.

**CaractÃ©ristiques** :
- ğŸ“Š MÃ©triques en temps rÃ©el
- ğŸ”— IntÃ©gration native avec PostgreSQL
- ğŸ’¡ CorrÃ©lation entre mÃ©triques SQL et systÃ¨me
- âš¡ LÃ©ger impact sur les performances (<2%)

**Outil principal** : **pg_stat_kcache**

**Cas d'usage** :
- DÃ©terminer si une requÃªte est CPU-bound ou I/O-bound
- Mesurer la consommation rÃ©elle de ressources par requÃªte
- Optimiser en fonction des mÃ©triques systÃ¨me
- CorrÃ©ler problÃ¨mes PostgreSQL et infrastructure

**Avantages** :
- âœ… MÃ©triques prÃ©cises au niveau requÃªte
- âœ… ComplÃ¨te pg_stat_statements
- âœ… IdÃ©al pour l'optimisation fine
- âœ… Open-source

**Limites** :
- ğŸ§ Linux uniquement (utilise /proc)
- ğŸ”§ NÃ©cessite installation d'extension
- ğŸ“Š Requiert pg_stat_statements
- ğŸ’¾ MÃ©triques cumulatives (pas d'historique natif)

### 3. Stack de monitoring temps rÃ©el

**Principe** : Architecture complÃ¨te pour collecter, stocker et visualiser les mÃ©triques en continu.

**CaractÃ©ristiques** :
- â° Monitoring en temps rÃ©el (rafraÃ®chissement toutes les 15-60s)
- ğŸ“ˆ Historique des mÃ©triques (rÃ©tention configurable)
- ğŸš¨ Alerting automatique
- ğŸ¨ Dashboards interactifs

**Stack principal** : **Prometheus + postgres_exporter + Grafana**

**Cas d'usage** :
- Surveiller plusieurs instances PostgreSQL
- CrÃ©er des dashboards pour toute l'Ã©quipe
- Recevoir des alertes Slack/email en cas de problÃ¨me
- Analyser les tendances sur plusieurs semaines/mois

**Avantages** :
- âœ… Standard de l'industrie (CNCF)
- âœ… Open-source et gratuit
- âœ… TrÃ¨s flexible et extensible
- âœ… Grande communautÃ©
- âœ… Dashboards partagÃ©s (Grafana)

**Limites** :
- ğŸ”§ Installation et configuration plus complexes
- ğŸ–¥ï¸ Infrastructure Ã  gÃ©rer (serveurs Prometheus/Grafana)
- ğŸ“š Courbe d'apprentissage (PromQL)
- ğŸ’¾ Consommation de ressources (stockage des mÃ©triques)

### 4. Solutions cloud natives

**Principe** : Services de monitoring intÃ©grÃ©s fournis par les clouds providers (AWS, Azure, GCP).

**CaractÃ©ristiques** :
- â˜ï¸ ZÃ©ro infrastructure Ã  gÃ©rer
- ğŸ”— IntÃ©gration automatique avec les services cloud
- ğŸ“Š MÃ©triques collectÃ©es par dÃ©faut
- ğŸ’³ ModÃ¨le de tarification pay-as-you-go

**Outils principaux** :
- **AWS CloudWatch** (avec Performance Insights)
- **Azure Monitor** (avec Query Performance Insights)
- **Google Cloud Monitoring** (avec Query Insights)

**Cas d'usage** :
- PostgreSQL hÃ©bergÃ© sur RDS, Azure Database, Cloud SQL
- Monitoring multi-services (DB + apps + infra)
- Ã‰quipe sans expertise monitoring
- Besoin de dÃ©marrer rapidement

**Avantages** :
- âœ… DÃ©ploiement instantanÃ©
- âœ… ScalabilitÃ© automatique
- âœ… Maintenance zÃ©ro
- âœ… IntÃ©gration native avec le cloud
- âœ… Support professionnel

**Limites** :
- ğŸ’° CoÃ»t potentiellement Ã©levÃ©
- ğŸ”’ Vendor lock-in
- ğŸ¨ Moins de flexibilitÃ© qu'une solution open-source
- ğŸ“Š LimitÃ© aux services cloud du provider

---

## Tableau comparatif des approches

| CritÃ¨re | Analyse logs (pgBadger) | Extensions (pg_stat_kcache) | Stack temps rÃ©el (Prometheus) | Cloud natives (CloudWatch) |
|---------|-------------------------|----------------------------|-------------------------------|---------------------------|
| **ComplexitÃ©** | â­ TrÃ¨s simple | â­â­ Simple | â­â­â­â­ AvancÃ©e | â­â­ Simple |
| **CoÃ»t** | Gratuit | Gratuit | Gratuit (infra Ã  gÃ©rer) | Payant |
| **Temps rÃ©el** | âŒ Non (post-mortem) | âœ… Oui | âœ… Oui | âœ… Oui |
| **Historique** | âœ… IllimitÃ© (logs) | âŒ Non (cumulatif) | âœ… Configurable | âœ… Configurable |
| **Alerting** | âŒ Non | âŒ Non | âœ… Oui | âœ… Oui |
| **Maintenance** | â­ Aucune | â­ Minimale | â­â­â­â­ Importante | â­ Aucune |
| **Multi-instances** | â­â­ Possible | â­â­â­ Possible | â­â­â­â­â­ Excellent | â­â­â­â­â­ Excellent |
| **Visualisation** | ğŸ“„ Rapports HTML statiques | ğŸ“Š SQL queries | ğŸ“ˆ Dashboards Grafana | ğŸ“Š Dashboards cloud |
| **Performance impact** | âœ… Aucun (hors ligne) | âš ï¸ LÃ©ger (~1-2%) | âš ï¸ LÃ©ger (~1-2%) | âš ï¸ LÃ©ger |
| **Courbe d'apprentissage** | â­ Facile | â­â­ Moyenne | â­â­â­â­ Ã‰levÃ©e | â­â­ Moyenne |

---

## Quelle approche choisir ?

Il n'y a pas **une seule bonne rÃ©ponse**. Le choix dÃ©pend de votre contexte.

### Recommandations par contexte

#### Startup / Petit projet

**Besoins** : Simple, gratuit, dÃ©marrage rapide

**Recommandation** :
```
Base : pg_stat_statements (intÃ©grÃ© PostgreSQL)
â†“
Analyse ponctuelle : pgBadger (quand problÃ¨me)
â†“
Si PostgreSQL sur cloud : Utiliser CloudWatch/Azure Monitor/GCP
```

**Pourquoi** : Minimal viable, zÃ©ro coÃ»t, efficace.

#### Projet en croissance

**Besoins** : Monitoring continu, alerting, Ã©quipe qui grandit

**Recommandation** :
```
Stack : Prometheus + postgres_exporter + Grafana
â†“
ComplÃ©ter avec : pgBadger pour analyses approfondies
â†“
+ pg_stat_kcache si besoin d'optimisation fine
```

**Pourquoi** : Ã‰volutif, pas de vendor lock-in, gratuit.

#### Entreprise / Production critique

**Besoins** : FiabilitÃ©, SLA, support, multi-environnements

**Recommandation** :
```
Option A (Cloud) : CloudWatch/Azure Monitor/GCP (selon cloud)
Option B (Hybride) : Prometheus + Grafana + pgBadger
Option C (Commercial) : Datadog, New Relic (APM complet)
```

**Pourquoi** : Support professionnel, SLA garantis, features avancÃ©es.

#### Multi-cloud / Hybride

**Besoins** : Vue unifiÃ©e sur plusieurs clouds/on-premise

**Recommandation** :
```
Hub central : Grafana (avec plusieurs data sources)
â†“
Data sources : Prometheus (on-prem) + CloudWatch (AWS) + Azure Monitor
â†“
OU : Solution tierce (Datadog, New Relic) pour unification complÃ¨te
```

**Pourquoi** : Seule faÃ§on d'avoir une vue cohÃ©rente.

---

## StratÃ©gie de monitoring complÃ¨te

**L'idÃ©al n'est pas de choisir UN outil, mais de combiner plusieurs approches** en fonction de leurs forces respectives.

### Stack recommandÃ©e (Production)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Monitoring complet                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Temps rÃ©el & Alerting                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Prometheus + Grafana                       â”‚         â”‚
â”‚  â”‚ â€¢ Dashboards live                          â”‚         â”‚
â”‚  â”‚ â€¢ Alertes Slack/email                      â”‚         â”‚
â”‚  â”‚ â€¢ Historique 30 jours                      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                         â”‚
â”‚  Analyse approfondie                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ pgBadger (hebdomadaire)                    â”‚         â”‚
â”‚  â”‚ â€¢ Analyse dÃ©taillÃ©e des requÃªtes           â”‚         â”‚
â”‚  â”‚ â€¢ Audit de sÃ©curitÃ©                        â”‚         â”‚
â”‚  â”‚ â€¢ Tendances sur la semaine                 â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                         â”‚
â”‚  Optimisation fine                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ pg_stat_kcache                             â”‚         â”‚
â”‚  â”‚ â€¢ MÃ©triques CPU/I/O par requÃªte            â”‚         â”‚
â”‚  â”‚ â€¢ Identification CPU-bound vs I/O-bound    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                         â”‚
â”‚  MÃ©triques systÃ¨me                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ node_exporter (Prometheus)                 â”‚         â”‚
â”‚  â”‚ â€¢ CPU, RAM, Disque, RÃ©seau                 â”‚         â”‚
â”‚  â”‚ â€¢ CorrÃ©lation infra/PostgreSQL             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow type

**1. Monitoring quotidien** :
- Consulter les dashboards Grafana
- VÃ©rifier les alertes (s'il y en a)
- Observer les tendances (CPU, connexions, cache hit ratio)

**2. Analyse hebdomadaire** :
- GÃ©nÃ©rer un rapport pgBadger
- Identifier les nouvelles requÃªtes lentes
- Comparer avec la semaine prÃ©cÃ©dente

**3. Optimisation ciblÃ©e** :
- Utiliser pg_stat_kcache pour analyser une requÃªte spÃ©cifique
- DÃ©terminer si le problÃ¨me est CPU, I/O, ou autre
- Appliquer l'optimisation appropriÃ©e

**4. Planification mensuelle** :
- Analyser les tendances de croissance
- Anticiper les besoins futurs (CPU, RAM, Disque)
- Ajuster les ressources si nÃ©cessaire

---

## Les mÃ©triques essentielles Ã  surveiller

Quel que soit l'outil choisi, certaines mÃ©triques sont **incontournables**.

### MÃ©triques critiques (alertes obligatoires)

| MÃ©trique | Seuil alerte | Impact si dÃ©passÃ© |
|----------|--------------|-------------------|
| **Espace disque disponible** | < 20% | Base inaccessible si 0% |
| **CPU utilisation** | > 80% sur 10min | Ralentissement gÃ©nÃ©ralisÃ© |
| **Connexions actives** | > 80% de max_connections | Nouvelles connexions refusÃ©es |
| **RÃ©plication lag** | > 60 secondes | DonnÃ©es obsolÃ¨tes sur replicas |
| **Cache hit ratio** | < 90% | Performances dÃ©gradÃ©es (I/O) |

### MÃ©triques importantes (surveillance rÃ©guliÃ¨re)

| MÃ©trique | Valeur cible | Pourquoi |
|----------|--------------|----------|
| **Transactions committÃ©es** | Stable ou croissant | SantÃ© de l'activitÃ© |
| **Transactions rollbackÃ©es** | < 5% du total | ProblÃ¨mes applicatifs |
| **Checkpoints forcÃ©s** | < 20% du total | Configuration WAL sous-optimale |
| **Dead tuples ratio** | < 10% | Besoin de VACUUM |
| **Temps moyen de requÃªte** | < 100ms | Performances utilisateur |

### MÃ©triques de tendance (planification)

| MÃ©trique | Observation | Action |
|----------|-------------|--------|
| **Croissance de la base** | GB/jour, GB/mois | PrÃ©voir scaling disque |
| **Ã‰volution des connexions** | Pics, moyennes | Dimensionner connection pool |
| **Patterns d'utilisation** | Heures de pointe | Planifier maintenance hors pics |
| **RequÃªtes lentes** | Nouvelles vs rÃ©currentes | Prioriser optimisations |

---

## PrÃ©parer PostgreSQL pour le monitoring

Avant de dÃ©ployer des outils de monitoring, il faut **configurer PostgreSQL** pour qu'il expose les bonnes informations.

### Configuration minimale recommandÃ©e

**Dans postgresql.conf** :

```ini
# ============================================
# Configuration pour le monitoring
# ============================================

# 1. Logging
logging_collector = on
log_destination = 'stderr'
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB

# Format des logs (pour pgBadger et autres)
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

# Quoi logger
log_connections = on
log_disconnections = on
log_duration = off  # off car on utilise log_min_duration_statement
log_min_duration_statement = 1000  # 1 seconde (ajuster selon contexte)
log_checkpoints = on
log_lock_waits = on
log_temp_files = 0  # Logger tous les fichiers temporaires

# 2. Statistiques (pg_stat_statements)
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
pg_stat_statements.track_utility = on

# 3. Autres paramÃ¨tres utiles
track_activities = on
track_counts = on
track_io_timing = on  # Peut avoir lÃ©ger impact, mais trÃ¨s utile
track_functions = all
```

**Pourquoi chaque paramÃ¨tre** :

- `logging_collector = on` : Active la collecte de logs (essentiel pour pgBadger)
- `log_min_duration_statement` : Logger les requÃªtes lentes uniquement (rÃ©duit le volume)
- `log_checkpoints` : Comprendre les patterns de checkpoints
- `pg_stat_statements` : Extension de base pour toutes les statistiques de requÃªtes
- `track_io_timing` : Mesurer les temps I/O (crucial pour optimisation)

### CrÃ©er l'extension pg_stat_statements

```sql
-- Se connecter en tant que superuser
psql -U postgres -d mydatabase

-- CrÃ©er l'extension
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- VÃ©rifier
SELECT * FROM pg_stat_statements LIMIT 5;
```

### RedÃ©marrer PostgreSQL

```bash
# RedÃ©marrage nÃ©cessaire pour shared_preload_libraries
sudo systemctl restart postgresql

# Ou
sudo pg_ctlcluster 18 main restart
```

---

## Les sections suivantes

Dans les sous-sections qui suivent, nous allons explorer en dÃ©tail chaque catÃ©gorie d'outils :

### 14.7.1. pgBadger : Analyse de logs

**Vous apprendrez** :
- Installer et configurer pgBadger
- GÃ©nÃ©rer des rapports HTML dÃ©taillÃ©s
- Identifier les requÃªtes lentes et les problÃ¨mes
- Automatiser l'analyse quotidienne

**Cas d'usage principal** : Analyse post-mortem, audit, comprÃ©hension des patterns.

### 14.7.2. pg_stat_kcache : MÃ©triques systÃ¨me

**Vous apprendrez** :
- Installer l'extension pg_stat_kcache
- Collecter les mÃ©triques CPU et I/O par requÃªte
- Distinguer les requÃªtes CPU-bound des requÃªtes I/O-bound
- Optimiser en fonction des mÃ©triques systÃ¨me

**Cas d'usage principal** : Optimisation fine des requÃªtes, corrÃ©lation SQL/systÃ¨me.

### 14.7.3. Prometheus + postgres_exporter + Grafana

**Vous apprendrez** :
- DÃ©ployer la stack complÃ¨te (Docker ou natif)
- Configurer postgres_exporter pour PostgreSQL
- CrÃ©er des dashboards Grafana personnalisÃ©s
- Mettre en place des alertes automatiques

**Cas d'usage principal** : Monitoring temps rÃ©el, alerting, dashboards Ã©quipe.

### 14.7.4. Solutions cloud natives

**Vous apprendrez** :
- Utiliser AWS CloudWatch pour RDS PostgreSQL
- Configurer Azure Monitor pour Azure Database
- Exploiter GCP Cloud Monitoring pour Cloud SQL
- Comparer les trois solutions et choisir la bonne

**Cas d'usage principal** : PostgreSQL hÃ©bergÃ© sur le cloud, zÃ©ro infrastructure.

---

## Checklist avant de commencer

Avant de plonger dans les outils, assurez-vous d'avoir :

**PrÃ©requis techniques** :
- [ ] PostgreSQL 18 installÃ© et fonctionnel
- [ ] AccÃ¨s superuser ou privilÃ¨ges suffisants
- [ ] Configuration postgresql.conf Ã©ditable
- [ ] PossibilitÃ© de redÃ©marrer PostgreSQL
- [ ] Espace disque pour les logs (au moins 10GB)

**Connaissances** :
- [ ] Bases de SQL (SELECT, FROM, WHERE)
- [ ] Notions de performance (index, EXPLAIN)
- [ ] Ligne de commande Linux (cd, ls, cat)
- [ ] Concepts de monitoring (mÃ©triques, alertes, dashboards)

**Environnement recommandÃ©** :
- [ ] Environnement de test/staging (ne pas commencer en production)
- [ ] DonnÃ©es rÃ©alistes (pour voir des mÃ©triques significatives)
- [ ] Navigateur web (pour Grafana, pgBadger)
- [ ] Ã‰diteur de texte (vim, nano, VSCode)

---

## Conclusion de l'introduction

Le monitoring n'est pas un luxe, c'est une **nÃ©cessitÃ© absolue** pour toute base de donnÃ©es en production. Sans lui, vous pilotez Ã  l'aveugle et vous dÃ©couvrirez les problÃ¨mes quand il sera trop tard.

**Les points clÃ©s Ã  retenir** :

- âœ… **Monitoring = VisibilitÃ©** : Vous ne pouvez pas amÃ©liorer ce que vous ne mesurez pas
- âœ… **Proactif > RÃ©actif** : DÃ©tecter avant que Ã§a casse
- âœ… **Plusieurs outils** : Combiner les approches selon les besoins
- âœ… **Commencer simple** : pgBadger + pg_stat_statements suffit pour dÃ©buter
- âœ… **Ã‰voluer progressivement** : Ajouter Prometheus/Grafana quand le besoin se fait sentir

**Dans les sections suivantes**, nous allons passer de la thÃ©orie Ã  la pratique en explorant chaque outil en dÃ©tail, avec des exemples concrets et des configurations prÃªtes Ã  l'emploi.

**PrÃªt Ã  prendre le contrÃ´le de votre PostgreSQL ?** Passons Ã  la premiÃ¨re section : pgBadger.

---

## Ressources complÃ©mentaires

### Documentation officielle PostgreSQL

- **Monitoring Database Activity** : https://www.postgresql.org/docs/current/monitoring.html
- **Statistics Collector** : https://www.postgresql.org/docs/current/monitoring-stats.html
- **pg_stat_statements** : https://www.postgresql.org/docs/current/pgstatstatements.html

### Livres recommandÃ©s

- "PostgreSQL: Up and Running" - Regina Obe, Leo Hsu (Chapitre Monitoring)
- "PostgreSQL 14 Administration Cookbook" - Simon Riggs (Recipes monitoring)
- "The Art of PostgreSQL" - Dimitri Fontaine (Performance monitoring)

### Blogs et articles

- **2ndQuadrant Blog** : https://www.2ndquadrant.com/en/blog/ (excellent contenu monitoring)
- **Percona PostgreSQL Blog** : https://www.percona.com/blog/category/postgresql/
- **Cybertec PostgreSQL** : https://www.cybertec-postgresql.com/en/blog/

### CommunautÃ©s

- **PostgreSQL Mailing Lists** : pgsql-general, pgsql-performance
- **Reddit** : r/PostgreSQL
- **Discord** : Serveurs PostgreSQL francophones et anglophones
- **Stack Overflow** : Tag [postgresql]

---


â­ï¸ [pgBadger : Analyse de logs](/14-observabilite-et-monitoring/07.1-pgbadger.md)
