üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.6. M√©triques Vitales

## Introduction

Imaginez que vous pilotez un avion. Sur votre tableau de bord, parmi des dizaines d'indicateurs, certains sont **absolument critiques** : l'altitude, la vitesse, le niveau de carburant, la temp√©rature des moteurs. Ces quelques m√©triques vous disent imm√©diatement si tout va bien ou si vous devez agir.

PostgreSQL, c'est pareil. Il existe des centaines de statistiques disponibles, mais seulement **quelques m√©triques vitales** vous donnent l'essentiel de l'information sur la sant√© de votre base de donn√©es.

Dans ce chapitre, nous allons d√©couvrir les **5 m√©triques vitales** que tout administrateur de base de donn√©es doit surveiller en priorit√©. Ce sont les indicateurs qui font la diff√©rence entre un syst√®me performant et un syst√®me qui va bient√¥t rencontrer des probl√®mes.

---

## Qu'est-ce qu'une M√©trique Vitale ?

### D√©finition

Une **m√©trique vitale** est un indicateur de performance qui :

1. **R√©v√®le l'√©tat de sant√© global** du syst√®me
2. **Pr√©dit les probl√®mes** avant qu'ils deviennent critiques
3. **Guide les d√©cisions** d'optimisation
4. **Se lit rapidement** et se comprend facilement

### Analogie M√©dicale

Quand vous allez chez le m√©decin, il prend d'abord vos **signes vitaux** :

| Signe Vital | Signification | Alerte si... |
|-------------|---------------|--------------|
| **Temp√©rature** | Infection/Inflammation | > 38¬∞C |
| **Tension art√©rielle** | Sant√© cardiovasculaire | > 140/90 |
| **Fr√©quence cardiaque** | Effort du c≈ìur | > 100 bpm au repos |
| **Saturation O‚ÇÇ** | Oxyg√©nation | < 95% |

Ces 4 mesures simples donnent une vision globale de votre √©tat de sant√©. Le m√©decin n'a pas besoin de faire 50 examens pour savoir si vous allez bien.

**PostgreSQL a aussi ses signes vitaux !**

---

## Pourquoi ces M√©triques sont Essentielles

### 1. D√©tection Pr√©coce des Probl√®mes

Les m√©triques vitales agissent comme un **syst√®me d'alerte pr√©coce**.

**Sans monitoring :**
```
Jour 1 : Tout semble normal
Jour 5 : Quelques lenteurs occasionnelles
Jour 10 : Base de donn√©es tr√®s lente
Jour 15 : üí• Crash complet - Clients furieux - Urgence 3h du matin
```

**Avec monitoring des m√©triques vitales :**
```
Jour 1 : Alerte "Cache Hit Ratio en baisse" ‚Üí Investigation
Jour 2 : Identification du probl√®me (bloat) ‚Üí Action planifi√©e
Jour 3 : Correction appliqu√©e en heures creuses
Jour 4+ : Tout fonctionne normalement ‚úÖ
```

**B√©n√©fice :** Vous passez de la **gestion de crise** √† la **gestion proactive**.

### 2. Priorisation des Efforts

Vous ne pouvez pas tout optimiser en m√™me temps. Les m√©triques vitales vous disent **o√π agir en priorit√©**.

**Exemple de dashboard :**
```
Cache Hit Ratio       : 99.2% üü¢ ‚Üí OK, ne rien faire
Bloat                : 8%    üü¢ ‚Üí OK, surveiller
I/O Wait             : 45%   üî¥ ‚Üí PRIORIT√â 1 : Probl√®me critique !
Connexions actives   : 65%   üü° ‚Üí √Ä surveiller
Checkpoints forc√©s   : 5%    üü¢ ‚Üí OK
```

**Conclusion imm√©diate :** Concentrez vos efforts sur l'I/O avant tout le reste.

### 3. Mesure de l'Impact des Changements

Quand vous faites une modification (ajout d'index, changement de configuration), les m√©triques vitales vous disent **si √ßa a fonctionn√©**.

**Exemple :**
```
AVANT l'ajout d'un index :
- Cache Hit Ratio : 92%
- I/O Wait : 25%
- Requ√™tes lentes : 150/min

APR√àS l'ajout d'un index :
- Cache Hit Ratio : 97%  ‚úÖ +5%
- I/O Wait : 12%         ‚úÖ -13%
- Requ√™tes lentes : 20/min ‚úÖ -87%
```

**Verdict :** L'optimisation est un succ√®s, mesurable et quantifiable.

### 4. Communication avec les Parties Prenantes

Les m√©triques vitales permettent d'expliquer simplement la situation aux non-techniciens.

**Mauvaise communication (trop technique) :**
> "Le ratio de dirty pages dans le shared buffer pool a augment√© de 15 points de base suite √† une augmentation du facteur de charge transactionnelle..."

**Bonne communication (m√©trique vitale) :**
> "Notre base de donn√©es utilise 95% de ses connexions disponibles. Nous devons passer √† un syst√®me de pooling cette semaine pour √©viter une saturation."

---

## Vue d'Ensemble des 5 M√©triques Vitales

Voici les 5 m√©triques que nous allons explorer en d√©tail dans les sections suivantes :

### 1. Cache Hit Ratio (Taux de Succ√®s du Cache)

**Question :** Mes donn√©es sont-elles en m√©moire ou PostgreSQL attend-il constamment apr√®s le disque ?

**Indicateur :** Pourcentage de lectures servies depuis la RAM vs le disque

**Seuil cible :** > 99%

**Impact si probl√®me :**
- Requ√™tes lentes
- I/O √©lev√©
- Serveur qui "rame"

**Ce que vous apprendrez :**
- Comment mesurer le cache hit ratio
- Interpr√©ter les r√©sultats
- Augmenter l'efficacit√© du cache
- Optimiser shared_buffers

### 2. Table et Index Bloat (Gonflement)

**Question :** Ma base de donn√©es contient-elle beaucoup d'espace mort inutilis√© ?

**Indicateur :** Pourcentage de "d√©chets" dans vos tables et index

**Seuil cible :** < 20%

**Impact si probl√®me :**
- Tables anormalement volumineuses
- Performances d√©grad√©es progressivement
- Sauvegardes plus longues
- Cache moins efficace

**Ce que vous apprendrez :**
- Comprendre le bloat et pourquoi il se produit
- Le mesurer pr√©cis√©ment
- Le pr√©venir avec VACUUM
- Le corriger si n√©cessaire

### 3. I/O Wait et Disk Latency (Attente Disque)

**Question :** Mon syst√®me passe-t-il trop de temps √† attendre le disque ?

**Indicateur :** Pourcentage de temps CPU en attente I/O + latence disque

**Seuil cible :** I/O Wait < 10%, Latency < 5ms (SSD)

**Impact si probl√®me :**
- Lenteurs g√©n√©ralis√©es
- CPU inactif mais syst√®me lent
- Gaspillage de ressources

**Ce que vous apprendrez :**
- Mesurer l'I/O Wait avec iostat et top
- Comprendre la latence disque
- Identifier les requ√™tes probl√©matiques
- Optimiser (SSD, index, cache)

### 4. Connexions Actives et Pools Satur√©s

**Question :** Ai-je trop ou pas assez de connexions disponibles ?

**Indicateur :** Nombre de connexions actives vs limite maximale

**Seuil cible :** < 70% de max_connections

**Impact si probl√®me :**
- Erreurs "too many clients"
- Timeouts fr√©quents
- Saturation m√©moire
- Performances impr√©visibles

**Ce que vous apprendrez :**
- Comprendre le co√ªt d'une connexion
- D√©tecter les connection leaks
- Impl√©menter PgBouncer (pooling)
- Dimensionner correctement

### 5. Checkpoints et WAL Generation

**Question :** Mes checkpoints causent-ils des ralentissements p√©riodiques ?

**Indicateur :** Fr√©quence et dur√©e des checkpoints + volume WAL

**Seuil cible :** Checkpoints forc√©s < 10%, dur√©e < timeout

**Impact si probl√®me :**
- Pics de lenteur r√©guliers (ex: toutes les 5 min)
- I/O burst pendant checkpoints
- Exp√©rience utilisateur d√©grad√©e

**Ce que vous apprendrez :**
- Comprendre le WAL et les checkpoints
- Configurer max_wal_size et checkpoint_timeout
- Lisser les √©critures
- Monitorer efficacement

---

## Comment Utiliser ce Chapitre

### Pour les D√©butants

**Approche recommand√©e :**

1. **Lisez l'introduction** (cette page) pour comprendre la vue d'ensemble
2. **√âtudiez chaque m√©trique** dans l'ordre (14.6.1 ‚Üí 14.6.5)
3. **Appliquez imm√©diatement** : Mesurez ces m√©triques sur votre syst√®me
4. **Cr√©ez un dashboard simple** avec ces 5 indicateurs
5. **Revenez r√©guli√®rement** : Ces m√©triques doivent devenir une seconde nature

**Temps estim√© :** 5-8 heures pour ma√Ætriser les 5 m√©triques

### Pour les Administrateurs Exp√©riment√©s

**Utilisation en r√©f√©rence :**

- Consultez directement la m√©trique qui vous int√©resse
- Utilisez les requ√™tes SQL fournies
- Int√©grez les alertes recommand√©es dans votre monitoring
- Adaptez les seuils √† votre contexte sp√©cifique

### Pour les DevOps/SRE

**Focus op√©rationnel :**

- Automatisez la collecte de ces m√©triques
- Cr√©ez des dashboards Grafana
- Configurez des alertes PagerDuty/Slack
- Documentez vos seuils dans des runbooks

---

## La Pyramide du Monitoring

### Niveau 1 : M√©triques Vitales (Ce chapitre)

**5 m√©triques essentielles**
- Cache Hit Ratio
- Bloat
- I/O Wait
- Connexions
- Checkpoints

**Fr√©quence :** Surveillance continue (temps r√©el)

**Action :** Si l'une est dans le rouge ‚Üí Investigation imm√©diate

### Niveau 2 : M√©triques Secondaires

**20-30 m√©triques importantes**
- Query duration (p50, p95, p99)
- Table sizes
- Index usage
- Replication lag
- Transaction rate
- etc.

**Fr√©quence :** Consultation quotidienne

**Action :** Tendances et optimisations planifi√©es

### Niveau 3 : M√©triques D√©taill√©es

**100+ m√©triques sp√©cifiques**
- Toutes les vues pg_stat_*
- M√©triques syst√®me compl√®tes
- M√©triques applicatives

**Fr√©quence :** Consultation lors d'investigations

**Action :** Debug approfondi et troubleshooting

**Principe 80/20 :** 80% de la valeur vient des 20% de m√©triques (les m√©triques vitales).

---

## Outils Recommand√©s pour le Monitoring

### Pour D√©buter (Gratuit et Simple)

#### 1. Requ√™tes SQL Manuelles

Chaque section fournit des requ√™tes SQL √† copier-coller.

**Avantages :**
- ‚úÖ Pas d'installation
- ‚úÖ Fonctionne partout
- ‚úÖ Apprentissage des concepts

**Inconv√©nients :**
- ‚ùå Manuel (pas de temps r√©el)
- ‚ùå Pas d'historique
- ‚ùå Pas d'alertes

#### 2. pgAdmin

Interface graphique avec monitoring int√©gr√©.

**Avantages :**
- ‚úÖ Visuel et intuitif
- ‚úÖ Dashboard simple
- ‚úÖ Gratuit

**Inconv√©nients :**
- ‚ùå Limit√© en fonctionnalit√©s
- ‚ùå Pas d'alertes avanc√©es

### Pour la Production (Professionnel)

#### 1. Stack Prometheus + Grafana

**Architecture :**
```
PostgreSQL ‚Üí postgres_exporter ‚Üí Prometheus ‚Üí Grafana
```

**Avantages :**
- ‚úÖ Open source
- ‚úÖ Tr√®s puissant
- ‚úÖ Alertes compl√®tes
- ‚úÖ Historique long terme
- ‚úÖ Standard de l'industrie

**Effort :** 1-2 jours d'installation et configuration

#### 2. Solutions Cloud Natives

**AWS CloudWatch RDS**, **Azure Monitor**, **GCP Cloud Monitoring**

**Avantages :**
- ‚úÖ Int√©gr√© si vous utilisez RDS/Cloud SQL
- ‚úÖ Z√©ro configuration
- ‚úÖ Alertes automatiques

**Inconv√©nients :**
- ‚ùå Co√ªt suppl√©mentaire
- ‚ùå Moins de flexibilit√©

#### 3. Solutions Commerciales

**DataDog**, **New Relic**, **AppDynamics**

**Avantages :**
- ‚úÖ Tout-en-un
- ‚úÖ Support professionnel
- ‚úÖ IA et pr√©dictions

**Inconv√©nients :**
- ‚ùå Co√ªteux (50-500$/mois)

---

## M√©thodologie : De la Mesure √† l'Action

### √âtape 1 : √âtat des Lieux (Baseline)

**Objectif :** Conna√Ætre vos m√©triques actuelles

**Actions :**
1. Ex√©cuter les requ√™tes de mesure pour chaque m√©trique
2. Noter les valeurs dans un tableau
3. Comparer aux seuils recommand√©s
4. Identifier les m√©triques "dans le rouge"

**Dur√©e :** 30 minutes

### √âtape 2 : Priorisation

**Objectif :** D√©terminer quoi corriger en premier

**R√®gle de priorisation :**
```
Priorit√© 1 (üî¥) : Critique - Action sous 24h
    - Cache Hit Ratio < 90%
    - I/O Wait > 30%
    - Connexions > 90%

Priorit√© 2 (üü†) : Important - Action sous 1 semaine
    - Cache Hit Ratio < 95%
    - Bloat > 30%
    - Checkpoints forc√©s > 30%

Priorit√© 3 (üü°) : √Ä surveiller - Action planifiable
    - Toute m√©trique en zone orange
```

### √âtape 3 : Investigation

**Objectif :** Comprendre la cause racine

**Approche :**
1. Lire la section d√©taill√©e de la m√©trique probl√©matique
2. Ex√©cuter les requ√™tes de diagnostic fournies
3. Identifier la cause (requ√™te lente, bloat, config, etc.)
4. Documenter vos d√©couvertes

**Dur√©e :** 1-4 heures selon complexit√©

### √âtape 4 : Action

**Objectif :** Corriger le probl√®me

**Types d'actions :**

**Actions imm√©diates (< 1 heure) :**
- Ajout d'index
- VACUUM manuel
- Terminer connexions zombie
- Ajustement configuration (reload)

**Actions planifi√©es (fen√™tre maintenance) :**
- VACUUM FULL
- REINDEX
- Migration SSD
- Redimensionnement serveur

### √âtape 5 : Validation

**Objectif :** V√©rifier que la correction a fonctionn√©

**Actions :**
1. Re-mesurer la m√©trique apr√®s l'action
2. Comparer avant/apr√®s
3. Surveiller pendant 24-48h
4. Documenter le r√©sultat

**Crit√®re de succ√®s :** M√©trique revenue dans la zone verte

### √âtape 6 : Pr√©vention

**Objectif :** √âviter que le probl√®me se reproduise

**Actions :**
- Mettre en place alertes automatiques
- Documenter dans un runbook
- Automatiser la correction si possible
- Revoir la configuration pour pr√©venir r√©currence

---

## Exemple de Dashboard Simple

Voici un exemple de dashboard minimaliste mais efficace que vous pouvez cr√©er :

### Dashboard "PostgreSQL Health"

**Section 1 : M√©triques Vitales (Gauges)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cache Hit Ratio        Bloat         I/O Wait  ‚îÇ
‚îÇ     [99.2%]üü¢          [12%]üü¢        [8%]üü¢    ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  Connexions          Checkpoints Forc√©s         ‚îÇ
‚îÇ    [45/100]üü¢           [7%]üü¢                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Section 2 : Graphiques Temporels**

```
Cache Hit Ratio (24h)
100% |----------------------------------------
 95% |========================================
 90% |----------------------------------------
     +----------------------------------------
       0h     6h     12h    18h    24h
      Stable √† 99% toute la journ√©e ‚úÖ


I/O Wait % (24h)
 30% |----------------------------------------
 15% |          ‚ñà‚ñà
  5% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  0% +----------------------------------------
       0h     6h     12h    18h    24h
      Pic √† 15% pendant checkpoint (normal) ‚úÖ
```

**Section 3 : Top N**

```
Top 5 Tables par Bloat          Top 5 Requ√™tes I/O
1. orders       : 35% üî¥      1. SELECT ... : 5.2s
2. logs         : 28% üü†      2. UPDATE ... : 3.8s
3. sessions     : 15% üü°      3. INSERT ... : 2.1s
4. products     : 8%  üü¢      4. DELETE ... : 1.5s
5. users        : 5%  üü¢      5. SELECT ... : 1.2s
```

**L√©gende Couleurs :**
- üü¢ Vert : OK
- üü° Jaune : √Ä surveiller
- üü† Orange : Attention
- üî¥ Rouge : Action requise

---

## Fr√©quence de Consultation Recommand√©e

### Monitoring Actif (Production)

**Temps r√©el (Grafana/DataDog) :**
- Dashboard affich√© en permanence dans la salle de contr√¥le
- Alertes envoy√©es automatiquement
- Consultation r√©flexe en cas de probl√®me

### Revue Quotidienne (10 minutes)

**Chaque matin :**
1. Consulter le dashboard des m√©triques vitales
2. V√©rifier que tout est dans le vert
3. Noter les tendances (am√©lioration/d√©gradation)
4. Si orange/rouge : Planifier investigation

### Revue Hebdomadaire (30 minutes)

**Chaque lundi :**
1. Analyser les tendances de la semaine
2. Identifier les patterns (jour/heure probl√©matiques)
3. Planifier optimisations si n√©cessaire
4. Documenter les incidents r√©solus

### Revue Mensuelle (2 heures)

**D√©but de chaque mois :**
1. Analyse approfondie des 5 m√©triques
2. Comparaison mois N vs mois N-1
3. Ajustement des seuils d'alerte si besoin
4. Planning d'optimisations pour le mois
5. Rapport pour le management

---

## Checklist de D√©marrage

Voici une checklist pour bien d√©marrer avec les m√©triques vitales :

### Phase 1 : Pr√©paration (Jour 1)

- [ ] Lire cette introduction compl√®tement
- [ ] Choisir un outil de monitoring (commencer simple)
- [ ] S'assurer d'avoir acc√®s en lecture √† PostgreSQL
- [ ] Cr√©er un document pour noter les r√©sultats

### Phase 2 : Mesure Initiale (Jour 1-2)

- [ ] Mesurer le Cache Hit Ratio
- [ ] Mesurer le Bloat des tables principales
- [ ] Mesurer l'I/O Wait
- [ ] Compter les connexions actives
- [ ] V√©rifier les statistiques de checkpoints
- [ ] Documenter toutes les valeurs (baseline)

### Phase 3 : Analyse (Jour 2-3)

- [ ] Identifier les m√©triques probl√©matiques
- [ ] Prioriser les actions (üî¥ > üü† > üü°)
- [ ] Lire en d√©tail les sections concern√©es
- [ ] Planifier les corrections

### Phase 4 : Action (Semaine 1)

- [ ] Appliquer les corrections prioritaires
- [ ] Re-mesurer apr√®s chaque action
- [ ] Valider les am√©liorations
- [ ] Documenter ce qui a fonctionn√©

### Phase 5 : Automatisation (Semaine 2-3)

- [ ] Mettre en place un dashboard
- [ ] Configurer des alertes automatiques
- [ ] √âtablir une routine de consultation
- [ ] Former l'√©quipe

### Phase 6 : Am√©lioration Continue (Permanent)

- [ ] Revue quotidienne (10 min)
- [ ] Revue hebdomadaire (30 min)
- [ ] Revue mensuelle (2h)
- [ ] Ajustements et optimisations continues

---

## Ce que Vous Allez Apprendre

En ma√Ætrisant ces 5 m√©triques vitales, vous serez capable de :

### Comp√©tences Techniques

- ‚úÖ **Diagnostiquer** 90% des probl√®mes de performance PostgreSQL
- ‚úÖ **Mesurer** pr√©cis√©ment la sant√© de votre base de donn√©es
- ‚úÖ **Optimiser** de mani√®re cibl√©e et efficace
- ‚úÖ **Pr√©venir** les pannes avant qu'elles surviennent
- ‚úÖ **Expliquer** les probl√®mes en termes compr√©hensibles

### Comp√©tences Op√©rationnelles

- ‚úÖ **Monitorer** efficacement en production
- ‚úÖ **Alerter** au bon moment (ni trop, ni trop peu)
- ‚úÖ **Prioriser** les efforts d'optimisation
- ‚úÖ **Documenter** l'√©tat de sant√© du syst√®me
- ‚úÖ **Communiquer** avec les stakeholders

### √âtat d'Esprit

- ‚úÖ **Proactivit√©** : Agir avant que √ßa casse
- ‚úÖ **Data-driven** : D√©cisions bas√©es sur des mesures
- ‚úÖ **Pragmatisme** : Focus sur ce qui compte vraiment
- ‚úÖ **Confiance** : Savoir o√π regarder quand il y a un probl√®me

---

## Transition vers les Sections D√©taill√©es

Vous √™tes maintenant pr√™t √† plonger dans le d√©tail de chaque m√©trique vitale.

**Ordre recommand√© :**

1. **Cache Hit Ratio** (14.6.1) - Le plus universel et le plus impactant
2. **I/O Wait et Disk Latency** (14.6.3) - Souvent li√© au cache
3. **Table et Index Bloat** (14.6.2) - Affecte cache et I/O
4. **Connexions et Pools** (14.6.4) - Critique pour la disponibilit√©
5. **Checkpoints et WAL** (14.6.5) - Optimisation avanc√©e

Chaque section est **autonome** et peut √™tre consult√©e ind√©pendamment, mais elles sont aussi **interconnect√©es** : am√©liorer l'une impacte souvent les autres positivement.

**Bon monitoring et bonnes optimisations !** üöÄ

---

## Points Cl√©s √† Retenir

‚úÖ **5 m√©triques vitales** suffisent pour 90% du monitoring PostgreSQL

‚úÖ **Principe 80/20** : Ces quelques m√©triques donnent l'essentiel de l'information

‚úÖ **Monitoring proactif** : D√©tecter les probl√®mes avant qu'ils deviennent critiques

‚úÖ **Mesure ‚Üí Action ‚Üí Validation** : Approche m√©thodique et quantifiable

‚úÖ **Commencer simple** : Requ√™tes SQL manuelles puis automatisation progressive

‚úÖ **Dashboard minimal** : 5 indicateurs bien choisis > 50 indicateurs inutilis√©s

‚úÖ **Routine essentielle** : Consultation quotidienne (10 min) + revue hebdomadaire (30 min)

‚úÖ **Chaque m√©trique a un seuil** : Savoir ce qui est OK, √† surveiller, ou critique

---


‚è≠Ô∏è [Cache Hit Ratio (Buffer Cache)](/14-observabilite-et-monitoring/06.1-cache-hit-ratio.md)
