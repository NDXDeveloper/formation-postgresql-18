üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.4. Nouveaut√© PG 18 : Statistiques I/O et WAL par backend

## Introduction

PostgreSQL 18 introduit une fonctionnalit√© r√©volutionnaire pour le diagnostic des performances : la possibilit√© de suivre **les op√©rations I/O (entr√©es/sorties) et la g√©n√©ration de WAL (Write-Ahead Log) au niveau de chaque processus backend** individuel.

Cette nouveaut√© transforme radicalement votre capacit√© √† identifier et r√©soudre les probl√®mes de performance. Mais avant de plonger dans les d√©tails, commen√ßons par comprendre les concepts fondamentaux.

---

## Concepts Fondamentaux : Pour les D√©butants

### Qu'est-ce qu'un "backend" ?

En termes simples, un **backend** (ou processus backend) est un processus du syst√®me d'exploitation d√©di√© √† g√©rer **une seule connexion cliente** √† PostgreSQL.

**Analogie** : Imaginez une banque. Chaque guichetier est un "backend" qui s'occupe d'un client √† la fois. Si vous vous connectez √† PostgreSQL, un processus backend est cr√©√© sp√©cialement pour vous et reste actif pendant toute la dur√©e de votre connexion.

**Points cl√©s** :
- 1 connexion = 1 backend
- Chaque backend a son propre Process ID (PID) dans le syst√®me d'exploitation
- Les backends ex√©cutent vos requ√™tes SQL
- Ils g√®rent la m√©moire, l'I/O, et interagissent avec le disque

**Visualisation** :
```
Client 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Backend (PID: 12345) ‚îÄ‚îê
Client 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Backend (PID: 12346) ‚îÄ‚î§
Client 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Backend (PID: 12347) ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ PostgreSQL
Client 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Backend (PID: 12348) ‚îÄ‚î§      (Data + WAL)
Client 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Backend (PID: 12349) ‚îÄ‚îò
```

### Qu'est-ce que l'I/O (Input/Output) ?

**I/O** signifie "Entr√©es/Sorties" et repr√©sente toutes les op√©rations de **lecture et √©criture sur le disque**.

**Pourquoi c'est important** : Le disque est **beaucoup plus lent** que la RAM (m√©moire). Une requ√™te qui lit beaucoup de donn√©es sur le disque sera lente.

**Types d'I/O dans PostgreSQL** :
1. **Lectures (Read I/O)** : Charger des donn√©es depuis le disque vers la m√©moire
2. **√âcritures (Write I/O)** : Sauvegarder des donn√©es de la m√©moire vers le disque
3. **Cache hits** : Donn√©es trouv√©es en m√©moire (pas d'I/O disque = rapide !)
4. **Cache misses** : Donn√©es non trouv√©es en m√©moire (I/O disque requis = lent)

**Analogie** :
- RAM = Bureau (acc√®s rapide)
- Disque = Armoire d'archives (acc√®s lent)
- I/O = Aller-retour entre votre bureau et l'armoire

### Qu'est-ce que le WAL (Write-Ahead Log) ?

Le **WAL** (aussi appel√© journal de transactions) est un m√©canisme de s√©curit√© qui garantit que vos donn√©es ne seront jamais perdues, m√™me en cas de crash.

**Principe** : Avant de modifier une donn√©e sur le disque, PostgreSQL √©crit d'abord **ce qu'il va faire** dans le WAL. Si le serveur crashe, PostgreSQL peut "rejouer" le WAL pour r√©cup√©rer les changements.

**Analogie** : Le WAL est comme le journal de bord d'un navire. Avant de faire une action importante, le capitaine l'√©crit dans son journal. Si quelque chose tourne mal, il peut consulter le journal pour savoir ce qui s'est pass√© et reprendre l√† o√π il s'√©tait arr√™t√©.

**Ce qui g√©n√®re du WAL** :
- `INSERT`, `UPDATE`, `DELETE` : Modifications de donn√©es
- `CREATE INDEX`, `ALTER TABLE` : Modifications de structure
- `COMMIT` : Validation de transactions
- Checkpoints : Synchronisation p√©riodique

**Pourquoi surveiller le WAL** :
- Trop de WAL = beaucoup d'√©critures = charge √©lev√©e
- Peut saturer le disque ou le r√©seau (r√©plication)
- Indicateur de l'intensit√© des modifications

---

## Le Probl√®me Avant PostgreSQL 18

### Diagnostic limit√© et frustrant

Avant PostgreSQL 18, vous pouviez voir des **statistiques globales** d'I/O et de WAL, mais pas au niveau de chaque connexion/backend individuel.

**Sc√©nario typique** :
```
Vous : "Le serveur est lent !"
Monitoring : "Il y a 200 GO d'I/O en cours"
Vous : "OK... mais QUELLE requ√™te/connexion cause √ßa ?"
Monitoring : "ü§∑ Aucune id√©e"
```

### Ce que vous pouviez voir

**Statistiques globales** (dans `pg_stat_database`, `pg_stat_bgwriter`) :
```sql
SELECT
    blks_read,      -- Total de blocs lus depuis le disque (TOUTES connexions)
    blks_hit        -- Total de blocs trouv√©s en cache (TOUTES connexions)
FROM pg_stat_database
WHERE datname = 'ma_base';

  blks_read  |   blks_hit
-------------+--------------
  45234789   | 923847234
```

**Probl√®me** : Vous voyez l'activit√© totale, mais impossible d'identifier le coupable !

### Ce que vous NE pouviez PAS voir

- ‚ùå Quelle connexion (backend) g√©n√®re le plus d'I/O ?
- ‚ùå Quel utilisateur ou application est responsable ?
- ‚ùå Quelle requ√™te sp√©cifique cause une g√©n√©ration excessive de WAL ?
- ‚ùå Est-ce un backend unique qui pose probl√®me ou plusieurs ?

**R√©sultat** : Des heures de recherche pour trouver la source d'un probl√®me de performance.

---

## La Solution PostgreSQL 18 : Statistiques par Backend

### Vue d'ensemble de la nouveaut√©

PostgreSQL 18 introduit de **nouvelles colonnes dans pg_stat_activity** qui exposent :
- Les statistiques d'I/O pour chaque backend
- La g√©n√©ration de WAL pour chaque backend
- Le tout en temps r√©el !

### Les nouvelles vues et colonnes

#### Extension de pg_stat_activity

`pg_stat_activity` est enrichie avec de nouvelles colonnes d'I/O et WAL :

**Colonnes I/O (nouvelles dans PG 18)** :

| Colonne | Description |
|---------|-------------|
| `io_blks_read` | Nombre de blocs lus depuis le disque par ce backend |
| `io_blks_written` | Nombre de blocs √©crits sur le disque par ce backend |
| `io_blks_hit` | Nombre de blocs trouv√©s dans le cache (shared buffers) |
| `io_read_time` | Temps total pass√© √† lire depuis le disque (en millisecondes) |
| `io_write_time` | Temps total pass√© √† √©crire sur le disque (en millisecondes) |

**Colonnes WAL (nouvelles dans PG 18)** :

| Colonne | Description |
|---------|-------------|
| `wal_records` | Nombre d'enregistrements WAL g√©n√©r√©s |
| `wal_bytes` | Nombre d'octets WAL g√©n√©r√©s |
| `wal_fpi` | Nombre de Full Page Images WAL g√©n√©r√©s |

**Note** : Ces statistiques sont **cumulatives** depuis le d√©but de la connexion du backend.

#### Nouvelle vue : pg_stat_io_backend

PostgreSQL 18 peut aussi introduire une vue d√©di√©e `pg_stat_io_backend` (selon la version finale) qui offre une vision encore plus d√©taill√©e.

---

## Utiliser les Nouvelles Statistiques : Exemples Pratiques

### Exemple 1 : Identifier le backend qui g√©n√®re le plus d'I/O

```sql
SELECT
    pid,
    usename,
    application_name,
    state,
    io_blks_read,
    io_blks_written,
    io_blks_hit,
    ROUND(100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0), 2) AS cache_hit_ratio,
    query
FROM pg_stat_activity
WHERE state = 'active'
  AND io_blks_read + io_blks_written > 0
ORDER BY (io_blks_read + io_blks_written) DESC
LIMIT 10;
```

**Interpr√©tation** :
- `io_blks_read` √©lev√© : Ce backend lit beaucoup depuis le disque (potentiellement lent)
- `io_blks_written` √©lev√© : Ce backend √©crit beaucoup (INSERT/UPDATE/DELETE massif)
- `cache_hit_ratio` faible : Les donn√©es ne sont pas en cache, beaucoup d'I/O disque
- Vous voyez aussi la `query` exacte qui cause ce comportement !

**R√©sultat exemple** :
```
  pid  | usename |  application_name  |  state  | io_blks_read | io_blks_written | io_blks_hit | cache_hit_ratio |              query
-------+---------+--------------------+---------+--------------+-----------------+-------------+-----------------+--------------------------------
 12345 | appuser | MyApp              | active  |      845320  |          12450  |    12340    |           1.44  | SELECT * FROM huge_table WHERE...
 12348 | etl     | DataImport         | active  |        1230  |         945000  |      450    |          26.79  | INSERT INTO logs VALUES...
```

### Exemple 2 : Trouver les backends qui g√©n√®rent le plus de WAL

```sql
SELECT
    pid,
    usename,
    application_name,
    state,
    wal_records,
    pg_size_pretty(wal_bytes) AS wal_size,
    wal_fpi,
    query_start,
    NOW() - query_start AS query_duration,
    LEFT(query, 100) AS query_preview
FROM pg_stat_activity
WHERE wal_bytes > 0
ORDER BY wal_bytes DESC
LIMIT 10;
```

**Interpr√©tation** :
- `wal_bytes` √©lev√© : Ce backend modifie beaucoup de donn√©es
- `wal_fpi` √©lev√© : Beaucoup de Full Page Images (apr√®s un checkpoint)
- `query_duration` long : La requ√™te tourne depuis longtemps
- Corr√©lation entre `wal_bytes` et dur√©e : Requ√™te lente ET gourmande

**Cas d'usage** :
- Identifier les ETL (Extract-Transform-Load) qui surchargent le disque
- D√©tecter les imports massifs mal optimis√©s
- Trouver les transactions longues qui bloquent le VACUUM

### Exemple 3 : Comparer les performances I/O de plusieurs backends

```sql
SELECT
    usename,
    application_name,
    COUNT(*) AS nb_connections,
    SUM(io_blks_read) AS total_reads,
    SUM(io_blks_written) AS total_writes,
    SUM(io_blks_hit) AS total_cache_hits,
    ROUND(AVG(100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0)), 2) AS avg_cache_hit_ratio,
    pg_size_pretty(SUM(wal_bytes)) AS total_wal
FROM pg_stat_activity
WHERE state != 'idle'
GROUP BY usename, application_name
ORDER BY total_reads + total_writes DESC;
```

**Interpr√©tation** :
- Vue agr√©g√©e par utilisateur ou application
- Identifie quelle application consomme le plus de ressources I/O
- Compare le `cache_hit_ratio` entre applications
- Montre la contribution au WAL par application

**R√©sultat exemple** :
```
 usename |  application_name  | nb_connections | total_reads | total_writes | total_cache_hits | avg_cache_hit_ratio | total_wal
---------+--------------------+----------------+-------------+--------------+------------------+--------------------+-----------
 etl     | NightlyImport      |              5 |     4532100 |      2340100 |           234500 |              4.92  | 234 GB
 webapp  | MyWebApp           |             45 |      123400 |        23400 |         23400000 |             99.47  | 1245 MB
 analyst | Tableau            |              3 |     1234500 |          450 |          4500000 |             78.45  | 45 MB
```

**Insight** : L'ETL `NightlyImport` a un cache hit ratio catastrophique (4.92%) et g√©n√®re √©norm√©ment de WAL !

### Exemple 4 : Surveiller l'I/O en temps r√©el

```sql
-- Snapshot 1 : Capturer l'√©tat initial
CREATE TEMP TABLE io_snapshot_1 AS
SELECT
    pid,
    io_blks_read,
    io_blks_written,
    wal_bytes,
    query
FROM pg_stat_activity
WHERE state = 'active';

-- Attendre quelques secondes
SELECT pg_sleep(5);

-- Snapshot 2 : Capturer l'√©tat apr√®s 5 secondes
CREATE TEMP TABLE io_snapshot_2 AS
SELECT
    pid,
    io_blks_read,
    io_blks_written,
    wal_bytes,
    query
FROM pg_stat_activity
WHERE state = 'active';

-- Calculer le delta (l'activit√© pendant ces 5 secondes)
SELECT
    s2.pid,
    s2.query,
    (s2.io_blks_read - COALESCE(s1.io_blks_read, 0)) AS reads_per_5sec,
    (s2.io_blks_written - COALESCE(s1.io_blks_written, 0)) AS writes_per_5sec,
    pg_size_pretty((s2.wal_bytes - COALESCE(s1.wal_bytes, 0))) AS wal_per_5sec
FROM io_snapshot_2 s2
LEFT JOIN io_snapshot_1 s1 USING (pid)
WHERE (s2.io_blks_read - COALESCE(s1.io_blks_read, 0)) > 100
   OR (s2.io_blks_written - COALESCE(s1.io_blks_written, 0)) > 100
ORDER BY reads_per_5sec + writes_per_5sec DESC;
```

**Utilit√©** :
- Mesure l'activit√© I/O **instantan√©e** (sur 5 secondes)
- Identifie les backends qui consomment le plus en ce moment pr√©cis
- Permet de suivre l'√©volution dans le temps

### Exemple 5 : Corr√©lation avec le temps CPU et les locks

```sql
SELECT
    pid,
    usename,
    application_name,
    state,
    io_blks_read + io_blks_written AS total_io_blocks,
    pg_size_pretty(wal_bytes) AS wal_generated,
    NOW() - query_start AS duration,
    NOW() - state_change AS time_in_state,
    wait_event_type,
    wait_event,
    LEFT(query, 80) AS query_preview
FROM pg_stat_activity
WHERE state = 'active'
  AND (io_blks_read + io_blks_written > 1000 OR wal_bytes > 1048576)  -- > 1 MB WAL
ORDER BY total_io_blocks DESC;
```

**Interpr√©tation** :
- `wait_event_type = 'IO'` : Le backend attend des I/O (lent !)
- `wait_event = 'DataFileRead'` : Lecture de fichiers de donn√©es
- `duration` long + `total_io_blocks` √©lev√© : Requ√™te lente √† cause des I/O
- Corr√©lation entre WAL et attentes de locks

---

## Sc√©narios de Diagnostic Avanc√©s

### Sc√©nario 1 : Le serveur est lent, qui est le coupable ?

**Sympt√¥me** : L'ensemble du serveur ralentit. Les utilisateurs se plaignent.

**Diagnostic avec PostgreSQL 18** :

```sql
-- √âtape 1 : Vue d'ensemble des backends actifs
SELECT
    pid,
    usename,
    application_name,
    NOW() - query_start AS duration,
    io_blks_read,
    io_blks_written,
    pg_size_pretty(wal_bytes) AS wal,
    state,
    wait_event_type,
    LEFT(query, 60) AS query
FROM pg_stat_activity
WHERE state != 'idle'
ORDER BY io_blks_read + io_blks_written DESC
LIMIT 20;
```

**Analyse** : Supposons que vous trouvez :
```
  pid  | usename |  duration  | io_blks_read | io_blks_written |   wal   |  state  | wait_event_type |           query
-------+---------+------------+--------------+-----------------+---------+---------+-----------------+--------------------------
 14532 | appuser | 00:45:23   |    23450000  |         450000  | 345 GB  | active  | IO              | DELETE FROM audit_logs WHERE...
```

**Conclusion** : Le backend 14532 effectue une suppression massive qui :
- Lit 23 millions de blocs depuis le disque
- G√©n√®re 345 GB de WAL
- Est en attente I/O (goulot d'√©tranglement disque)
- Tourne depuis 45 minutes

**Actions** :
1. √âvaluer si cette requ√™te est l√©gitime ou peut √™tre interrompue
2. Si l√©gitime : optimiser (ajouter un index, d√©couper en batches)
3. Si bloquante : tuer le backend : `SELECT pg_terminate_backend(14532);`

### Sc√©nario 2 : Saturation du WAL et r√©plication qui prend du retard

**Sympt√¥me** : Les r√©plicas (standby servers) sont en retard. Le WAL s'accumule.

**Diagnostic avec PostgreSQL 18** :

```sql
-- Identifier les backends qui g√©n√®rent le plus de WAL
SELECT
    pid,
    usename,
    application_name,
    pg_size_pretty(wal_bytes) AS wal_generated,
    wal_records,
    NOW() - backend_start AS connection_age,
    NOW() - query_start AS query_age,
    state,
    LEFT(query, 100) AS query
FROM pg_stat_activity
WHERE wal_bytes > 0
ORDER BY wal_bytes DESC
LIMIT 10;
```

**Analyse** : Supposons que vous trouvez :
```
  pid  | usename |  application_name  | wal_generated | wal_records | connection_age | query_age  |  state  |           query
-------+---------+--------------------+---------------+-------------+----------------+------------+---------+--------------------------
 15234 | etl     | DataMigration      | 1.2 TB        |  234500000  | 08:30:00       | 08:29:45   | active  | INSERT INTO target_table SELECT...
```

**Conclusion** : Une migration de donn√©es g√©n√®re 1,2 TB de WAL en continu !

**Actions** :
1. **Optimiser** : Utiliser `COPY` au lieu d'`INSERT` (10√ó plus rapide, moins de WAL)
2. **Throttler** : Ajouter des pauses entre les batches
3. **Parall√©liser** : R√©partir sur plusieurs transactions pour permettre au WAL d'√™tre r√©pliqu√©
4. **Temporaire** : Augmenter `max_wal_size` pendant la migration

### Sc√©nario 3 : Ratio de cache anormalement bas

**Sympt√¥me** : Les performances se d√©gradent progressivement. Le monitoring montre un cache hit ratio global qui chute.

**Diagnostic avec PostgreSQL 18** :

```sql
SELECT
    usename,
    application_name,
    COUNT(*) AS connections,
    SUM(io_blks_read) AS total_disk_reads,
    SUM(io_blks_hit) AS total_cache_hits,
    ROUND(100.0 * SUM(io_blks_hit) / NULLIF(SUM(io_blks_hit) + SUM(io_blks_read), 0), 2) AS cache_hit_ratio,
    ROUND(AVG(io_blks_read)::numeric, 0) AS avg_disk_reads_per_conn
FROM pg_stat_activity
WHERE state != 'idle'
  AND backend_type = 'client backend'
GROUP BY usename, application_name
HAVING SUM(io_blks_read) > 0
ORDER BY cache_hit_ratio ASC;
```

**Analyse** : Supposons que vous trouvez :
```
 usename |  application_name  | connections | total_disk_reads | total_cache_hits | cache_hit_ratio | avg_disk_reads_per_conn
---------+--------------------+-------------+------------------+------------------+-----------------+------------------------
 analyst | BusinessIntel      |           8 |         45600000 |           234500 |            0.51 |              5700000
 webapp  | MyApp              |          45 |          1234000 |        123400000 |           99.01 |                27422
```

**Conclusion** : L'application `BusinessIntel` a un cache hit ratio catastrophique (0,51%) !

**Causes possibles** :
- Tables trop grandes pour tenir en cache
- Requ√™tes qui scannent toute la table
- Pas d'index appropri√©s

**Actions** :
1. Analyser les requ√™tes de `BusinessIntel` (via `pg_stat_statements`)
2. V√©rifier les plans d'ex√©cution (`EXPLAIN ANALYZE`)
3. Ajouter des index si n√©cessaire
4. Envisager d'augmenter `shared_buffers` si le serveur a de la RAM disponible

### Sc√©nario 4 : Temps d'I/O excessif

**Sympt√¥me** : Les requ√™tes sont lentes, mais le CPU est faible. Probl√®me d'I/O ?

**Diagnostic avec PostgreSQL 18** :

```sql
SELECT
    pid,
    usename,
    application_name,
    ROUND(io_read_time::numeric, 2) AS read_time_ms,
    ROUND(io_write_time::numeric, 2) AS write_time_ms,
    ROUND((io_read_time + io_write_time)::numeric, 2) AS total_io_time_ms,
    io_blks_read,
    io_blks_written,
    ROUND((io_read_time / NULLIF(io_blks_read, 0))::numeric, 2) AS avg_read_latency_ms,
    LEFT(query, 80) AS query
FROM pg_stat_activity
WHERE state = 'active'
  AND (io_read_time > 0 OR io_write_time > 0)
ORDER BY total_io_time_ms DESC
LIMIT 10;
```

**Interpr√©tation** :
- `total_io_time_ms` √©lev√© : Beaucoup de temps pass√© √† attendre les I/O
- `avg_read_latency_ms` √©lev√© : Disque lent ou satur√©
- Comparaison entre diff√©rents backends : Certains sont-ils plus affect√©s ?

**Analyse** : Si `avg_read_latency_ms > 10 ms`, vous avez un probl√®me de disque !

**Actions** :
1. V√©rifier la sant√© du disque (SMART, I/O wait syst√®me)
2. V√©rifier si d'autres processus saturent le disque
3. Envisager un upgrade disque (SSD, NVMe)
4. Optimiser les requ√™tes pour r√©duire les I/O

---

## Int√©gration avec les Outils de Monitoring

### Grafana + Prometheus

Exposez ces nouvelles m√©triques pour les visualiser dans Grafana :

```sql
-- M√©triques √† exporter pour Prometheus (postgres_exporter)

-- 1. I/O par application
SELECT
    application_name,
    SUM(io_blks_read) AS total_blks_read,
    SUM(io_blks_written) AS total_blks_written,
    SUM(io_blks_hit) AS total_blks_hit
FROM pg_stat_activity
WHERE backend_type = 'client backend'
GROUP BY application_name;

-- 2. WAL par application
SELECT
    application_name,
    SUM(wal_bytes) AS total_wal_bytes,
    SUM(wal_records) AS total_wal_records
FROM pg_stat_activity
WHERE backend_type = 'client backend'
GROUP BY application_name;

-- 3. Top 10 des backends par I/O
SELECT
    pid,
    usename,
    (io_blks_read + io_blks_written) AS total_io
FROM pg_stat_activity
WHERE state = 'active'
ORDER BY total_io DESC
LIMIT 10;
```

**Dashboards Grafana recommand√©s** :
- Graphique de l'I/O par backend au fil du temps
- Heatmap de la g√©n√©ration WAL par heure
- Alerte si un backend d√©passe 1 GB de WAL en 5 minutes

### Scripts de Monitoring Personnalis√©s

Voici un script shell pour surveiller l'I/O en continu :

```bash
#!/bin/bash
# monitor_backend_io.sh

THRESHOLD_IO_BLOCKS=100000
THRESHOLD_WAL_GB=5

while true; do
    psql -U postgres -d ma_base -t -A -c "
    SELECT
        pid,
        usename,
        io_blks_read + io_blks_written AS total_io,
        ROUND(wal_bytes / 1073741824.0, 2) AS wal_gb
    FROM pg_stat_activity
    WHERE state = 'active'
      AND (io_blks_read + io_blks_written > $THRESHOLD_IO_BLOCKS
           OR wal_bytes > $THRESHOLD_WAL_GB * 1073741824)
    " | while IFS='|' read pid user io wal; do
        if [ ! -z "$pid" ]; then
            echo "[ALERT] Backend $pid (user: $user) - IO: $io blocks, WAL: ${wal}GB"
            # Envoyer une notification (email, Slack, etc.)
        fi
    done

    sleep 30
done
```

### Int√©gration avec pg_stat_statements

Combinez les statistiques par backend avec `pg_stat_statements` pour une analyse compl√®te :

```sql
-- Trouver les requ√™tes qui g√©n√®rent le plus de WAL
SELECT
    pss.queryid,
    LEFT(pss.query, 100) AS query_preview,
    pss.calls,
    pg_size_pretty(pss.total_exec_time::bigint * 1000) AS total_time,
    -- Corr√©lation avec les backends actifs
    (SELECT SUM(wal_bytes)
     FROM pg_stat_activity
     WHERE query LIKE '%' || SUBSTRING(pss.query, 1, 30) || '%') AS estimated_wal
FROM pg_stat_statements pss
WHERE pss.calls > 100
ORDER BY estimated_wal DESC NULLS LAST
LIMIT 10;
```

---

## Cas d'Usage Pratiques

### Cas 1 : Audit de consommation de ressources par application

```sql
-- Rapport hebdomadaire : Quelle application consomme le plus de ressources ?
CREATE MATERIALIZED VIEW weekly_app_resource_usage AS
SELECT
    application_name,
    usename,
    COUNT(DISTINCT pid) AS avg_concurrent_connections,
    pg_size_pretty(SUM(io_blks_read) * 8192) AS total_disk_reads,
    pg_size_pretty(SUM(io_blks_written) * 8192) AS total_disk_writes,
    pg_size_pretty(SUM(wal_bytes)) AS total_wal_generated,
    ROUND(AVG(100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0)), 2) AS avg_cache_hit_ratio
FROM pg_stat_activity
WHERE backend_type = 'client backend'
  AND backend_start > NOW() - INTERVAL '7 days'
GROUP BY application_name, usename
ORDER BY SUM(wal_bytes) DESC;

-- Rafra√Æchir toutes les semaines
-- SELECT refresh_materialized_view('weekly_app_resource_usage');
```

**Utilit√©** :
- Facturation interne par application
- Identification des applications √† optimiser
- Planification de capacit√©

### Cas 2 : D√©tection d'anomalies en temps r√©el

```sql
-- D√©tecter les backends avec une activit√© I/O ou WAL anormale
WITH backend_stats AS (
    SELECT
        pid,
        usename,
        application_name,
        io_blks_read + io_blks_written AS total_io,
        wal_bytes,
        query
    FROM pg_stat_activity
    WHERE state = 'active'
),
stats_summary AS (
    SELECT
        AVG(total_io) AS avg_io,
        STDDEV(total_io) AS stddev_io,
        AVG(wal_bytes) AS avg_wal,
        STDDEV(wal_bytes) AS stddev_wal
    FROM backend_stats
)
SELECT
    bs.pid,
    bs.usename,
    bs.application_name,
    bs.total_io,
    ROUND((bs.total_io - ss.avg_io) / NULLIF(ss.stddev_io, 0), 2) AS io_zscore,
    pg_size_pretty(bs.wal_bytes) AS wal,
    ROUND((bs.wal_bytes - ss.avg_wal) / NULLIF(ss.stddev_wal, 0), 2) AS wal_zscore,
    LEFT(bs.query, 80) AS query
FROM backend_stats bs, stats_summary ss
WHERE (bs.total_io - ss.avg_io) / NULLIF(ss.stddev_io, 0) > 3  -- 3 √©carts-types
   OR (bs.wal_bytes - ss.avg_wal) / NULLIF(ss.stddev_wal, 0) > 3
ORDER BY io_zscore DESC;
```

**Explication** : Utilise le **Z-score** (√©cart √† la moyenne) pour d√©tecter les backends avec une activit√© statistiquement anormale.

### Cas 3 : Optimisation de l'autovacuum

```sql
-- Identifier si l'autovacuum g√©n√®re beaucoup d'I/O
SELECT
    pid,
    backend_type,
    io_blks_read,
    io_blks_written,
    io_read_time,
    io_write_time,
    NOW() - backend_start AS duration,
    LEFT(query, 80) AS query
FROM pg_stat_activity
WHERE backend_type = 'autovacuum worker'
  AND state = 'active'
ORDER BY io_blks_read + io_blks_written DESC;
```

**Utilit√©** :
- V√©rifier si l'autovacuum consomme trop d'I/O (impacte les requ√™tes utilisateurs)
- Ajuster `autovacuum_vacuum_cost_delay` si n√©cessaire
- D√©tecter les autovacuums qui tournent trop longtemps

---

## Bonnes Pratiques et Recommandations

### 1. D√©finir des seuils d'alerte

Configurez des alertes bas√©es sur des seuils adapt√©s √† votre contexte :

**Seuils sugg√©r√©s (√† ajuster selon votre charge)** :

| M√©trique | Seuil Warning | Seuil Critical | Action |
|----------|---------------|----------------|--------|
| `io_blks_read + io_blks_written` | > 500 000 | > 2 000 000 | Analyser la requ√™te |
| `wal_bytes` | > 1 GB | > 10 GB | V√©rifier la transaction |
| `cache_hit_ratio` | < 95% | < 90% | Optimiser les index |
| `io_read_time + io_write_time` | > 60 000 ms | > 300 000 ms | V√©rifier le disque |

### 2. Surveiller en continu

Cr√©ez une vue d√©di√©e pour le monitoring :

```sql
CREATE OR REPLACE VIEW v_backend_io_monitoring AS
SELECT
    pid,
    usename,
    application_name,
    state,
    NOW() - backend_start AS connection_age,
    NOW() - query_start AS query_age,
    io_blks_read,
    io_blks_written,
    io_blks_hit,
    ROUND(100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0), 2) AS cache_hit_ratio,
    pg_size_pretty(wal_bytes) AS wal_size,
    ROUND(io_read_time::numeric, 2) AS read_time_ms,
    ROUND(io_write_time::numeric, 2) AS write_time_ms,
    wait_event_type,
    wait_event,
    LEFT(query, 100) AS query_preview
FROM pg_stat_activity
WHERE backend_type = 'client backend'
  AND state != 'idle';

-- Utilisation
SELECT * FROM v_backend_io_monitoring
WHERE cache_hit_ratio < 90 OR wal_size > '1 GB'
ORDER BY io_blks_read + io_blks_written DESC;
```

### 3. Corr√©ler avec d'autres m√©triques

Ne regardez jamais les statistiques I/O/WAL de mani√®re isol√©e. Combinez avec :

```sql
-- Vue compl√®te : I/O + Locks + CPU
SELECT
    a.pid,
    a.usename,
    a.application_name,
    a.state,
    a.io_blks_read + a.io_blks_written AS total_io,
    pg_size_pretty(a.wal_bytes) AS wal,
    a.wait_event_type,
    a.wait_event,
    -- Locks bloquants
    (SELECT COUNT(*)
     FROM pg_locks l
     WHERE l.pid = a.pid AND NOT l.granted) AS blocked_locks,
    -- Locks tenus
    (SELECT COUNT(*)
     FROM pg_locks l
     WHERE l.pid = a.pid AND l.granted) AS held_locks,
    LEFT(a.query, 80) AS query
FROM pg_stat_activity a
WHERE a.state = 'active'
ORDER BY total_io DESC;
```

### 4. Archiver les statistiques historiques

Les statistiques dans `pg_stat_activity` sont en temps r√©el et √©ph√©m√®res. Pour l'analyse historique :

```sql
-- Table d'historique
CREATE TABLE backend_io_history (
    captured_at timestamp DEFAULT NOW(),
    pid integer,
    usename text,
    application_name text,
    io_blks_read bigint,
    io_blks_written bigint,
    wal_bytes bigint,
    query text
);

-- Job p√©riodique (via pg_cron ou cron syst√®me)
INSERT INTO backend_io_history (pid, usename, application_name, io_blks_read, io_blks_written, wal_bytes, query)
SELECT pid, usename, application_name, io_blks_read, io_blks_written, wal_bytes, query
FROM pg_stat_activity
WHERE state = 'active' AND (io_blks_read + io_blks_written > 1000 OR wal_bytes > 1048576);
```

### 5. Documenter les patterns normaux

Chaque application a ses propres patterns d'I/O et WAL. Documentez ce qui est "normal" :

- ETL nocturne : 100 GB WAL / nuit = normal
- Application web : < 1 MB WAL / requ√™te = normal
- Rapports analytiques : 1M blocs lus = normal

Cela vous permettra de d√©tecter plus facilement les anomalies.

---

## Limites et Consid√©rations

### Ce que ces statistiques NE montrent PAS

1. **D√©tails des fichiers** : Vous ne savez pas quels fichiers/tables sont lus/√©crits
2. **D√©composition par requ√™te** : Si un backend ex√©cute plusieurs requ√™tes, les stats sont cumulatives
3. **I/O syst√®me** : Uniquement l'I/O PostgreSQL, pas l'I/O OS global
4. **Historique** : Statistiques depuis le d√©but de la connexion, pas de breakdown temporel

### Impact sur les performances

L'activation de ces statistiques a un **impact minimal** :
- Overhead : < 2% dans la plupart des cas
- D√©j√† activ√© par d√©faut dans PostgreSQL 18
- Peut √™tre d√©sactiv√© si vraiment n√©cessaire via `track_io_timing`

### Compatibilit√©

- **Nouvelle fonctionnalit√© PostgreSQL 18** : N'existe pas dans les versions ant√©rieures
- Migration depuis une version plus ancienne : Les colonnes appara√Ætront automatiquement
- Scripts existants : Non impact√©s (nouvelles colonnes ajout√©es)

---

## Requ√™tes Pr√™tes √† l'Emploi

### Dashboard de sant√© complet

```sql
-- Vue d'ensemble : Backends actifs avec I/O et WAL
SELECT
    ROW_NUMBER() OVER (ORDER BY io_blks_read + io_blks_written DESC) AS rank,
    pid,
    usename,
    application_name,
    state,
    ROUND(EXTRACT(EPOCH FROM (NOW() - query_start)) / 60, 1) AS minutes_running,
    io_blks_read AS disk_reads,
    io_blks_written AS disk_writes,
    io_blks_hit AS cache_hits,
    ROUND(100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0), 1) AS cache_hit_pct,
    pg_size_pretty(wal_bytes) AS wal_generated,
    wait_event_type || ': ' || COALESCE(wait_event, 'none') AS wait_status,
    LEFT(query, 60) AS query_preview
FROM pg_stat_activity
WHERE state != 'idle'
  AND backend_type = 'client backend'
ORDER BY io_blks_read + io_blks_written DESC
LIMIT 20;
```

### Top 5 des gourmands en ressources

```sql
-- Les 5 backends qui consomment le plus de chaque ressource
(SELECT 'TOP I/O READS' AS category, pid, usename, application_name,
        io_blks_read AS value, LEFT(query, 50) AS query
 FROM pg_stat_activity WHERE state = 'active' ORDER BY io_blks_read DESC LIMIT 5)
UNION ALL
(SELECT 'TOP I/O WRITES', pid, usename, application_name,
        io_blks_written, LEFT(query, 50)
 FROM pg_stat_activity WHERE state = 'active' ORDER BY io_blks_written DESC LIMIT 5)
UNION ALL
(SELECT 'TOP WAL GEN', pid, usename, application_name,
        wal_bytes, LEFT(query, 50)
 FROM pg_stat_activity WHERE state = 'active' ORDER BY wal_bytes DESC LIMIT 5);
```

### Alertes automatiques

```sql
-- Backends n√©cessitant une attention imm√©diate
SELECT
    'ALERT' AS priority,
    pid,
    usename,
    application_name,
    CASE
        WHEN io_blks_read > 1000000 THEN 'Excessive disk reads (>1M blocks)'
        WHEN io_blks_written > 1000000 THEN 'Excessive disk writes (>1M blocks)'
        WHEN wal_bytes > 10737418240 THEN 'Excessive WAL generation (>10GB)'
        WHEN (100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0)) < 80 THEN 'Low cache hit ratio (<80%)'
    END AS issue,
    pg_size_pretty(wal_bytes) AS wal,
    LEFT(query, 100) AS query
FROM pg_stat_activity
WHERE state = 'active'
  AND (io_blks_read > 1000000
       OR io_blks_written > 1000000
       OR wal_bytes > 10737418240
       OR (100.0 * io_blks_hit / NULLIF(io_blks_hit + io_blks_read, 0)) < 80);
```

---

## Conclusion

Les statistiques I/O et WAL par backend dans PostgreSQL 18 repr√©sentent une **r√©volution** pour le diagnostic et l'optimisation des performances.

### Avantages cl√©s

‚úÖ **Visibilit√© granulaire** : Voir exactement quel backend consomme des ressources

‚úÖ **Diagnostic rapide** : Identifier instantan√©ment la source d'un probl√®me de performance

‚úÖ **Responsabilisation** : Savoir quelle application ou utilisateur est responsable

‚úÖ **Optimisation cibl√©e** : Concentrer les efforts d'optimisation sur les vrais coupables

‚úÖ **Monitoring proactif** : D√©tecter les probl√®mes avant qu'ils n'impactent les utilisateurs

### Transformation du workflow

**Avant PostgreSQL 18** :
1. "Le serveur est lent" üò∞
2. Analyses laborieuses pendant des heures
3. Red√©marrage en dernier recours

**Avec PostgreSQL 18** :
1. "Le serveur est lent" üîç
2. Requ√™te sur `pg_stat_activity` (30 secondes)
3. Identification du backend coupable
4. Action cibl√©e (optimisation ou interruption)
5. Probl√®me r√©solu ‚úÖ

### Points cl√©s √† retenir

1. **I/O** : Mesure les acc√®s disque (lent) vs cache (rapide)
2. **WAL** : Mesure l'intensit√© des modifications de donn√©es
3. **Backend** : Un processus = une connexion = des statistiques d√©di√©es
4. Les colonnes `io_blks_*` et `wal_*` dans `pg_stat_activity` sont vos nouveaux meilleurs amis
5. Combinez toujours avec d'autres vues (`pg_stat_statements`, `pg_locks`) pour un diagnostic complet

### Prochaines √©tapes

1. **Migrer vers PostgreSQL 18** pour b√©n√©ficier de cette fonctionnalit√©
2. **Cr√©er des vues personnalis√©es** adapt√©es √† votre contexte
3. **Int√©grer dans votre monitoring** (Grafana, scripts, alertes)
4. **Former votre √©quipe** √† l'utilisation de ces nouvelles m√©triques
5. **Documenter les seuils** propres √† vos applications

### Ressources

- [Documentation PostgreSQL 18 - Monitoring](https://www.postgresql.org/docs/18/monitoring-stats.html)
- [Release Notes PostgreSQL 18 - I/O Statistics](https://www.postgresql.org/docs/18/release-18.html)
- Blog : "Deep Dive into PostgreSQL 18 Backend I/O Statistics"

---

**üí° Astuce finale** : Cr√©ez un alias psql pour interroger rapidement ces statistiques :

```sql
-- Dans votre fichier .psqlrc
\set io_top 'SELECT pid, usename, application_name, io_blks_read + io_blks_written AS total_io, pg_size_pretty(wal_bytes) AS wal, LEFT(query, 80) FROM pg_stat_activity WHERE state = \'active\' ORDER BY total_io DESC LIMIT 10;'

-- Utilisation : \:io_top
```

**üéØ Objectif** : Faire de PostgreSQL 18 le SGBD le plus observable et diagnosticable du march√© !

‚è≠Ô∏è [Analyse des logs : Configuration et interpr√©tation (log_line_prefix, auto_explain)](/14-observabilite-et-monitoring/05-analyse-des-logs.md)
