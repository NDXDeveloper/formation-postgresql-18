ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 17.2.1. Configuration Primary/Standby - RÃ©plication Physique PostgreSQL 18

## Introduction

La rÃ©plication Primary/Standby (aussi appelÃ©e Master/Slave ou Leader/Follower) est une architecture fondamentale de haute disponibilitÃ© dans PostgreSQL. Elle permet de maintenir une ou plusieurs copies de votre base de donnÃ©es sur des serveurs diffÃ©rents, garantissant ainsi la continuitÃ© de service en cas de panne.

### Pourquoi mettre en place une rÃ©plication Primary/Standby ?

**Les objectifs principaux :**
- **Haute disponibilitÃ© (HA)** : Si le serveur primaire tombe, un serveur secondaire peut prendre le relais
- **RÃ©partition de charge en lecture** : Les serveurs standby peuvent traiter les requÃªtes de lecture (SELECT), soulageant ainsi le serveur primaire
- **Sauvegardes sans impact** : Effectuer les sauvegardes sur un standby plutÃ´t que sur le serveur primaire
- **Disaster Recovery** : Avoir une copie des donnÃ©es dans un autre datacenter ou rÃ©gion gÃ©ographique

### Concepts clÃ©s Ã  comprendre

Avant de configurer la rÃ©plication, il est essentiel de comprendre quelques concepts :

**Le serveur Primary (Primaire)** :
- C'est le serveur "maÃ®tre" qui accepte les Ã©critures (INSERT, UPDATE, DELETE)
- Il gÃ©nÃ¨re les WAL (Write-Ahead Logs) qui contiennent toutes les modifications
- Il envoie ces WAL aux serveurs standby en temps rÃ©el

**Le serveur Standby (Secondaire)** :
- C'est une copie du serveur primaire, maintenue Ã  jour en continu
- Il reÃ§oit et rejoue les WAL envoyÃ©s par le primary
- En mode par dÃ©faut, il est en lecture seule (read-only)
- Il peut devenir primary en cas de promotion (failover)

**Les WAL (Write-Ahead Logs)** :
- Fichiers journaux contenant toutes les modifications de la base de donnÃ©es
- Ã‰crits avant que les donnÃ©es ne soient rÃ©ellement modifiÃ©es sur disque
- UtilisÃ©s pour la rÃ©cupÃ©ration en cas de crash et pour la rÃ©plication
- SituÃ©s dans le rÃ©pertoire `pg_wal/` de l'installation PostgreSQL

**Le Streaming Replication** :
- MÃ©canisme par lequel les WAL sont envoyÃ©s en continu (en "streaming") du primary au standby
- Utilise une connexion rÃ©seau dÃ©diÃ©e
- Permet une latence minimale entre primary et standby (quelques millisecondes)

---

## Architecture de la RÃ©plication Physical

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Serveur Primary       â”‚
â”‚   (Lecture/Ã‰criture)    â”‚
â”‚                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Base de donnÃ©es â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚              â”‚
â”‚          â†“              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   WAL Sender    â”‚   â”‚  â† Processus qui envoie les WAL
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Streaming (TCP/IP)
             â”‚ Port 5432 (ou personnalisÃ©)
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Serveur Standby       â”‚
â”‚   (Lecture seule)       â”‚
â”‚                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  WAL Receiver   â”‚   â”‚  â† Processus qui reÃ§oit les WAL
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚            â†“            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Startup Process â”‚   â”‚  â† Rejoue les WAL
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚            â†“            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Base de donnÃ©es â”‚   â”‚  â† Copie identique du primary
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Le cycle de vie de la rÃ©plication

1. **Initialisation** : Le standby est crÃ©Ã© Ã  partir d'une copie physique (backup) du primary
2. **Connexion** : Le standby se connecte au primary via une connexion rÃ©seau authentifiÃ©e
3. **Streaming** : Le primary envoie en continu les WAL au fur et Ã  mesure de leur gÃ©nÃ©ration
4. **Replay** : Le standby rejoue ces WAL pour maintenir sa copie Ã  jour
5. **Monitoring** : Les deux serveurs Ã©changent des informations sur leur Ã©tat (lag, position WAL, etc.)

---

## PrÃ©requis et Planification

### Configuration rÃ©seau

**ConnectivitÃ© requise :**
- Le serveur standby doit pouvoir se connecter au serveur primary sur le port PostgreSQL (par dÃ©faut 5432)
- PrÃ©voir une bande passante suffisante pour le transfert des WAL (dÃ©pend du volume d'Ã©critures)
- Latence rÃ©seau faible recommandÃ©e (< 10ms pour une rÃ©plication synchrone)

**ConsidÃ©rations de sÃ©curitÃ© :**
- Utiliser SSL/TLS pour chiffrer les connexions de rÃ©plication
- Configurer des rÃ¨gles firewall strictes
- CrÃ©er un utilisateur dÃ©diÃ© Ã  la rÃ©plication avec des privilÃ¨ges minimaux

### Configuration matÃ©rielle

**Pour le serveur Standby :**
- IdÃ©alement, configuration matÃ©rielle identique ou supÃ©rieure au primary
- MÃªme version de PostgreSQL (obligatoire pour la rÃ©plication physique)
- MÃªme systÃ¨me d'exploitation et architecture (x86_64, ARM, etc.)
- Espace disque suffisant pour stocker la base de donnÃ©es + WAL

**Calcul de l'espace disque pour les WAL :**
- Les WAL s'accumulent sur le primary si le standby est indisponible
- PrÃ©voir un espace tampon : au minimum 2-3Ã— la taille de `max_wal_size`
- Pour une production critique : 50-100 Go dÃ©diÃ©s aux WAL

### Planification de la topologie

**Options courantes :**
1. **Primary â†’ Standby simple** : Architecture minimale (1 primary + 1 standby)
2. **Primary â†’ Multiple Standbys** : Un primary peut alimenter plusieurs standbys
3. **Cascading Replication** : Primary â†’ Standby1 â†’ Standby2 (rÃ©duction de charge sur le primary)

---

## Configuration du Serveur Primary

### Ã‰tape 1 : CrÃ©er un utilisateur de rÃ©plication

Le primary a besoin d'un utilisateur dÃ©diÃ© que le standby utilisera pour se connecter.

**Concept :** Un utilisateur avec le privilÃ¨ge `REPLICATION` peut initier une connexion de rÃ©plication et recevoir les WAL.

**CrÃ©ation de l'utilisateur :**
```sql
-- Se connecter en tant que superutilisateur (postgres)
CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'mot_de_passe_fort';
```

**Explications :**
- `REPLICATION` : PrivilÃ¨ge spÃ©cial autorisant la rÃ©plication
- `LOGIN` : Permet Ã  cet utilisateur de se connecter
- `PASSWORD` : DÃ©finit un mot de passe sÃ©curisÃ© (Ã  remplacer par un mot de passe fort)

**Bonnes pratiques :**
- Ne pas utiliser le compte `postgres` pour la rÃ©plication
- GÃ©nÃ©rer un mot de passe long et complexe (20+ caractÃ¨res)
- Stocker ce mot de passe de maniÃ¨re sÃ©curisÃ©e (coffre-fort de mots de passe)

### Ã‰tape 2 : Configurer postgresql.conf sur le Primary

Le fichier `postgresql.conf` contient les paramÃ¨tres de configuration de PostgreSQL. Il est gÃ©nÃ©ralement situÃ© dans le rÃ©pertoire de donnÃ©es PostgreSQL (par exemple `/var/lib/postgresql/18/main/`).

**ParamÃ¨tres essentiels Ã  configurer :**

```ini
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration WAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Niveau de dÃ©tail des WAL (requis pour la rÃ©plication)
wal_level = replica

# Taille maximale des WAL entre deux checkpoints
# Plus cette valeur est Ã©levÃ©e, moins il y a de checkpoints (mais plus de WAL Ã  rejouer en cas de crash)
max_wal_size = 2GB

# Nombre de segments WAL Ã  conserver
# Utile si le standby est temporairement dÃ©connectÃ©
wal_keep_size = 1GB


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration de la rÃ©plication
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Nombre maximum de connexions WAL sender (1 par standby + quelques extras)
# Pour 2 standbys : mettre au moins 4-5
max_wal_senders = 5

# Active l'archivage des WAL (optionnel mais recommandÃ© pour PITR)
archive_mode = on

# Commande d'archivage (exemple : copie vers un rÃ©pertoire de sauvegarde)
# Ã€ adapter selon votre infrastructure
archive_command = 'test ! -f /archive/pg_wal/%f && cp %p /archive/pg_wal/%f'

# Timeout pour l'envoi des WAL (en millisecondes)
# Si le standby ne rÃ©pond pas pendant ce dÃ©lai, la connexion est fermÃ©e
wal_sender_timeout = 60s


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration des slots de rÃ©plication (recommandÃ©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Nombre de slots de rÃ©plication (1 par standby recommandÃ©)
max_replication_slots = 5

# Permet aux standbys de signaler leur progression
# NÃ©cessaire pour le monitoring
hot_standby_feedback = on
```

**Explications dÃ©taillÃ©es des paramÃ¨tres clÃ©s :**

**`wal_level = replica`**
- `minimal` : WAL minimum (pas de rÃ©plication possible)
- `replica` : WAL suffisant pour la rÃ©plication physique (valeur standard)
- `logical` : WAL suffisant pour la rÃ©plication logique (plus verbeux)

**`max_wal_senders`**
- Chaque standby nÃ©cessite 1 wal_sender
- Ajouter 2-3 connexions supplÃ©mentaires pour les outils (pg_basebackup, monitoring)
- Formule : `nombre_de_standbys + 3`

**`wal_keep_size`**
- WAL minimum Ã  conserver sur le primary mÃªme s'ils ne sont plus nÃ©cessaires localement
- Protection contre la perte de WAL si le standby est temporairement dÃ©connectÃ©
- Valeur typique : 1-2 Go (dÃ©pend du volume d'Ã©critures)

**`archive_command`**
- Commande shell exÃ©cutÃ©e pour chaque segment WAL Ã  archiver
- `%p` : chemin complet du fichier WAL
- `%f` : nom du fichier WAL
- Le test `! -f` Ã©vite d'Ã©craser un fichier existant (sÃ©curitÃ©)

**NouveautÃ©s PostgreSQL 18 :**
Dans PostgreSQL 18, le sous-systÃ¨me I/O asynchrone amÃ©liore les performances de rÃ©plication. Si vous souhaitez l'activer :

```ini
# Activer l'I/O asynchrone (PG 18+)
io_method = async
```

### Ã‰tape 3 : Configurer pg_hba.conf sur le Primary

Le fichier `pg_hba.conf` (Host-Based Authentication) contrÃ´le qui peut se connecter Ã  PostgreSQL et comment.

**Ajouter une ligne pour autoriser la rÃ©plication :**

```
# TYPE  DATABASE        USER            ADDRESS                 METHOD

# Autoriser la rÃ©plication depuis le serveur standby
# Remplacer 192.168.1.20/32 par l'adresse IP rÃ©elle du standby
host    replication     replicator      192.168.1.20/32         scram-sha-256
```

**Explications des colonnes :**
- **TYPE** : `host` (connexion TCP/IP), `replication` (connexion de rÃ©plication)
- **DATABASE** : `replication` (pseudo-base pour les connexions de rÃ©plication)
- **USER** : `replicator` (l'utilisateur crÃ©Ã© Ã  l'Ã©tape 1)
- **ADDRESS** : Adresse IP du standby (notation CIDR)
  - `/32` : Une seule adresse IP
  - `/24` : Tout un sous-rÃ©seau (ex: 192.168.1.0/24 = 192.168.1.0 Ã  192.168.1.255)
- **METHOD** : `scram-sha-256` (mÃ©thode d'authentification moderne et sÃ©curisÃ©e)

**Pour plusieurs standbys :**
```
host    replication     replicator      192.168.1.20/32         scram-sha-256
host    replication     replicator      192.168.1.21/32         scram-sha-256
host    replication     replicator      192.168.1.22/32         scram-sha-256
```

**Ou pour un sous-rÃ©seau entier (moins sÃ©curisÃ©) :**
```
host    replication     replicator      192.168.1.0/24          scram-sha-256
```

**Note importante sur SSL/TLS (recommandÃ© en production) :**
Pour forcer l'utilisation de SSL, remplacer `host` par `hostssl` :
```
hostssl replication     replicator      192.168.1.20/32         scram-sha-256
```

### Ã‰tape 4 : CrÃ©er un slot de rÃ©plication (recommandÃ©)

Un **slot de rÃ©plication** garantit que le primary conserve les WAL nÃ©cessaires tant que le standby ne les a pas reÃ§us, mÃªme si `wal_keep_size` est dÃ©passÃ©.

**CrÃ©ation du slot :**
```sql
-- Se connecter au primary en tant que superutilisateur
SELECT pg_create_physical_replication_slot('standby_slot_1');
```

**Explications :**
- `standby_slot_1` : Nom du slot (choisir un nom descriptif, ex: `standby_paris_slot`)
- Le slot est **persistant** : il survit aux redÃ©marrages
- Le primary conservera **tous** les WAL nÃ©cessaires pour ce slot

**Avantages des slots :**
- âœ… Protection contre la perte de WAL si le standby est en panne prolongÃ©e
- âœ… Pas besoin de calculer prÃ©cisÃ©ment `wal_keep_size`
- âœ… Monitoring facile de l'Ã©tat de rÃ©plication

**InconvÃ©nients :**
- âš ï¸ Les WAL s'accumulent indÃ©finiment si le standby ne se reconnecte jamais
- âš ï¸ Peut remplir le disque du primary si on ne surveille pas
- âš ï¸ NÃ©cessite une suppression manuelle du slot si le standby est dÃ©finitivement retirÃ©

**Surveillance d'un slot :**
```sql
-- VÃ©rifier l'Ã©tat des slots
SELECT slot_name, active, restart_lsn, confirmed_flush_lsn
FROM pg_replication_slots;
```

### Ã‰tape 5 : RedÃ©marrer le serveur Primary

AprÃ¨s avoir modifiÃ© `postgresql.conf` et `pg_hba.conf`, il faut redÃ©marrer PostgreSQL :

```bash
# Sur Linux avec systemd
sudo systemctl restart postgresql

# Ou avec pg_ctl
pg_ctl -D /var/lib/postgresql/18/main restart
```

**VÃ©rification aprÃ¨s redÃ©marrage :**
```sql
-- VÃ©rifier que wal_level est bien configurÃ©
SHOW wal_level;  -- Doit retourner 'replica' ou 'logical'

-- VÃ©rifier max_wal_senders
SHOW max_wal_senders;  -- Doit Ãªtre > 0

-- VÃ©rifier que le slot existe
SELECT * FROM pg_replication_slots;
```

---

## Configuration du Serveur Standby

### Ã‰tape 1 : CrÃ©er une copie initiale du Primary (Base Backup)

Le standby doit dÃ©marrer avec une copie physique exacte du serveur primary. PostgreSQL fournit l'outil `pg_basebackup` pour cela.

**Sur le serveur Standby, exÃ©cuter :**

```bash
# ArrÃªter PostgreSQL sur le standby (si dÃ©jÃ  en cours d'exÃ©cution)
sudo systemctl stop postgresql

# Sauvegarder l'ancien rÃ©pertoire de donnÃ©es (par prÃ©caution)
sudo mv /var/lib/postgresql/18/main /var/lib/postgresql/18/main.old

# CrÃ©er une copie du primary
pg_basebackup \
    -h 192.168.1.10 \              # Adresse IP du serveur primary
    -D /var/lib/postgresql/18/main \  # RÃ©pertoire de donnÃ©es du standby
    -U replicator \                # Utilisateur de rÃ©plication
    -P \                           # Afficher la progression
    -v \                           # Mode verbose
    -R \                           # CrÃ©er automatiquement standby.signal
    -X stream \                    # Inclure les WAL nÃ©cessaires
    -C \                           # CrÃ©er automatiquement le slot (si configurÃ©)
    -S standby_slot_1              # Nom du slot de rÃ©plication
```

**Explications des options :**

- `-h 192.168.1.10` : Adresse IP du serveur primary
- `-D /var/lib/postgresql/18/main` : RÃ©pertoire de destination (sera crÃ©Ã©)
- `-U replicator` : Utilisateur PostgreSQL pour la connexion
- `-P` : Affiche une barre de progression (utile pour les grandes bases)
- `-v` : Mode verbose (affiche les dÃ©tails de l'opÃ©ration)
- `-R` : CrÃ©e automatiquement le fichier `standby.signal` (marque le serveur comme standby)
- `-X stream` : Inclut les WAL gÃ©nÃ©rÃ©s pendant le backup (Ã©vite les trous)
- `-C` : CrÃ©e le slot de rÃ©plication automatiquement (si `-S` est utilisÃ©)
- `-S standby_slot_1` : Nom du slot Ã  crÃ©er/utiliser

**Le processus va :**
1. Se connecter au primary avec l'utilisateur `replicator`
2. Initier un backup cohÃ©rent de la base de donnÃ©es
3. Copier tous les fichiers de donnÃ©es vers le standby
4. CrÃ©er automatiquement les fichiers de configuration pour le standby

**DurÃ©e estimÃ©e :**
DÃ©pend de la taille de la base et de la bande passante rÃ©seau. Pour une base de 100 Go sur un rÃ©seau Ã  1 Gbps : environ 15-20 minutes.

### Ã‰tape 2 : Configurer postgresql.conf sur le Standby

Le standby nÃ©cessite quelques paramÃ¨tres spÃ©cifiques, bien que beaucoup soient hÃ©ritÃ©s du primary via le basebackup.

**ParamÃ¨tres spÃ©cifiques au Standby :**

```ini
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration Hot Standby (lectures sur le standby)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Active les connexions en lecture seule sur le standby
hot_standby = on

# Feedback du standby vers le primary (Ã©vite les conflits de requÃªtes)
hot_standby_feedback = on

# DÃ©lai maximal d'attente pour rÃ©soudre un conflit de requÃªte
max_standby_streaming_delay = 30s


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration de la connexion au Primary
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ChaÃ®ne de connexion au serveur primary
# Format : libpq connection string
primary_conninfo = 'host=192.168.1.10 port=5432 user=replicator password=mot_de_passe_fort application_name=standby1'

# Nom du slot de rÃ©plication Ã  utiliser
primary_slot_name = 'standby_slot_1'


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration des WAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# RÃ©pertoire de restauration des WAL archivÃ©s (optionnel, pour PITR)
restore_command = 'cp /archive/pg_wal/%f %p'

# Trigger file pour promouvoir le standby en primary
promote_trigger_file = '/tmp/postgresql.trigger.5432'
```

**Explications des paramÃ¨tres clÃ©s :**

**`hot_standby = on`**
- Permet aux utilisateurs de se connecter en lecture seule au standby
- IdÃ©al pour rÃ©partir les charges de lecture (reporting, analytics)
- Si `off`, le standby est inaccessible aux connexions clients

**`hot_standby_feedback = on`**
- Le standby informe le primary des requÃªtes en cours
- Ã‰vite que le primary ne supprime des lignes encore utilisÃ©es par le standby
- Peut lÃ©gÃ¨rement augmenter le bloat sur le primary (acceptable dans la plupart des cas)

**`max_standby_streaming_delay`**
- Temps maximum qu'une requÃªte sur le standby peut bloquer le replay des WAL
- Si une requÃªte en lecture entre en conflit avec un WAL (ex: DELETE), PostgreSQL attendra
- AprÃ¨s ce dÃ©lai, la requÃªte est annulÃ©e pour permettre au standby de rattraper son retard
- Valeurs typiques : 30s Ã  300s (selon tolÃ©rance)

**`primary_conninfo`**
- ChaÃ®ne de connexion au serveur primary (format libpq)
- **host** : IP du primary
- **user** : utilisateur de rÃ©plication
- **password** : mot de passe (ou utiliser un `.pgpass` plus sÃ©curisÃ©)
- **application_name** : nom pour identifier ce standby dans les vues de monitoring

**Format alternatif avec SSL :**
```ini
primary_conninfo = 'host=192.168.1.10 port=5432 user=replicator password=mot_de_passe_fort sslmode=require application_name=standby1'
```

**`primary_slot_name`**
- Nom du slot de rÃ©plication crÃ©Ã© sur le primary
- Si configurÃ©, le standby utilisera ce slot pour garantir la conservation des WAL

**`restore_command`**
- Commande pour restaurer des WAL archivÃ©s (utilisÃ© si le streaming Ã©choue)
- Fonctionne comme un filet de sÃ©curitÃ©
- `%f` : nom du fichier WAL demandÃ©
- `%p` : chemin oÃ¹ PostgreSQL attend le fichier

### Ã‰tape 3 : CrÃ©er le fichier standby.signal

Ce fichier est un marqueur qui indique Ã  PostgreSQL que ce serveur doit dÃ©marrer en mode standby.

**CrÃ©ation :**

Si vous avez utilisÃ© l'option `-R` avec `pg_basebackup`, ce fichier est crÃ©Ã© automatiquement. Sinon :

```bash
# Sur le serveur standby
touch /var/lib/postgresql/18/main/standby.signal
```

**Ce fichier :**
- Est un fichier vide (seule sa prÃ©sence compte)
- Doit Ãªtre dans le rÃ©pertoire de donnÃ©es PostgreSQL (`PGDATA`)
- Sera automatiquement supprimÃ© lors d'une promotion en primary

### Ã‰tape 4 : Ajuster les permissions (important)

Le rÃ©pertoire de donnÃ©es doit appartenir Ã  l'utilisateur PostgreSQL :

```bash
# Sur le serveur standby
sudo chown -R postgres:postgres /var/lib/postgresql/18/main
sudo chmod 700 /var/lib/postgresql/18/main
```

**Explications :**
- PostgreSQL refuse de dÃ©marrer si les permissions sont incorrectes (sÃ©curitÃ©)
- `700` : Seul le propriÃ©taire (postgres) peut lire/Ã©crire/exÃ©cuter
- Cela protÃ¨ge les donnÃ©es contre les accÃ¨s non autorisÃ©s

### Ã‰tape 5 : DÃ©marrer le serveur Standby

```bash
# Sur le serveur standby
sudo systemctl start postgresql

# VÃ©rifier le statut
sudo systemctl status postgresql
```

**Le standby va :**
1. DÃ©tecter le fichier `standby.signal`
2. Se connecter au primary avec les paramÃ¨tres de `primary_conninfo`
3. Commencer Ã  recevoir et rejouer les WAL
4. Devenir accessible en lecture seule (si `hot_standby = on`)

---

## VÃ©rification et Monitoring de la RÃ©plication

### Sur le serveur Primary

**1. VÃ©rifier les connexions de rÃ©plication actives :**

```sql
SELECT
    pid,                          -- ID du processus wal sender
    usename,                      -- Utilisateur de rÃ©plication
    application_name,             -- Nom du standby
    client_addr,                  -- Adresse IP du standby
    state,                        -- Ã‰tat (streaming, catchup, etc.)
    sent_lsn,                     -- Position WAL envoyÃ©e
    write_lsn,                    -- Position WAL Ã©crite par le standby
    flush_lsn,                    -- Position WAL flushÃ©e par le standby
    replay_lsn,                   -- Position WAL rejouÃ©e par le standby
    sync_state                    -- Ã‰tat de synchronisation (async/sync)
FROM pg_stat_replication;
```

**RÃ©sultat attendu :**
```
 pid  | usename    | application_name | client_addr   | state     | sent_lsn  | ...
------+------------+------------------+---------------+-----------+-----------+-----
 1234 | replicator | standby1         | 192.168.1.20  | streaming | 0/3000148 | ...
```

**InterprÃ©tation :**
- **state = 'streaming'** : La rÃ©plication fonctionne correctement
- **state = 'catchup'** : Le standby est en train de rattraper un retard
- **state = 'backup'** : Un basebackup est en cours

**2. Calculer le lag (retard) de rÃ©plication :**

```sql
SELECT
    application_name,
    client_addr,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) / 1024 / 1024 AS lag_mb,
    write_lag,
    flush_lag,
    replay_lag
FROM pg_stat_replication;
```

**Explication des mÃ©triques de lag :**
- **lag_bytes** : Nombre d'octets de WAL non encore rejouÃ©s par le standby
- **write_lag** : Temps entre l'Ã©criture sur le primary et la confirmation d'Ã©criture par le standby
- **flush_lag** : Temps pour que les WAL soient flushÃ©s sur disque sur le standby
- **replay_lag** : Temps pour que les WAL soient rejouÃ©s (appliquÃ©s) sur le standby

**Valeurs acceptables :**
- **lag_bytes < 16 MB** : Excellent
- **lag_bytes < 100 MB** : Acceptable
- **lag_bytes > 1 GB** : ProblÃ¨me (rÃ©seau lent, standby sous-dimensionnÃ©, ou problÃ¨me de performance)

**3. VÃ©rifier l'Ã©tat des slots de rÃ©plication :**

```sql
SELECT
    slot_name,
    slot_type,
    active,
    restart_lsn,
    pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) AS retained_bytes
FROM pg_replication_slots;
```

**InterprÃ©tation :**
- **active = true** : Le standby est connectÃ© et utilise ce slot
- **active = false** : Le slot existe mais aucun standby n'est connectÃ© (âš ï¸ WAL s'accumulent)
- **retained_bytes** : QuantitÃ© de WAL conservÃ©s pour ce slot

### Sur le serveur Standby

**1. VÃ©rifier que le serveur est bien en mode recovery (standby) :**

```sql
SELECT pg_is_in_recovery();
```

**RÃ©sultat attendu :** `true`

Si `false`, le serveur pense Ãªtre un primary (problÃ¨me de configuration).

**2. Afficher les informations de rÃ©plication :**

```sql
SELECT
    now() - pg_last_xact_replay_timestamp() AS replication_lag_time,
    pg_last_wal_receive_lsn(),
    pg_last_wal_replay_lsn(),
    pg_last_xact_replay_timestamp();
```

**Explications :**
- **replication_lag_time** : Temps Ã©coulÃ© depuis la derniÃ¨re transaction rejouÃ©e (en secondes)
- **pg_last_wal_receive_lsn()** : DerniÃ¨re position WAL reÃ§ue du primary
- **pg_last_wal_replay_lsn()** : DerniÃ¨re position WAL rejouÃ©e localement
- **pg_last_xact_replay_timestamp()** : Timestamp de la derniÃ¨re transaction rejouÃ©e

**Valeurs attendues :**
- **replication_lag_time < 5 secondes** : Excellent
- **replication_lag_time < 30 secondes** : Acceptable
- **replication_lag_time > 60 secondes** : ProblÃ¨me

**3. Tester une connexion en lecture seule :**

```sql
-- Cette requÃªte doit fonctionner
SELECT count(*) FROM ma_table;

-- Cette requÃªte doit Ã©chouer avec une erreur "cannot execute INSERT in a read-only transaction"
INSERT INTO ma_table VALUES (1);
```

### Tests de bout en bout

**ScÃ©nario de test complet :**

**Sur le Primary :**
```sql
-- CrÃ©er une table de test
CREATE TABLE test_replication (
    id SERIAL PRIMARY KEY,
    created_at TIMESTAMP DEFAULT now(),
    message TEXT
);

-- InsÃ©rer une ligne
INSERT INTO test_replication (message) VALUES ('Test depuis le primary');
```

**Sur le Standby (attendre 1-2 secondes) :**
```sql
-- VÃ©rifier que la ligne est visible
SELECT * FROM test_replication;
```

**RÃ©sultat attendu :** La ligne insÃ©rÃ©e sur le primary doit apparaÃ®tre sur le standby quasi instantanÃ©ment (< 1 seconde).

---

## Fichiers de configuration : pgpass pour sÃ©curiser les mots de passe

PlutÃ´t que de stocker le mot de passe en clair dans `primary_conninfo`, vous pouvez utiliser un fichier `.pgpass`.

**Sur le serveur Standby, crÃ©er `/var/lib/postgresql/.pgpass` :**

```
# Format: hostname:port:database:username:password
192.168.1.10:5432:replication:replicator:mot_de_passe_fort
```

**Configurer les permissions (obligatoire) :**
```bash
chmod 600 /var/lib/postgresql/.pgpass
chown postgres:postgres /var/lib/postgresql/.pgpass
```

**Modifier `primary_conninfo` pour ne plus inclure le mot de passe :**
```ini
primary_conninfo = 'host=192.168.1.10 port=5432 user=replicator application_name=standby1'
```

PostgreSQL lira automatiquement le mot de passe depuis `.pgpass`.

---

## ProblÃ¨mes courants et dÃ©pannage

### ProblÃ¨me 1 : "FATAL: no pg_hba.conf entry for replication"

**SymptÃ´me :** Le standby ne peut pas se connecter au primary.

**Cause :** Le fichier `pg_hba.conf` du primary n'autorise pas la connexion de rÃ©plication depuis l'IP du standby.

**Solution :**
1. VÃ©rifier l'adresse IP rÃ©elle du standby : `ip addr show` ou `hostname -I`
2. Ajouter une ligne dans `pg_hba.conf` du primary :
   ```
   host    replication     replicator      <IP_STANDBY>/32         scram-sha-256
   ```
3. Recharger la configuration : `pg_ctl reload` ou `systemctl reload postgresql`

### ProblÃ¨me 2 : "FATAL: could not connect to the primary server"

**SymptÃ´me :** Le standby ne peut pas joindre le primary via le rÃ©seau.

**Causes possibles :**
- Firewall bloquant le port 5432
- Adresse IP incorrecte dans `primary_conninfo`
- Le primary n'est pas dÃ©marrÃ©
- ProblÃ¨me DNS

**Solutions :**
1. Tester la connectivitÃ© rÃ©seau : `telnet 192.168.1.10 5432` ou `nc -zv 192.168.1.10 5432`
2. VÃ©rifier les rÃ¨gles firewall :
   ```bash
   # Sur le primary
   sudo firewall-cmd --list-all  # (Fedora/CentOS)
   sudo ufw status               # (Ubuntu/Debian)
   ```
3. Autoriser le port 5432 :
   ```bash
   # Firewalld (CentOS/Fedora)
   sudo firewall-cmd --permanent --add-service=postgresql
   sudo firewall-cmd --reload

   # UFW (Ubuntu/Debian)
   sudo ufw allow from 192.168.1.20 to any port 5432
   ```

### ProblÃ¨me 3 : Lag de rÃ©plication Ã©levÃ© (> 100 MB)

**SymptÃ´me :** Le standby accumule un retard important par rapport au primary.

**Causes possibles :**
- RÃ©seau lent ou saturÃ©
- Standby sous-dimensionnÃ© (CPU, RAM, I/O)
- Volume d'Ã©critures trÃ¨s Ã©levÃ© sur le primary
- Le standby est occupÃ© par des requÃªtes en lecture lourdes

**Diagnostic :**
```sql
-- Sur le primary
SELECT
    application_name,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) / 1024 / 1024 AS lag_mb,
    write_lag,
    flush_lag,
    replay_lag
FROM pg_stat_replication;
```

**Solutions :**
1. **RÃ©seau :** VÃ©rifier la bande passante et la latence
   ```bash
   # Tester la bande passante avec iperf3
   # Sur le standby :
   iperf3 -s
   # Sur le primary :
   iperf3 -c 192.168.1.20
   ```

2. **Standby sous-dimensionnÃ© :**
   - Augmenter `max_wal_size` sur le primary
   - Augmenter `maintenance_work_mem` sur le standby
   - AmÃ©liorer les I/O disques (SSD, RAID)

3. **RequÃªtes en lecture sur le standby :**
   - Augmenter `max_standby_streaming_delay`
   - CrÃ©er un deuxiÃ¨me standby dÃ©diÃ© aux lectures
   - Utiliser un pool de connexion pour limiter les connexions

### ProblÃ¨me 4 : "requested WAL segment has already been removed"

**SymptÃ´me :** Le standby ne peut pas rejoindre le primary car les WAL nÃ©cessaires ont Ã©tÃ© supprimÃ©s.

**Cause :** Le standby Ã©tait dÃ©connectÃ© trop longtemps et les WAL ont Ã©tÃ© recyclÃ©s sur le primary.

**Solutions :**

**Option 1 : Utiliser un slot de rÃ©plication (prÃ©vention)**
```sql
-- Sur le primary
SELECT pg_create_physical_replication_slot('standby_slot_1');
```

**Option 2 : Reconstruire le standby**
Si le problÃ¨me est dÃ©jÃ  arrivÃ©, il faut refaire un `pg_basebackup` complet.

**Option 3 : Restaurer depuis l'archive WAL (si configurÃ©)**
Si vous avez configurÃ© `archive_command` et `restore_command`, le standby peut rÃ©cupÃ©rer les WAL manquants depuis l'archive.

### ProblÃ¨me 5 : Le slot de rÃ©plication accumule des WAL (disque plein)

**SymptÃ´me :** Le rÃ©pertoire `pg_wal/` sur le primary grandit indÃ©finiment.

**Cause :** Un slot de rÃ©plication existe mais le standby n'est pas connectÃ© (ou ne progresse pas).

**Diagnostic :**
```sql
SELECT
    slot_name,
    active,
    pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) / 1024 / 1024 AS wal_retained_mb
FROM pg_replication_slots;
```

Si `active = false` et `wal_retained_mb` est Ã©levÃ© (> 10 GB), c'est un problÃ¨me.

**Solution temporaire :** Supprimer le slot (attention : cela empÃªchera le standby de se reconnecter)
```sql
SELECT pg_drop_replication_slot('standby_slot_1');
```

**Solution permanente :**
1. Diagnostiquer pourquoi le standby ne se connecte pas
2. Reconstruire le standby avec un nouveau basebackup
3. RecrÃ©er le slot

---

## Bonnes pratiques de production

### 1. Monitoring et alerting

**MÃ©triques critiques Ã  surveiller :**
- **Lag de rÃ©plication** (objectif : < 10 secondes)
- **Ã‰tat de connexion** (pg_stat_replication : state = 'streaming')
- **Utilisation disque des WAL** (pg_wal/ sur le primary)
- **Ã‰tat des slots** (active = true)

**Outils recommandÃ©s :**
- **pg_stat_statements** : Extension de monitoring des requÃªtes
- **Prometheus + postgres_exporter** : Collecte de mÃ©triques
- **Grafana** : Visualisation de tableaux de bord
- **pgBadger** : Analyse de logs
- **Check_postgres** : Scripts Nagios/Icinga pour monitoring

**Alertes Ã  configurer :**
- Lag de rÃ©plication > 30 secondes
- Ã‰tat de connexion != 'streaming' pendant > 5 minutes
- Disque WAL > 80% de capacitÃ©
- Slot de rÃ©plication inactif

### 2. Sauvegardes rÃ©guliÃ¨res

La rÃ©plication n'est **pas** une sauvegarde !

**Raisons :**
- Une suppression accidentelle (DROP TABLE) sera rÃ©pliquÃ©e instantanÃ©ment sur tous les standbys
- Corruption de donnÃ©es sera propagÃ©e
- Pas de point de restauration dans le temps (sauf avec PITR)

**StratÃ©gie recommandÃ©e :**
- Sauvegardes logiques quotidiennes (pg_dump)
- Sauvegardes physiques hebdomadaires (pg_basebackup)
- Archivage des WAL (pour PITR)
- Tests de restauration mensuels
- Stockage hors site (autre rÃ©gion, cloud)

### 3. Tests de failover

Planifier des tests rÃ©guliers de promotion du standby :

**ProcÃ©dure de test (en environnement non-production) :**
1. ArrÃªter le primary : `systemctl stop postgresql`
2. Promouvoir le standby : `pg_ctl promote -D /var/lib/postgresql/18/main`
3. VÃ©rifier que le standby accepte les Ã©critures
4. Reconfigurer l'ancien primary en nouveau standby
5. Documenter les problÃ¨mes rencontrÃ©s

**FrÃ©quence recommandÃ©e :** Trimestrielle (tous les 3 mois)

### 4. Documentation

Maintenir une documentation Ã  jour :
- Architecture rÃ©seau (IPs, ports, DNS)
- ProcÃ©dures de failover
- Contacts d'escalade
- Runbooks pour incidents courants
- Historique des changements

### 5. SÃ©curitÃ©

**Checkliste de sÃ©curitÃ© :**
- âœ… Utiliser SSL/TLS pour les connexions de rÃ©plication
- âœ… Authentification SCRAM-SHA-256 (pas MD5)
- âœ… Firewall limitant les IPs autorisÃ©es
- âœ… Mots de passe forts (20+ caractÃ¨res)
- âœ… Fichier .pgpass avec permissions 600
- âœ… Principe du moindre privilÃ¨ge (utilisateur dÃ©diÃ© Ã  la rÃ©plication)
- âœ… Logs d'audit activÃ©s
- âœ… Surveillance des tentatives de connexion Ã©chouÃ©es

### 6. Dimensionnement

**RÃ¨gles gÃ©nÃ©rales :**
- **Standby = Primary** (configuration matÃ©rielle identique)
- **Bande passante rÃ©seau** : Au moins 10 Mbps dÃ©diÃ©, idÃ©alement 100 Mbps+
- **Latence rÃ©seau** : < 10ms pour rÃ©plication synchrone, < 50ms pour asynchrone
- **Disque WAL** : 50-100 Go sur le primary (sÃ©curitÃ©)

---

## RÃ©sumÃ© : Checklist de mise en place

### Sur le Primary

- [ ] CrÃ©er l'utilisateur de rÃ©plication : `CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD '...'`
- [ ] Configurer `postgresql.conf` :
  - [ ] `wal_level = replica`
  - [ ] `max_wal_senders = 5`
  - [ ] `wal_keep_size = 1GB`
  - [ ] `max_replication_slots = 5`
- [ ] Configurer `pg_hba.conf` : ajouter ligne pour connexion de rÃ©plication
- [ ] CrÃ©er un slot de rÃ©plication : `SELECT pg_create_physical_replication_slot('standby_slot_1')`
- [ ] RedÃ©marrer PostgreSQL : `systemctl restart postgresql`
- [ ] VÃ©rifier la configuration : `SHOW wal_level;`

### Sur le Standby

- [ ] ArrÃªter PostgreSQL (si en cours) : `systemctl stop postgresql`
- [ ] Effectuer un basebackup : `pg_basebackup -h <PRIMARY_IP> -D <DATA_DIR> -U replicator -P -v -R -X stream -S standby_slot_1`
- [ ] Configurer `postgresql.conf` :
  - [ ] `hot_standby = on`
  - [ ] `primary_conninfo = '...'`
  - [ ] `primary_slot_name = 'standby_slot_1'`
- [ ] VÃ©rifier que `standby.signal` existe
- [ ] Ajuster les permissions : `chown -R postgres:postgres <DATA_DIR> && chmod 700 <DATA_DIR>`
- [ ] DÃ©marrer PostgreSQL : `systemctl start postgresql`
- [ ] VÃ©rifier : `SELECT pg_is_in_recovery();` (doit retourner `true`)

### VÃ©rifications finales

- [ ] Sur le primary : `SELECT * FROM pg_stat_replication;` (doit montrer le standby connectÃ©)
- [ ] Sur le primary : InsÃ©rer une ligne de test
- [ ] Sur le standby : VÃ©rifier que la ligne apparaÃ®t
- [ ] VÃ©rifier le lag : `SELECT pg_wal_lsn_diff(sent_lsn, replay_lsn) FROM pg_stat_replication;`
- [ ] Configurer le monitoring et les alertes
- [ ] Documenter l'architecture

---

## Conclusion

La configuration Primary/Standby est la pierre angulaire de la haute disponibilitÃ© dans PostgreSQL. Cette architecture offre :

- âœ… **Redondance** : Protection contre les pannes matÃ©rielles
- âœ… **RÃ©partition de charge** : Lectures sur le standby
- âœ… **Reprise rapide** : Promotion en quelques secondes
- âœ… **FlexibilitÃ©** : Supporte plusieurs standbys et cascading replication

**Points clÃ©s Ã  retenir :**
- La rÃ©plication physique copie les WAL (Write-Ahead Logs) du primary au standby
- Le standby est une copie bit-Ã -bit du primary, maintenue Ã  jour en temps rÃ©el
- Les slots de rÃ©plication garantissent la conservation des WAL
- Le monitoring du lag est crucial pour dÃ©tecter les problÃ¨mes
- La rÃ©plication n'est pas une sauvegarde : continuez Ã  faire des backups !

**Prochaines Ã©tapes :**
- Configurer une rÃ©plication synchrone (17.2.2)
- Mettre en place le cascading replication (17.2.3)
- Automatiser le failover avec Patroni
- Explorer la rÃ©plication logique pour des cas d'usage spÃ©cifiques

La maÃ®trise de cette architecture est essentielle pour tout administrateur ou dÃ©veloppeur PostgreSQL travaillant sur des systÃ¨mes critiques en production.

---

**Ressources complÃ©mentaires :**
- Documentation officielle PostgreSQL : [High Availability, Load Balancing, and Replication](https://www.postgresql.org/docs/18/high-availability.html)
- Wiki PostgreSQL : [Replication, Clustering, and Connection Pooling](https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling)
- Blog 2ndQuadrant : [Understanding PostgreSQL Replication](https://www.2ndquadrant.com/en/blog/)

---


â­ï¸ [Synchronous vs Asynchronous](/17-haute-disponibilite-et-replication/02.2-synchronous-vs-asynchronous.md)
