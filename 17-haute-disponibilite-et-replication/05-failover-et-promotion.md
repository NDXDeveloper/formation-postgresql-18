ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 17.5. Failover et Promotion : Introduction

## Vue d'Ensemble

Dans une architecture de haute disponibilitÃ© PostgreSQL, le **failover** et la **promotion** sont des mÃ©canismes essentiels qui permettent Ã  votre systÃ¨me de continuer Ã  fonctionner mÃªme lorsque le serveur principal (Primary) rencontre un problÃ¨me.

Cette section explore les diffÃ©rentes stratÃ©gies pour gÃ©rer ces situations critiques, du plus simple (intervention manuelle) au plus sophistiquÃ© (automatisation complÃ¨te avec consensus distribuÃ©).

---

## Qu'est-ce que le Failover et la Promotion ?

### DÃ©finitions

#### Promotion

La **promotion** est l'action de transformer un serveur Standby (rÃ©plica en lecture seule) en serveur Primary (serveur principal acceptant les Ã©critures).

**Analogie** : Si le capitaine d'une Ã©quipe est blessÃ©, un autre joueur devient le nouveau capitaine.

**Techniquement** :
- Le Standby arrÃªte de recevoir les WAL (Write-Ahead Logs) du Primary
- Il termine de rejouer tous les WAL en attente
- Il passe en mode "lecture-Ã©criture"
- Il devient le nouveau Primary et commence Ã  gÃ©nÃ©rer ses propres WAL

#### Failover

Le **failover** est le processus complet de basculement depuis un Primary dÃ©faillant vers un Standby promu.

**Le failover inclut** :
1. DÃ©tection de la panne du Primary
2. SÃ©lection d'un Standby appropriÃ©
3. Promotion de ce Standby en nouveau Primary
4. Reconfiguration des autres Standby pour suivre le nouveau Primary
5. Redirection des applications vers le nouveau Primary

**Analogie** : C'est comme un plan de succession dans une entreprise. Si le PDG dÃ©missionne soudainement, il faut non seulement nommer un nouveau PDG (promotion), mais aussi rÃ©organiser toute l'entreprise autour de ce nouveau leader (failover).

### Promotion vs Failover : Quelle DiffÃ©rence ?

| Aspect | Promotion | Failover |
|--------|-----------|----------|
| **PortÃ©e** | Une seule action | Processus complet |
| **Acteur** | Le Standby qui devient Primary | Tout le cluster |
| **Commande** | `pg_ctl promote` | Orchestration complexe |
| **DurÃ©e** | Quelques secondes | Quelques secondes Ã  minutes |

**En rÃ©sumÃ©** :
- La **promotion** est l'acte technique de transformer un Standby
- Le **failover** est le processus organisationnel complet

---

## Pourquoi le Failover est-il Crucial ?

### Les Pannes Sont InÃ©vitables

En production, les pannes ne sont pas une question de "si", mais de "quand" :

**Causes de panne courantes** :
- ğŸ’¥ **Panne matÃ©rielle** : Disque dur dÃ©faillant, RAM dÃ©fectueuse, alimentation coupÃ©e
- ğŸ”¥ **Incident datacenter** : Incendie, inondation, coupure Ã©lectrique
- ğŸŒ **ProblÃ¨me rÃ©seau** : CÃ¢ble sectionnÃ©, routeur en panne, DDoS
- ğŸ› **Bug logiciel** : Corruption de donnÃ©es, bug PostgreSQL (rare), bug du kernel
- ğŸ‘¤ **Erreur humaine** : Suppression accidentelle, mauvaise commande, mauvaise configuration
- ğŸ”„ **Maintenance** : Mise Ã  jour OS, migration hardware, patch de sÃ©curitÃ©

### Impact d'une Panne sans HA

Sans mÃ©canisme de failover, une panne du Primary signifie :

```
Primary tombe en panne Ã  10h00
         â†“
Applications ne peuvent plus Ã©crire
         â†“
Business en pause
         â†“
Ã‰quipe intervient manuellement
         â†“
Promotion manuelle d'un Standby (10-30 minutes)
         â†“
Reconfiguration des applications
         â†“
Retour Ã  la normale Ã  10h45

â†’ Perte de 45 minutes de productivitÃ©
â†’ Perte potentielle de transactions
â†’ Impact client majeur
```

### Objectifs du Failover

Un bon systÃ¨me de failover vise Ã  :

1. **Minimiser le RTO** (Recovery Time Objective) : Temps d'indisponibilitÃ©
2. **Minimiser le RPO** (Recovery Point Objective) : Perte de donnÃ©es
3. **Ã‰viter le split-brain** : Deux Primary actifs simultanÃ©ment
4. **Automatiser** : RÃ©duire l'intervention humaine
5. **Assurer la cohÃ©rence** : Garantir l'intÃ©gritÃ© des donnÃ©es

---

## RTO et RPO : Comprendre les Objectifs

### RTO (Recovery Time Objective)

Le **RTO** est le temps maximum acceptable d'indisponibilitÃ© de votre systÃ¨me.

**Question** : "Combien de temps mon application peut-elle Ãªtre en panne ?"

**Exemples** :
- **Site e-commerce** : RTO = 1-2 minutes (chaque minute = perte de revenus)
- **Application interne** : RTO = 15-30 minutes (acceptable pendant heures de bureau)
- **Service financier** : RTO = 30 secondes (critique)
- **Blog personnel** : RTO = 1 heure (moins critique)

**Impact du RTO sur l'architecture** :

| RTO Cible | Solution RecommandÃ©e | CoÃ»t |
|-----------|---------------------|------|
| < 30 secondes | Patroni + etcd + Fencing automatique | ğŸ’°ğŸ’°ğŸ’°ğŸ’° Ã‰levÃ© |
| 1-2 minutes | Patroni ou Repmgr avec failover automatique | ğŸ’°ğŸ’°ğŸ’° Moyen-Ã©levÃ© |
| 5-10 minutes | Repmgr avec intervention humaine rapide | ğŸ’°ğŸ’° Moyen |
| 15-30 minutes | Promotion manuelle avec procÃ©dure documentÃ©e | ğŸ’° Faible |

### RPO (Recovery Point Objective)

Le **RPO** est la quantitÃ© maximale de donnÃ©es que vous pouvez vous permettre de perdre.

**Question** : "Combien de transactions puis-je perdre en cas de panne ?"

**Exemples** :
- **Banque** : RPO = 0 (perte de transactions = catastrophe)
- **RÃ©seau social** : RPO = 1-5 minutes (quelques posts perdus = acceptable)
- **Application de logs** : RPO = 15 minutes (logs rÃ©cents perdus = acceptable)
- **Analytics** : RPO = 1 heure (donnÃ©es agrÃ©gÃ©es = recalculables)

**Impact du RPO sur la rÃ©plication** :

| RPO Cible | Mode de RÃ©plication | Trade-off |
|-----------|---------------------|-----------|
| 0 (zÃ©ro perte) | RÃ©plication **synchrone** | âš ï¸ Performances rÃ©duites |
| < 1 minute | RÃ©plication **asynchrone** avec Standby proche | âš ï¸ Perte de quelques transactions |
| 5-15 minutes | RÃ©plication **asynchrone** standard | âœ… Meilleures performances |

### Visualisation RTO vs RPO

```
                Panne du Primary
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚
        â–¼                             â–¼
    DerniÃ¨re                      Retour
    transaction                  en ligne
    sauvegardÃ©e                 du service
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€RPOâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€RTOâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RPO = DonnÃ©es perdues (axe temporel)
RTO = DurÃ©e d'indisponibilitÃ© (axe temporel)
```

**Objectif idÃ©al** : RTO et RPO les plus courts possibles

**RÃ©alitÃ©** : Trade-off entre :
- CoÃ»t (infrastructure, complexitÃ©)
- Performance (rÃ©plication synchrone = plus lent)
- Risque (automatisation = risque de faux positifs)

---

## Le ProblÃ¨me du Split-Brain

### Qu'est-ce que le Split-Brain ?

Le **split-brain** (cerveau divisÃ©) est une situation catastrophique oÃ¹ **deux serveurs PostgreSQL se considÃ¨rent tous les deux comme Primary** et acceptent des Ã©critures simultanÃ©ment.

**Analogie** : Imaginez une entreprise avec deux PDG qui prennent des dÃ©cisions contradictoires. Le chaos est garanti.

### Comment le Split-Brain Se Produit-il ?

**ScÃ©nario classique** :

```
Ã‰tat Initial :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Standby     â”‚
â”‚  (postgres1) â”‚         â”‚  (postgres2) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰tape 1 : Perte de connexion rÃ©seau
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    âœ—    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary     â”‚         â”‚  Standby     â”‚
â”‚  (postgres1) â”‚         â”‚  (postgres2) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                        â–²
     â”‚                        â”‚
"Je suis toujours       "Je ne vois plus
 le Primary"             le Primary"

Ã‰tape 2 : Promotion du Standby (sans vÃ©rification)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    âœ—    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary     â”‚         â”‚  Primary     â”‚ ğŸ’¥
â”‚  (postgres1) â”‚         â”‚  (postgres2) â”‚ ğŸ’¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                        â–²
     â”‚                        â”‚
Ã‰crit dans               Ã‰crit dans
Timeline 1               Timeline 2

â†’ SPLIT-BRAIN !
â†’ Deux Primary actifs
â†’ Divergence des donnÃ©es
â†’ Corruption garantie
```

### ConsÃ©quences du Split-Brain

**Impact immÃ©diat** :
- ğŸ’¥ Deux copies de donnÃ©es qui divergent
- ğŸ’¥ Impossible de les rÃ©concilier automatiquement
- ğŸ’¥ Perte de donnÃ©es garantie
- ğŸ’¥ Corruption potentielle

**Impact Ã  long terme** :
- ğŸš¨ Restauration complexe (heures/jours)
- ğŸš¨ Perte de confiance des utilisateurs
- ğŸš¨ Analyse forensique nÃ©cessaire
- ğŸš¨ Possible perte dÃ©finitive de donnÃ©es

### Comment Ã‰viter le Split-Brain ?

**MÃ©canisme 1 : Fencing (Isolation)**

Le **fencing** consiste Ã  isoler physiquement l'ancien Primary pour garantir qu'il ne peut plus accepter d'Ã©critures.

**Techniques de fencing** :
- **STONITH** (Shoot The Other Node In The Head) : Ã‰teindre physiquement le serveur
- **Isolation rÃ©seau** : Couper la connexion rÃ©seau
- **PostgreSQL pause** : Mettre PostgreSQL en pause automatiquement

**Exemple avec Patroni** :
```
Patroni dÃ©tecte la perte du leader lock
       â†“
Patroni met automatiquement PostgreSQL en pause
       â†“
PostgreSQL ne peut plus accepter d'Ã©critures
       â†“
Pas de split-brain possible
```

**MÃ©canisme 2 : Consensus DistribuÃ©**

Utiliser un systÃ¨me de consensus (etcd, Consul) qui garantit qu'**un seul serveur Ã  la fois** peut dÃ©tenir le "leader lock".

**Exemple avec Patroni + etcd** :
```
3 nÅ“uds PostgreSQL + 3 nÅ“uds etcd

Pour devenir Primary, il faut :
1. Obtenir le leader lock dans etcd
2. Seul 1 nÅ“ud peut dÃ©tenir ce lock
3. Le lock a une durÃ©e de vie (TTL = 30s)
4. Si le Primary ne renouvelle pas le lock â†’ il expire
5. Un autre nÅ“ud peut alors l'obtenir

â†’ Impossible d'avoir 2 Primary (garanti par etcd)
```

**MÃ©canisme 3 : Witness Node (Arbitre)**

Utiliser un **serveur tÃ©moin** qui sert d'arbitre en cas de doute.

**Exemple avec Repmgr** :
```
Standby ne voit plus le Primary
       â†“
Standby demande au Witness : "Est-ce que tu vois le Primary ?"
       â†“
Si Witness dit "Non" â†’ C'est une vraie panne â†’ OK pour promouvoir
Si Witness dit "Oui" â†’ C'est juste un problÃ¨me rÃ©seau local â†’ NE PAS promouvoir
```

---

## Types de Failover

### 1. Failover Non PlanifiÃ© (Unplanned Failover)

**Contexte** : Le Primary tombe en panne de maniÃ¨re inattendue.

**CaractÃ©ristiques** :
- âš ï¸ Aucune prÃ©paration
- âš ï¸ Risque de perte de donnÃ©es (RPO > 0)
- âš ï¸ Urgence maximum
- âš ï¸ Stress Ã©levÃ© pour l'Ã©quipe

**DÃ©clencheurs** :
- Panne matÃ©rielle soudaine
- Crash du systÃ¨me
- Corruption de donnÃ©es
- Incident datacenter

**Objectif** : Minimiser le RTO (revenir en ligne le plus vite possible)

### 2. Switchover PlanifiÃ© (Planned Switchover)

**Contexte** : Vous dÃ©cidez volontairement de basculer vers un Standby.

**CaractÃ©ristiques** :
- âœ… Maintenance planifiÃ©e
- âœ… Aucune perte de donnÃ©es (RPO = 0)
- âœ… Processus contrÃ´lÃ©
- âœ… Downtime minimal

**Cas d'usage** :
- Mise Ã  jour de l'OS du Primary
- Migration hardware
- Changement de datacenter
- Test de la procÃ©dure de failover

**Processus** :
```
1. Annoncer une fenÃªtre de maintenance
2. ArrÃªter proprement le Primary (pg_ctl stop -m fast)
3. Attendre que le Standby soit 100% Ã  jour
4. Promouvoir le Standby
5. Reconfigurer les applications
6. (Optionnel) Transformer l'ancien Primary en Standby
```

**Avantage** : RTO minimal (quelques secondes) et RPO = 0

---

## Les Trois Approches de Failover

Cette section prÃ©sente trois approches pour gÃ©rer le failover, de la plus simple Ã  la plus sophistiquÃ©e.

### Approche 1 : Promotion Manuelle (Section 17.5.1)

**Principe** : Un administrateur dÃ©tecte la panne et lance manuellement la promotion.

**Outil** : `pg_ctl promote`

**Avantages** :
- âœ… SimplicitÃ© maximale
- âœ… Pas d'infrastructure additionnelle
- âœ… ContrÃ´le total
- âœ… Pas de faux positifs

**InconvÃ©nients** :
- âŒ Intervention humaine requise 24/7
- âŒ RTO Ã©levÃ© (plusieurs minutes minimum)
- âŒ Risque d'erreur humaine
- âŒ Pas de protection split-brain automatique

**Cas d'usage** :
- Environnements de dÃ©veloppement/test
- Switchover planifiÃ©
- Clusters simples avec Ã©quipe disponible
- Budget trÃ¨s limitÃ©

**Architecture** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Standby     â”‚
â”‚              â”‚         â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
                              â”‚
                         Administrateur
                       (exÃ©cute pg_ctl promote)
```

### Approche 2 : Automatisation ComplÃ¨te avec Patroni (Section 17.5.2)

**Principe** : Un systÃ¨me de consensus (etcd/Consul) coordonne automatiquement le failover.

**Outil** : Patroni + etcd/Consul/ZooKeeper

**Avantages** :
- âœ… Failover automatique (15-30 secondes)
- âœ… Protection split-brain excellente
- âœ… Fencing automatique
- âœ… Pas d'intervention humaine nÃ©cessaire
- âœ… IdÃ©al pour production critique

**InconvÃ©nients** :
- âŒ ComplexitÃ© Ã©levÃ©e
- âŒ Infrastructure additionnelle (etcd/Consul)
- âŒ Courbe d'apprentissage importante
- âŒ CoÃ»t d'exploitation plus Ã©levÃ©

**Cas d'usage** :
- Production critique (99.99%+ uptime)
- Clusters complexes (5+ nÅ“uds)
- DÃ©ploiements Kubernetes
- Applications financiÃ¨res, e-commerce

**Architecture** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SystÃ¨me de Consensus (etcd/Consul)    â”‚
â”‚   "Un seul Primary Ã  la fois"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–²
                  â”‚ Leader Lock
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Patroni 1â”‚ â”‚ Patroni 2â”‚ â”‚ Patroni 3â”‚
â”‚          â”‚ â”‚          â”‚ â”‚          â”‚
â”‚ Primary  â”‚ â”‚ Standby  â”‚ â”‚ Standby  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Approche 3 : Solution IntermÃ©diaire avec Repmgr (Section 17.5.3)

**Principe** : Gestion simplifiÃ©e avec failover optionnellement automatique, sans systÃ¨me de consensus externe.

**Outil** : Repmgr + repmgrd

**Avantages** :
- âœ… SimplicitÃ© relative
- âœ… Pas d'infrastructure externe obligatoire
- âœ… Failover automatique possible (avec witness)
- âœ… Courbe d'apprentissage courte
- âœ… Excellent pour cloner des Standby

**InconvÃ©nients** :
- âš ï¸ Protection split-brain moins robuste
- âš ï¸ Pas de fencing automatique
- âš ï¸ RTO un peu plus long (30-60 secondes)
- âš ï¸ NÃ©cessite un witness pour Ãªtre fiable

**Cas d'usage** :
- PME et startups
- Clusters de petite Ã  moyenne taille (2-5 nÅ“uds)
- Ã‰quipes privilÃ©giant la simplicitÃ©
- Budget moyen

**Architecture** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  repmgrd 1   â”‚         â”‚  repmgrd 2   â”‚
â”‚      â”‚       â”‚         â”‚      â”‚       â”‚
â”‚      â–¼       â”‚         â”‚      â–¼       â”‚
â”‚  Primary     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Standby     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Witness    â”‚ â† Arbitre
          â”‚   (optionnel)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison des Trois Approches

| CritÃ¨re | Manuelle (17.5.1) | Patroni (17.5.2) | Repmgr (17.5.3) |
|---------|-------------------|------------------|-----------------|
| **ComplexitÃ©** | â­ TrÃ¨s simple | â­â­â­â­â­ Complexe | â­â­â­ Moyenne |
| **Infrastructure** | Aucune | etcd/Consul/ZK | Aucune (+ witness) |
| **RTO** | 5-30 minutes | 15-30 secondes | 30-60 secondes |
| **Failover auto** | âŒ | âœ… | âš ï¸ Optionnel |
| **Split-brain protection** | âŒ | âœ…âœ…âœ… | âš ï¸ Partielle |
| **Fencing auto** | âŒ | âœ… | âŒ |
| **CoÃ»t** | ğŸ’° TrÃ¨s faible | ğŸ’°ğŸ’°ğŸ’°ğŸ’° Ã‰levÃ© | ğŸ’°ğŸ’° Moyen |
| **Maintenance** | Faible | Moyenne | Faible |
| **Production critique** | âŒ | âœ… | âš ï¸ PME |
| **IdÃ©al pour** | Dev/Test | Entreprise | Petits clusters |

---

## Facteurs de DÃ©cision : Quelle Approche Choisir ?

### Matrice de DÃ©cision

**Utilisez la promotion MANUELLE (17.5.1) si** :
- ğŸ“Œ Vous Ãªtes en environnement de dÃ©veloppement ou test
- ğŸ“Œ Votre RTO acceptable est > 15 minutes
- ğŸ“Œ Vous avez une Ã©quipe disponible 24/7
- ğŸ“Œ Votre budget est trÃ¨s limitÃ©
- ğŸ“Œ Vous voulez comprendre les mÃ©canismes de base

**Utilisez PATRONI (17.5.2) si** :
- ğŸ“Œ Votre RTO doit Ãªtre < 1 minute
- ğŸ“Œ Vous avez des exigences de uptime critiques (99.99%+)
- ğŸ“Œ Vous gÃ©rez un cluster complexe (5+ nÅ“uds)
- ğŸ“Œ Vous dÃ©ployez dans Kubernetes
- ğŸ“Œ Vous avez les ressources pour gÃ©rer etcd/Consul
- ğŸ“Œ Le split-brain doit Ãªtre impossible

**Utilisez REPMGR (17.5.3) si** :
- ğŸ“Œ Vous voulez plus que du manuel mais moins que Patroni
- ğŸ“Œ Votre RTO acceptable est 1-5 minutes
- ğŸ“Œ Vous gÃ©rez un cluster simple (2-5 nÅ“uds)
- ğŸ“Œ Vous privilÃ©giez la simplicitÃ©
- ğŸ“Œ Votre budget est moyen
- ğŸ“Œ Vous voulez faciliter le clonage de Standby

### Questions Ã  Se Poser

**1. Quel est mon RTO cible ?**
- < 30 secondes â†’ Patroni obligatoire
- 1-2 minutes â†’ Patroni ou Repmgr
- 5-15 minutes â†’ Repmgr ou Manuel
- > 15 minutes â†’ Manuel acceptable

**2. Quel est mon RPO cible ?**
- 0 (zÃ©ro perte) â†’ RÃ©plication synchrone + Patroni
- < 1 minute â†’ RÃ©plication asynchrone + Patroni/Repmgr
- 5-15 minutes â†’ RÃ©plication asynchrone standard

**3. Quelle est ma criticitÃ© mÃ©tier ?**
- Financier/E-commerce â†’ Patroni
- SaaS/Web app â†’ Patroni ou Repmgr
- Application interne â†’ Repmgr ou Manuel
- Dev/Test â†’ Manuel

**4. Quelle est la taille de mon Ã©quipe ?**
- DevOps expÃ©rimentÃ©s â†’ Patroni
- Ã‰quipe polyvalente â†’ Repmgr
- Ã‰quipe rÃ©duite â†’ Manuel (avec procÃ©dure documentÃ©e)

**5. Quel est mon budget infrastructure ?**
- Ã‰levÃ© â†’ Patroni (etcd + monitoring complet)
- Moyen â†’ Repmgr (+ witness si possible)
- Faible â†’ Manuel (+ documentation)

---

## PrÃ©requis Communs aux Trois Approches

Quelle que soit l'approche choisie, certains Ã©lÃ©ments sont indispensables :

### 1. RÃ©plication PostgreSQL ConfigurÃ©e

**Minimum requis** :
- Au moins 1 Primary + 1 Standby configurÃ©s
- RÃ©plication en streaming fonctionnelle
- Tests rÃ©guliers de la rÃ©plication

### 2. Monitoring

**MÃ©triques essentielles** :
- Ã‰tat du Primary (up/down)
- Ã‰tat des Standby (up/down)
- Retard de rÃ©plication (replication lag)
- Connexions actives
- Logs d'erreurs

### 3. Documentation

**Documents obligatoires** :
- Architecture du cluster (schÃ©ma)
- ProcÃ©dure de failover (runbook)
- Contacts d'urgence
- Historique des incidents

### 4. Tests RÃ©guliers

**Plan de tests** :
- Switchover planifiÃ© (mensuel)
- Failover non planifiÃ© (trimestriel)
- Reconstruction d'un Standby (trimestriel)
- Test du split-brain (annuel)

### 5. Backup et Recovery

**Essentiels** :
- Backups rÃ©guliers automatisÃ©s
- Tests de restauration
- Point-In-Time Recovery (PITR) configurÃ©
- Backups hors site (3-2-1 rule)

---

## Concepts AvancÃ©s Ã  ConnaÃ®tre

### Timeline PostgreSQL

AprÃ¨s chaque promotion, PostgreSQL incrÃ©mente la **timeline** :

```
Timeline 1 : Primary original
     â”‚
     â”œâ”€ Promotion â†’ Timeline 2 : Nouveau Primary
     â”‚                   â”‚
     â”‚                   â”œâ”€ Promotion â†’ Timeline 3 : Nouveau Primary
     â”‚
Historique tracÃ© dans l'historique des timelines
```

**UtilitÃ©** :
- Tracer l'historique des promotions
- Ã‰viter la corruption lors de la rÃ©intÃ©gration d'un ancien Primary
- Permettre le Point-In-Time Recovery correct

### LSN (Log Sequence Number)

Le **LSN** est la position dans les fichiers WAL.

**Format** : `0/3000000` (segment/offset)

**UtilitÃ©** :
- Mesurer le retard de rÃ©plication
- Choisir le meilleur Standby lors d'un failover (LSN le plus Ã©levÃ©)
- VÃ©rifier la cohÃ©rence aprÃ¨s failover

### pg_rewind

**pg_rewind** permet de resynchroniser un ancien Primary sans reconstruction complÃ¨te.

**ScÃ©nario** :
```
1. postgres1 Ã©tait Primary (Timeline 1)
2. Failover â†’ postgres2 devient Primary (Timeline 2)
3. postgres1 revient en ligne
4. pg_rewind peut resynchroniser postgres1 avec postgres2
   sans refaire un pg_basebackup complet
```

**Avantage** : RÃ©intÃ©gration rapide (minutes au lieu d'heures)

---

## Checklist PrÃ©-Failover

Avant de mettre en place un systÃ¨me de failover, assurez-vous que :

### Infrastructure

- [ ] Au moins 1 Primary + 1 Standby configurÃ©s et testÃ©s
- [ ] RÃ©plication en streaming fonctionnelle
- [ ] pg_rewind activÃ© (`wal_log_hints = on` ou checksums)
- [ ] RÃ©seau stable entre les nÅ“uds
- [ ] Latence rÃ©seau acceptable (< 10ms pour synchro)

### SÃ©curitÃ©

- [ ] Mots de passe robustes pour rÃ©plication
- [ ] pg_hba.conf correctement configurÃ©
- [ ] SSL/TLS activÃ© pour la rÃ©plication (recommandÃ©)
- [ ] Firewall configurÃ©

### Monitoring

- [ ] Monitoring du Primary (heartbeat)
- [ ] Monitoring des Standby (lag, Ã©tat)
- [ ] Alertes configurÃ©es (Primary down, lag Ã©levÃ©)
- [ ] Logs centralisÃ©s

### Documentation

- [ ] Architecture documentÃ©e
- [ ] Runbook de failover rÃ©digÃ© et testÃ©
- [ ] Contacts d'urgence Ã  jour
- [ ] ProcÃ©dure de rollback documentÃ©e

### Tests

- [ ] Test de switchover planifiÃ© rÃ©ussi
- [ ] Test de failover non planifiÃ© rÃ©ussi
- [ ] Test de rÃ©intÃ©gration de l'ancien Primary
- [ ] Test de reconstruction complÃ¨te d'un Standby

### Backup

- [ ] Backups automatisÃ©s actifs
- [ ] Test de restauration rÃ©cent (< 1 mois)
- [ ] PITR configurÃ© et testÃ©
- [ ] Backups stockÃ©s hors site

---

## Structure de Cette Section

Cette section 17.5 est organisÃ©e en trois parties progressives :

### 17.5.1. Promotion Manuelle (pg_ctl promote)

**Vous allez apprendre** :
- La commande `pg_ctl promote` en dÃ©tail
- ProcÃ©dure complÃ¨te de promotion manuelle
- ScÃ©narios de failover manuel
- Troubleshooting des problÃ¨mes courants

**PrÃ©requis** : Aucun (base de dÃ©part)

**DurÃ©e d'apprentissage** : 1-2 heures

---

### 17.5.2. Patroni : HA AutomatisÃ© avec Consensus

**Vous allez apprendre** :
- Architecture de Patroni
- SystÃ¨mes de consensus (etcd, Consul, ZooKeeper)
- Configuration complÃ¨te
- Failover automatique
- Load balancing avec HAProxy

**PrÃ©requis** : MaÃ®trise de 17.5.1

**DurÃ©e d'apprentissage** : 4-8 heures

---

### 17.5.3. Repmgr : Gestion de RÃ©plication SimplifiÃ©e

**Vous allez apprendre** :
- Architecture de Repmgr
- Configuration sans systÃ¨me externe
- Failover semi-automatique
- Witness node pour Ã©viter les faux positifs
- Comparaison avec Patroni

**PrÃ©requis** : MaÃ®trise de 17.5.1, lecture de 17.5.2 recommandÃ©e

**DurÃ©e d'apprentissage** : 3-6 heures

---

## Parcours d'Apprentissage RecommandÃ©

### Pour les DÃ©butants

```
Jour 1 : Lire l'introduction (cette section)
         â†’ Comprendre les concepts (RTO, RPO, split-brain)

Jour 2-3 : Section 17.5.1 (Promotion Manuelle)
           â†’ Pratiquer sur un cluster de test
           â†’ Faire un failover manuel complet

Jour 4 : DÃ©cision stratÃ©gique
         â†’ Ai-je besoin d'automatisation ?
         â†’ Quel est mon RTO cible ?

Si automatisation nÃ©cessaire :
    Jour 5-8 : Section 17.5.2 (Patroni) OU Section 17.5.3 (Repmgr)
```

### Pour les IntermÃ©diaires

```
Jour 1 : RÃ©vision rapide de l'introduction
         â†’ Focus sur RTO/RPO et split-brain

Jour 2 : Comparaison Patroni vs Repmgr
         â†’ Choix selon contexte

Jour 3-5 : ImplÃ©mentation de la solution choisie
           â†’ Configuration complÃ¨te
           â†’ Tests de failover

Jour 6 : Production
         â†’ Monitoring
         â†’ Documentation
```

### Pour les AvancÃ©s

```
Jour 1 : Ã‰valuation architecturale
         â†’ RTO/RPO rÃ©els de mon systÃ¨me actuel
         â†’ Identification des risques

Jour 2 : Migration vers solution HA
         â†’ Planification de la transition
         â†’ Tests en environnement de staging

Jour 3-4 : Mise en production
           â†’ DÃ©ploiement progressif
           â†’ Monitoring renforcÃ©

Jour 5+ : Optimisation continue
          â†’ Tuning des paramÃ¨tres
          â†’ AmÃ©lioration du RTO/RPO
```

---

## Ressources ComplÃ©mentaires

### Documentation Officielle

- **PostgreSQL** : https://www.postgresql.org/docs/current/warm-standby.html
- **Patroni** : https://patroni.readthedocs.io/
- **Repmgr** : https://www.repmgr.org/docs/current/
- **etcd** : https://etcd.io/docs/
- **Consul** : https://www.consul.io/docs

### Blogs Techniques RecommandÃ©s

- **2ndQuadrant** : https://www.2ndquadrant.com/en/blog/
- **Percona** : https://www.percona.com/blog/
- **CrunchyData** : https://www.crunchydata.com/blog
- **Zalando** : https://engineering.zalando.com/tags/postgresql.html

### ConfÃ©rences

- **PGConf** (confÃ©rences rÃ©gionales et internationales)
- **PostgreSQL sessions** (France)
- **FOSDEM** (Bruxelles) - Track PostgreSQL

---

## Conclusion de l'Introduction

Le failover et la promotion sont au cÅ“ur de toute stratÃ©gie de haute disponibilitÃ© PostgreSQL. La comprÃ©hension de ces mÃ©canismes est essentielle pour :

- **ProtÃ©ger votre business** contre les interruptions
- **Minimiser les pertes de donnÃ©es** en cas d'incident
- **Dormir tranquille** en sachant que votre systÃ¨me peut se rÃ©parer (partiellement ou totalement)

Cette section vous donne les clÃ©s pour choisir et implÃ©menter la solution adaptÃ©e Ã  vos besoins, que vous privilÃ©giez :
- La **simplicitÃ©** (promotion manuelle)
- L'**automatisation totale** (Patroni)
- Le **compromis** (Repmgr)

**Prochaine Ã©tape** : Passez Ã  la section 17.5.1 pour dÃ©couvrir la promotion manuelle, fondement de toutes les solutions de failover.

---

**Points clÃ©s Ã  retenir** :
- âœ… Le failover est inÃ©vitable en production
- âœ… RTO et RPO doivent guider votre choix d'architecture
- âœ… Le split-brain est la pire situation possible (Ã©vitez-le Ã  tout prix)
- âœ… Trois approches existent : manuelle, Patroni, Repmgr
- âœ… Testez rÃ©guliÃ¨rement votre procÃ©dure de failover
- âœ… La documentation et les backups sont aussi importants que la solution technique

Bonne lecture et bon apprentissage ! ğŸš€

â­ï¸ [Promotion manuelle (pg_ctl promote)](/17-haute-disponibilite-et-replication/05.1-promotion-manuelle.md)
