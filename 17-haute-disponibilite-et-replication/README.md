ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 17. Haute DisponibilitÃ© et RÃ©plication

## Introduction

La **haute disponibilitÃ©** (High Availability ou HA) et la **rÃ©plication** sont deux piliers essentiels des systÃ¨mes de bases de donnÃ©es en production. Dans un monde oÃ¹ les applications doivent Ãªtre accessibles 24h/24 et 7j/7, oÃ¹ les donnÃ©es doivent Ãªtre protÃ©gÃ©es contre toute perte, et oÃ¹ les performances doivent Ãªtre maintenues mÃªme sous une charge intense, ces concepts deviennent non pas un luxe, mais une nÃ©cessitÃ© absolue.

Ce chapitre vous guidera Ã  travers les diffÃ©rentes stratÃ©gies et technologies que PostgreSQL met Ã  votre disposition pour construire des architectures rÃ©silientes, performantes et fiables.

---

## 1. Qu'est-ce que la Haute DisponibilitÃ© ?

### 1.1. DÃ©finition et objectifs

La **haute disponibilitÃ©** est la capacitÃ© d'un systÃ¨me Ã  rester opÃ©rationnel et accessible pendant une trÃ¨s grande partie du temps, malgrÃ© les pannes matÃ©rielles, logicielles ou les opÃ©rations de maintenance.

**Objectif principal :**
> Minimiser les interruptions de service et garantir que votre base de donnÃ©es reste accessible aux utilisateurs, mÃªme en cas de problÃ¨me.

### 1.2. Mesurer la disponibilitÃ© : Les "9" (nines)

La disponibilitÃ© se mesure gÃ©nÃ©ralement en pourcentage, souvent exprimÃ© en nombre de "9" :

| DisponibilitÃ© | Downtime par an | Downtime par mois | Downtime par semaine | Classification |
|---------------|-----------------|-------------------|---------------------|----------------|
| 90% ("one nine") | 36,5 jours | 3 jours | 16,8 heures | Inacceptable |
| 99% ("two nines") | 3,65 jours | 7,3 heures | 1,68 heures | Faible |
| 99,9% ("three nines") | 8,76 heures | 43,8 minutes | 10,1 minutes | Basique |
| 99,99% ("four nines") | 52,6 minutes | 4,4 minutes | 1,01 minutes | Haute disponibilitÃ© |
| 99,999% ("five nines") | 5,26 minutes | 26 secondes | 6 secondes | TrÃ¨s haute disponibilitÃ© |
| 99,9999% ("six nines") | 31,5 secondes | 2,6 secondes | 0,6 secondes | DisponibilitÃ© extrÃªme |

**Exemples concrets :**

**99% (deux nines) :**
```
â†’ Votre site est inaccessible 3,65 jours par an
â†’ Acceptable pour : blog personnel, site vitrine PME
â†’ Inacceptable pour : e-commerce, banque en ligne
```

**99,99% (quatre nines) :**
```
â†’ Votre site est inaccessible ~53 minutes par an
â†’ Standard pour : applications d'entreprise critiques
â†’ CoÃ»t : ModÃ©rÃ© (rÃ©plication, monitoring)
```

**99,999% (cinq nines) :**
```
â†’ Votre site est inaccessible ~5 minutes par an
â†’ Requis pour : services financiers, tÃ©lÃ©communications
â†’ CoÃ»t : Ã‰levÃ© (redondance complÃ¨te, Ã©quipes 24/7)
```

### 1.3. Pourquoi la haute disponibilitÃ© est-elle critique ?

#### Impact Business

**E-commerce :**
- 1 heure d'indisponibilitÃ© = 100 000â‚¬ de chiffre d'affaires perdu (pour un site moyen)
- Perte de confiance des clients
- Impact sur le rÃ©fÃ©rencement (SEO)

**Services Financiers :**
- Obligation rÃ©glementaire (Basel III, MiFID II)
- Risque de pÃ©nalitÃ©s financiÃ¨res
- RÃ©putation compromise

**SaaS / Cloud :**
- SLA contractuels avec pÃ©nalitÃ©s
- Churn clients (dÃ©parts)
- Dommage d'image irrÃ©versible

**SantÃ© / Urgences :**
- Vies humaines en jeu
- ImpossibilitÃ© d'accÃ©der aux dossiers mÃ©dicaux
- Risque juridique majeur

#### Analogie du magasin physique

Imaginez un magasin physique qui ferme de maniÃ¨re imprÃ©visible :
```
Lundi 10h : FermÃ© 2 heures (panne Ã©lectrique)
Mercredi 14h : FermÃ© 4 heures (problÃ¨me informatique)
Samedi 16h : FermÃ© toute l'aprÃ¨s-midi (jour de plus forte affluence)

RÃ©sultat :
â†’ Les clients vont chez le concurrent
â†’ Perte de revenus directe
â†’ RÃ©putation ternie
```

C'est exactement ce qui se passe avec une base de donnÃ©es non disponible.

---

## 2. Les Causes d'IndisponibilitÃ©

Comprendre **pourquoi** un systÃ¨me devient indisponible est essentiel pour choisir la bonne stratÃ©gie de protection.

### 2.1. Pannes matÃ©rielles (Hardware Failures)

**Exemples courants :**

1. **Panne de disque dur**
   - DurÃ©e de vie moyenne : 3-5 ans
   - ProbabilitÃ© : ~1-3% par disque et par an
   - Impact : Perte de donnÃ©es si pas de RAID

2. **Panne de RAM**
   - Peut causer de la corruption de donnÃ©es
   - DÃ©tection difficile sans ECC (Error-Correcting Code)

3. **Panne d'alimentation**
   - Coupure Ã©lectrique
   - DÃ©faillance de l'onduleur (UPS)
   - Surtension

4. **Panne rÃ©seau**
   - Carte rÃ©seau dÃ©fectueuse
   - Switch tombÃ©
   - CÃ¢ble dÃ©branchÃ© (erreur humaine)

5. **Panne CPU / Carte mÃ¨re**
   - Plus rare mais impact total
   - Remplacement long (plusieurs heures)

**Statistiques :**
Selon une Ã©tude de Google (2007) sur des millions de disques :
- Le taux de pannes annuel varie entre 1,7% et 8,6%
- Les pannes arrivent souvent par "vagues" (lots dÃ©fectueux)
- Les disques jeunes (<1 an) et vieux (>3 ans) sont plus Ã  risque

### 2.2. Pannes logicielles (Software Failures)

**Exemples :**

1. **Bug PostgreSQL**
   - Rare mais possible (surtout sur nouvelles versions)
   - Crash du serveur
   - Corruption de donnÃ©es

2. **Bug systÃ¨me d'exploitation**
   - Kernel panic (Linux)
   - Blue Screen of Death (Windows)

3. **Saturation de ressources**
   - Disque plein (pg_wal/ ou data)
   - MÃ©moire Ã©puisÃ©e (OOM Killer)
   - Connexions saturÃ©es (max_connections)

4. **Mise Ã  jour ratÃ©e**
   - Migration de version mal prÃ©parÃ©e
   - Extension incompatible
   - Configuration incorrecte

### 2.3. Erreurs humaines (Human Errors)

**Les plus frÃ©quentes et dangereuses :**

1. **Suppression accidentelle de donnÃ©es**
   ```sql
   -- Oubli du WHERE !
   DELETE FROM customers;  -- Au lieu de DELETE FROM customers WHERE id = 123;
   ```

2. **DROP de table ou base**
   ```sql
   DROP DATABASE production;  -- Pensait Ãªtre sur "dev"
   ```

3. **Modification de configuration cassante**
   ```ini
   # Oups, trop restrictif
   max_connections = 1
   ```

4. **Commande systÃ¨me destructive**
   ```bash
   rm -rf /var/lib/postgresql/14/main/  # Mauvais rÃ©pertoire
   ```

**Statistiques alarmantes :**
- 70% des pannes en production sont dues Ã  des erreurs humaines (Gartner)
- 50% des erreurs humaines surviennent pendant les maintenances planifiÃ©es

### 2.4. Catastrophes naturelles et sinistres

**Risques physiques :**

1. **Incendie** dans le datacenter
2. **Inondation** (datacenter sous-sol)
3. **Tremblement de terre**
4. **Ouragan / TempÃªte**
5. **Coupure Ã©lectrique gÃ©nÃ©ralisÃ©e**
6. **Attentat / Sabotage**

**Exemple rÃ©el :**
En 2021, un incendie chez OVH (hÃ©bergeur franÃ§ais) a dÃ©truit complÃ¨tement 4 datacenters :
- 3,6 millions de sites web inaccessibles
- DonnÃ©es perdues pour les clients sans backup externe
- Impact Ã©conomique : plusieurs centaines de millions d'euros

### 2.5. Maintenance planifiÃ©e

MÃªme les opÃ©rations prÃ©vues causent de l'indisponibilitÃ© :

- **Mise Ã  jour de PostgreSQL** : RedÃ©marrage requis (5-30 minutes)
- **Changement de configuration** : RedÃ©marrage parfois nÃ©cessaire
- **Migration de serveur** : Plusieurs heures de downtime
- **Extension du stockage** : ArrÃªt du service

---

## 3. Les MÃ©triques de la Haute DisponibilitÃ©

Pour Ã©valuer et gÃ©rer la HA, on utilise des mÃ©triques standardisÃ©es :

### 3.1. RTO (Recovery Time Objective)

**DÃ©finition :**
> Temps maximum acceptable pour restaurer le service aprÃ¨s une panne.

**Exemples :**

| Type d'application | RTO acceptable |
|-------------------|----------------|
| Blog personnel | 24 heures |
| Site e-commerce PME | 1 heure |
| Plateforme bancaire | 15 minutes |
| Service d'urgence mÃ©dical | 30 secondes |

**Implications techniques :**

- **RTO = 24h** : Backup quotidien + restauration manuelle suffit
- **RTO = 1h** : Hot standby + monitoring + procÃ©dure de failover
- **RTO = 15 min** : RÃ©plication synchrone + failover automatisÃ© (Patroni)
- **RTO = 30s** : Multi-master ou rÃ©plication quasi-synchrone + dÃ©tection automatique

### 3.2. RPO (Recovery Point Objective)

**DÃ©finition :**
> QuantitÃ© maximale de donnÃ©es qu'on peut se permettre de perdre, exprimÃ©e en temps.

**Exemples :**

| Type d'application | RPO acceptable | Perte maximale |
|-------------------|----------------|----------------|
| Blog | 24 heures | Articles de la journÃ©e |
| E-commerce | 5 minutes | Quelques commandes |
| Banque | 0 secondes | Aucune transaction |
| RÃ©seau social | 1 minute | Quelques posts/likes |

**Implications techniques :**

- **RPO = 24h** : Backup quotidien nocturne
- **RPO = 1h** : Archivage WAL toutes les heures
- **RPO = 5 min** : RÃ©plication asynchrone + WAL shipping continu
- **RPO = 0** : RÃ©plication synchrone (sacrifice de performance)

### 3.3. MTBF (Mean Time Between Failures)

**DÃ©finition :**
> Temps moyen entre deux pannes.

**Exemple :**
```
Si un serveur tombe en panne 4 fois par an :
â†’ MTBF = 365 jours / 4 = 91,25 jours
â†’ En moyenne, le serveur fonctionne 91 jours avant de tomber en panne
```

**UtilitÃ© :**
- Ã‰valuer la fiabilitÃ© du matÃ©riel
- Planifier les remplacements prÃ©ventifs
- Dimensionner les garanties et SLA

### 3.4. MTTR (Mean Time To Recovery)

**DÃ©finition :**
> Temps moyen nÃ©cessaire pour rÃ©parer et remettre en service aprÃ¨s une panne.

**Exemples :**

| Type de panne | MTTR typique |
|---------------|--------------|
| RedÃ©marrage serveur | 2-5 minutes |
| Remplacement disque (avec RAID) | 30-60 minutes |
| Remplacement serveur complet | 4-8 heures |
| Restauration depuis backup | 1-6 heures |
| Failover automatique | 30 secondes - 2 minutes |

**Calcul de disponibilitÃ© :**
```
DisponibilitÃ© = MTBF / (MTBF + MTTR)

Exemple :
MTBF = 90 jours = 2160 heures
MTTR = 2 heures

DisponibilitÃ© = 2160 / (2160 + 2) = 99,9%
```

---

## 4. Les StratÃ©gies de Haute DisponibilitÃ©

PostgreSQL offre plusieurs approches pour atteindre la haute disponibilitÃ©, chacune avec ses avantages et compromis.

### 4.1. Vue d'ensemble des stratÃ©gies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 StratÃ©gies de HA PostgreSQL                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Backups & Point-In-Time Recovery (PITR)
   â””â”€ Protection : Erreurs humaines, corruption
   â””â”€ RTO : Heures    RPO : Minutes
   â””â”€ CoÃ»t : Faible

2. RÃ©plication Physique (Streaming Replication)
   â””â”€ Protection : Pannes matÃ©rielles, catastrophes
   â””â”€ RTO : Minutes   RPO : Secondes
   â””â”€ CoÃ»t : Moyen

3. RÃ©plication Logique
   â””â”€ Protection : Migrations, rÃ©plication sÃ©lective
   â””â”€ RTO : Minutes   RPO : Secondes
   â””â”€ CoÃ»t : Moyen-Ã‰levÃ©

4. Pooling de Connexions
   â””â”€ Protection : Surcharge, saturation connexions
   â””â”€ Impact : AmÃ©liore la disponibilitÃ© sous charge
   â””â”€ CoÃ»t : Faible

5. Haute DisponibilitÃ© AutomatisÃ©e (Patroni, Repmgr)
   â””â”€ Protection : Automatisation du failover
   â””â”€ RTO : Secondes  RPO : Secondes
   â””â”€ CoÃ»t : Ã‰levÃ©
```

### 4.2. Backups et PITR (Point-In-Time Recovery)

**Principe :**
- Sauvegarde rÃ©guliÃ¨re complÃ¨te (pg_basebackup)
- Archivage continu des fichiers WAL
- Restauration possible Ã  n'importe quel point dans le temps

**Avantages :**
- âœ… Protection contre les erreurs humaines
- âœ… Protection contre la corruption de donnÃ©es
- âœ… ConformitÃ© rÃ©glementaire (audit trail)
- âœ… CoÃ»t faible

**InconvÃ©nients :**
- âŒ RTO Ã©levÃ© (plusieurs heures pour restaurer)
- âŒ NÃ©cessite une procÃ©dure manuelle
- âŒ Ne protÃ¨ge pas contre les pannes en temps rÃ©el

**Cas d'usage :**
- Couche de protection de base (obligatoire pour TOUS les systÃ¨mes)
- Applications non critiques (RTO > 1 heure acceptable)
- Protection contre le "DROP TABLE" accidentel

### 4.3. RÃ©plication Physique (Streaming Replication)

**Principe :**
- Serveur principal (Primary) envoie les modifications binaires (WAL)
- Serveur(s) secondaire(s) (Standby) applique ces modifications
- Copie bit-Ã -bit identique

**Architecture simple :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Primary      â”‚  (Lecture/Ã‰criture)
â”‚   (Production)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WAL Stream
         â”‚ (binaire)
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Standby 1â”‚ â”‚Standby 2â”‚  (Lecture seule)
â”‚ (Paris) â”‚ â”‚(Londres)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Modes :**

1. **Asynchrone** (par dÃ©faut)
   - Le primary n'attend pas la confirmation du standby
   - Performance maximale
   - RPO : quelques secondes (risque de perte de donnÃ©es si crash)

2. **Synchrone**
   - Le primary attend la confirmation du standby avant COMMIT
   - RPO = 0 (aucune perte de donnÃ©es)
   - Performance impactÃ©e (latence rÃ©seau)

**Avantages :**
- âœ… RTO faible (quelques minutes avec failover manuel)
- âœ… RPO faible (secondes en async, 0 en sync)
- âœ… Standbys utilisables en lecture (load balancing)
- âœ… Protection contre pannes matÃ©rielles et catastrophes

**InconvÃ©nients :**
- âŒ Ne protÃ¨ge PAS contre les erreurs humaines (rÃ©pliquÃ©es instantanÃ©ment)
- âŒ Architecture identique requise (version, CPU)
- âŒ ComplexitÃ© de configuration

**Cas d'usage :**
- Applications critiques (e-commerce, SaaS)
- Architecture multi-datacenter
- Disaster Recovery (DR)

### 4.4. RÃ©plication Logique

**Principe :**
- DÃ©codage du WAL en changements logiques (INSERT, UPDATE, DELETE)
- RÃ©plication sÃ©lective (certaines tables uniquement)
- Versions PostgreSQL diffÃ©rentes possibles

**Architecture :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary (PG 14) â”‚
â”‚                  â”‚
â”‚  Publication:    â”‚
â”‚  - users         â”‚
â”‚  - orders        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Changements SQL
         â”‚ (format logique)
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Secondary â”‚ â”‚  Analytics   â”‚
â”‚ (PG 15)  â”‚ â”‚  (PG 14)     â”‚
â”‚          â”‚ â”‚  tables: *   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages :**
- âœ… RÃ©plication sÃ©lective (tables spÃ©cifiques)
- âœ… Versions PostgreSQL diffÃ©rentes
- âœ… Transformations possibles (ETL-like)
- âœ… Architectures hÃ©tÃ©rogÃ¨nes

**InconvÃ©nients :**
- âŒ Overhead supÃ©rieur (dÃ©codage + encodage)
- âŒ Ne rÃ©plique pas le DDL automatiquement
- âŒ Gestion des conflits plus complexe
- âŒ Ne rÃ©plique pas les SEQUENCES ni LARGE OBJECTS

**Cas d'usage :**
- Migrations en ligne (upgrade de version)
- RÃ©plication vers data warehouse analytique
- Multi-master (avec prÃ©cautions)
- Synchronisation inter-rÃ©gions avec filtrage

### 4.5. Pooling de Connexions (PgBouncer, PgPool-II)

**Principe :**
- Pool de connexions rÃ©utilisables
- Ã‰vite la crÃ©ation/destruction coÃ»teuse de connexions
- Limite le nombre de connexions PostgreSQL

**Architecture :**
```
Applications (1000 clients)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PgBouncer     â”‚  Pool: 100 connexions
â”‚  (Connection     â”‚
â”‚   Pooler)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL     â”‚  max_connections = 100
â”‚   (Primary)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Modes de pooling :**

1. **Session pooling**
   - Connexion assignÃ©e pour toute la session
   - Compatible avec toutes les fonctionnalitÃ©s PG
   - RÃ©duction modÃ©rÃ©e des connexions

2. **Transaction pooling** (recommandÃ©)
   - Connexion rendue au pool aprÃ¨s chaque transaction
   - RÃ©duction maximale des connexions
   - Incompatible avec certaines fonctionnalitÃ©s (LISTEN/NOTIFY, cursors)

3. **Statement pooling**
   - Connexion rendue aprÃ¨s chaque requÃªte
   - TrÃ¨s agressif, rarement utilisÃ©

**Avantages :**
- âœ… AmÃ©liore dramatiquement la capacitÃ© de charge
- âœ… ProtÃ¨ge contre la saturation de connexions
- âœ… RÃ©duit la consommation mÃ©moire
- âœ… Transparence pour l'application

**InconvÃ©nients :**
- âŒ Point de dÃ©faillance unique (si pas redondÃ©)
- âŒ ComplexitÃ© ajoutÃ©e dans l'architecture
- âŒ Debugging plus difficile

**Cas d'usage :**
- Applications web avec nombreux clients concurrents
- Serverless (AWS Lambda, Azure Functions)
- Protection contre connection storms

### 4.6. Haute DisponibilitÃ© AutomatisÃ©e

**Solutions :**

1. **Patroni**
   - Gestionnaire de cluster HA open-source
   - Utilise etcd/Consul/ZooKeeper pour le consensus
   - Failover automatique en ~30 secondes

2. **Repmgr**
   - Suite d'outils de gestion de rÃ©plication
   - Failover automatique ou manuel
   - Plus simple que Patroni

3. **Cloud-Native (AWS RDS, Azure, GCP)**
   - HA entiÃ¨rement gÃ©rÃ©e par le provider
   - Failover automatique transparent
   - Configuration simplifiÃ©e

**Architecture Patroni (exemple) :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  etcd / Consul / ZooKeeper       â”‚  (Consensus distribuÃ©)
â”‚  (Qui est le leader ?)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
     â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Patroni  â”‚  â”‚Patroni  â”‚
â”‚  +      â”‚  â”‚  +      â”‚
â”‚Primary  â”‚  â”‚Standby  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²           â”‚
     â”‚           â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Failover automatique
    si Primary down
```

**Avantages :**
- âœ… RTO minimal (30 secondes - 2 minutes)
- âœ… Aucune intervention humaine requise
- âœ… DÃ©tection automatique des pannes
- âœ… Reconfiguration automatique du cluster

**InconvÃ©nients :**
- âŒ ComplexitÃ© trÃ¨s Ã©levÃ©e
- âŒ NÃ©cessite une expertise spÃ©cifique
- âŒ Infrastructure additionnelle (consensus service)
- âŒ CoÃ»t Ã©levÃ© (Ã©quipe, matÃ©riel)

**Cas d'usage :**
- Applications critiques 24/7
- Exigence de RTO < 5 minutes
- Budget IT consÃ©quent

---

## 5. Choisir la Bonne StratÃ©gie

### 5.1. Matrice de dÃ©cision

| CritÃ¨re | Backup/PITR | RÃ©plication Async | RÃ©plication Sync | HA AutomatisÃ©e |
|---------|-------------|-------------------|------------------|----------------|
| **RTO** | Heures | Minutes | Minutes | Secondes |
| **RPO** | Minutes | Secondes | 0 | 0 |
| **CoÃ»t** | â‚¬ | â‚¬â‚¬ | â‚¬â‚¬ | â‚¬â‚¬â‚¬â‚¬ |
| **ComplexitÃ©** | Faible | Moyenne | Moyenne | TrÃ¨s Ã©levÃ©e |
| **Erreurs humaines** | âœ… | âŒ | âŒ | âŒ |
| **Pannes matÃ©rielles** | âš ï¸ | âœ… | âœ… | âœ… |
| **Catastrophes** | âœ… | âœ… | âœ… | âœ… |
| **Lecture distribuÃ©e** | âŒ | âœ… | âœ… | âœ… |

### 5.2. Recommandations par type d'application

#### Startup / MVP
```
StratÃ©gie : Backup/PITR uniquement
- Backup quotidien automatisÃ©
- Archivage WAL vers S3/GCS
- RPO : 1 heure, RTO : 4 heures
- CoÃ»t : ~100â‚¬/mois
```

#### PME / Application d'entreprise
```
StratÃ©gie : RÃ©plication Async + Backup/PITR
- 1 Primary + 1 Standby async
- Backup quotidien
- Archivage WAL continu
- RPO : 30 secondes, RTO : 15 minutes
- CoÃ»t : ~500â‚¬/mois
```

#### Startup en croissance / SaaS
```
StratÃ©gie : RÃ©plication Sync + Backup/PITR + Pooling
- 1 Primary + 2 Standbys (1 sync, 1 async)
- PgBouncer pour gÃ©rer la charge
- Backup quotidien + PITR
- RPO : 0, RTO : 10 minutes
- CoÃ»t : ~2000â‚¬/mois
```

#### Entreprise critique / E-commerce majeur
```
StratÃ©gie : HA AutomatisÃ©e (Patroni) + Multi-datacenter
- Cluster Patroni 3 nÅ“uds minimum
- RÃ©plication synchrone
- PgBouncer redondÃ© (HAProxy)
- Multi-datacenter (DR)
- Ã‰quipe DevOps dÃ©diÃ©e
- RPO : 0, RTO : 30 secondes
- CoÃ»t : ~10000â‚¬/mois
```

#### Banque / Service critique
```
StratÃ©gie : HA Full-Stack avec tous les niveaux
- Patroni + Multi-datacenter + Multi-rÃ©gion
- RÃ©plication synchrone quorum-based
- Backup continu + PITR + archivage long terme
- Monitoring 24/7 + Astreintes
- Tests de failover hebdomadaires
- RPO : 0, RTO : 15 secondes
- CoÃ»t : ~50000â‚¬/mois
```

### 5.3. L'approche par couches (Defense in Depth)

**Recommandation universelle :** Ne jamais dÃ©pendre d'une seule stratÃ©gie !

```
Couche 1 : BACKUPS (obligatoire)
  â””â”€ Protection contre erreurs humaines et corruption

Couche 2 : RÃ‰PLICATION (recommandÃ©e)
  â””â”€ Protection contre pannes matÃ©rielles

Couche 3 : MONITORING & ALERTING (obligatoire)
  â””â”€ DÃ©tection proactive des problÃ¨mes

Couche 4 : TESTS RÃ‰GULIERS (critique)
  â””â”€ Validation que les protections fonctionnent

Couche 5 : PROCÃ‰DURES DOCUMENTÃ‰ES (essentielle)
  â””â”€ Runbooks pour rÃ©agir rapidement
```

---

## 6. Ce que Vous Allez Apprendre

Ce chapitre 17 est organisÃ© en plusieurs sous-chapitres qui couvrent progressivement tous les aspects de la haute disponibilitÃ© :

### 6.1. Fondamentaux (Sections 17.1 Ã  17.4)

**17.1. Le concept de WAL Shipping et archive_mode**
- Comprendre le Write-Ahead Log (WAL)
- Configuration de l'archivage continu
- Base pour PITR et rÃ©plication
- Surveillance et maintenance

**17.2. RÃ©plication Physique (Streaming Replication)**
- Configuration Primary/Standby
- Modes synchrone vs asynchrone
- Hot Standby (lecture sur replica)
- Cascading replication

**17.3. RÃ©plication Logique**
- Publications et Subscriptions
- Cas d'usage et limitations
- RÃ©plication sÃ©lective
- Gestion des conflits

**17.4. Slots de rÃ©plication : Physical vs Logical**
- PrÃ©vention de la perte de WAL
- DiffÃ©rences entre slots physiques et logiques
- Monitoring et maintenance
- Risques et mitigation

### 6.2. OpÃ©rations (Sections 17.5 Ã  17.6)

**17.5. Failover et Promotion**
- Promotion manuelle d'un standby
- DÃ©tection de panne
- ProcÃ©dures de basculement
- Reconstruction aprÃ¨s failover

**17.6. Architectures HA complÃ¨tes**
- Patroni : HA automatisÃ©e avec consensus
- Repmgr : Gestion de rÃ©plication simplifiÃ©e
- Architectures multi-datacenter
- Load balancing avec HAProxy/PgPool-II

---

## 7. PrÃ©requis et PrÃ©paration

### 7.1. Connaissances requises

Pour aborder ce chapitre sereinement, vous devriez Ãªtre Ã  l'aise avec :

âœ… **PostgreSQL de base :**
- Installation et configuration
- Fichier postgresql.conf et pg_hba.conf
- Notion de cluster et data directory

âœ… **Administration systÃ¨me :**
- Linux : fichiers, permissions, services (systemd)
- RÃ©seau : IP, ports, pare-feu
- Stockage : disques, partitions, montages

âœ… **SQL :**
- RequÃªtes de base (SELECT, INSERT, UPDATE, DELETE)
- Notions de transactions (COMMIT, ROLLBACK)

### 7.2. Environnement de test

Pour expÃ©rimenter (en dehors de ce tutoriel thÃ©orique), il est recommandÃ© d'avoir :

**Option 1 : Machines virtuelles locales**
```
- 3 VM Linux (Ubuntu 22.04 / Debian 12)
- 2 CPU, 4 GB RAM chacune
- RÃ©seau privÃ© entre les VMs
```

**Option 2 : Cloud (AWS, GCP, Azure)**
```
- 3 instances t3.small (ou Ã©quivalent)
- RÃ©seau VPC privÃ©
- CoÃ»t : ~100â‚¬/mois (pensez Ã  Ã©teindre quand inutilisÃ© !)
```

**Option 3 : Docker (pour tests rapides)**
```
- Docker Compose avec 3 conteneurs PostgreSQL
- RÃ©seau bridge personnalisÃ©
- Volumes persistants
```

### 7.3. Versions PostgreSQL

Les concepts de ce chapitre s'appliquent Ã  partir de **PostgreSQL 10** (introduction de la rÃ©plication logique native).

**Versions recommandÃ©es :**
- PostgreSQL 14+ : AmÃ©liorations de performance et monitoring
- PostgreSQL 15+ : AmÃ©lioration de la rÃ©plication logique
- **PostgreSQL 18** : I/O asynchrone, optimisations majeures (focus de ce tutoriel)

---

## 8. Avertissements et Bonnes Pratiques

### 8.1. Avertissements importants

âš ï¸ **La haute disponibilitÃ© ne remplace PAS les backups**
- MÃªme avec rÃ©plication, vous devez avoir des backups rÃ©guliers
- La rÃ©plication rÃ©plique aussi les erreurs (DROP TABLE, suppression de donnÃ©es)

âš ï¸ **Testez, testez, testez !**
- Un plan de HA non testÃ© est un plan qui Ã©chouera en production
- Organisez des "game days" : simulez des pannes volontairement

âš ï¸ **Documentez tout**
- ProcÃ©dures de failover Ã©crites et Ã  jour
- Runbooks accessibles 24/7
- Contacts d'astreinte clairement dÃ©finis

âš ï¸ **Surveillez proactivement**
- Le monitoring n'est pas optionnel
- Alertes configurÃ©es sur toutes les mÃ©triques critiques
- Tests des alertes rÃ©guliers

âš ï¸ **La rÃ©plication synchrone a un coÃ»t**
- Latence rÃ©seau impacte directement les performances
- Peut diviser le dÃ©bit par 2 ou plus
- Ã€ utiliser avec discernement

### 8.2. Checklist avant de commencer

Avant d'implÃ©menter une stratÃ©gie HA en production :

**Infrastructure :**
- [ ] Serveurs redondants disponibles
- [ ] RÃ©seau stable et surveillÃ©
- [ ] Stockage avec redondance (RAID, SAN)
- [ ] Monitoring en place (Prometheus, Grafana, etc.)

**PostgreSQL :**
- [ ] Version supportÃ©e (ideally LTS)
- [ ] Configuration optimisÃ©e
- [ ] Backups testÃ©s et fonctionnels
- [ ] ProcÃ©dure de restauration documentÃ©e

**Ã‰quipe :**
- [ ] CompÃ©tences PostgreSQL et Linux
- [ ] Astreintes organisÃ©es (24/7 pour HA critique)
- [ ] Formation sur les outils (Patroni, Repmgr)
- [ ] Runbooks Ã©crits et validÃ©s

**Processus :**
- [ ] Tests de basculement planifiÃ©s
- [ ] MÃ©triques RTO/RPO dÃ©finies
- [ ] SLA contractualisÃ©s (si applicable)
- [ ] Post-mortem aprÃ¨s chaque incident

---

## 9. Conclusion de l'Introduction

La **haute disponibilitÃ©** n'est pas un produit que l'on achÃ¨te, mais un **Ã©tat** que l'on atteint grÃ¢ce Ã  une combinaison de :
- Technologies appropriÃ©es (rÃ©plication, pooling, clustering)
- Architecture rÃ©siliente (redondance, gÃ©o-distribution)
- Processus rigoureux (monitoring, tests, documentation)
- Culture d'Ã©quipe (proactivitÃ©, amÃ©lioration continue)

PostgreSQL offre tous les outils nÃ©cessaires pour construire des architectures hautement disponibles, de la simple rÃ©plication Ã  deux nÅ“uds jusqu'aux clusters multi-datacenter automatisÃ©s.

Dans les sections suivantes, nous allons explorer chacune de ces technologies en dÃ©tail, avec des explications accessibles, des exemples concrets, et les meilleures pratiques Ã©prouvÃ©es par des annÃ©es de production.

**Rappelez-vous :**
> "La meilleure stratÃ©gie HA est celle que vous comprenez, que vous testez rÃ©guliÃ¨rement, et qui correspond rÃ©ellement Ã  vos besoins mÃ©tier."

Ne visez pas la perfection thÃ©orique (six nines Ã  tout prix), mais l'Ã©quilibre optimal entre disponibilitÃ©, coÃ»t, et complexitÃ© pour **votre** contexte spÃ©cifique.

---

## PrÃªt Ã  commencer ?

Passons maintenant aux fondamentaux en commenÃ§ant par le **WAL Shipping** et l'**archive_mode**, la pierre angulaire de toute stratÃ©gie de haute disponibilitÃ© avec PostgreSQL.

---


â­ï¸ [Le concept de WAL Shipping et archive_mode](/17-haute-disponibilite-et-replication/01-wal-shipping-archive-mode.md)
