üîù Retour au [Sommaire](/SOMMAIRE.md)

# 17.2.3. Cascading Replication - R√©plication en Cascade PostgreSQL 18

## Introduction

La **r√©plication en cascade** (Cascading Replication) est une fonctionnalit√© avanc√©e de PostgreSQL qui permet √† un serveur standby de servir lui-m√™me de source de r√©plication pour d'autres standbys. Autrement dit, au lieu que tous les standbys se connectent directement au primary, certains standbys peuvent se connecter √† d'autres standbys pour recevoir les WAL.

Cette architecture permet de r√©duire la charge sur le serveur primary, d'am√©liorer la scalabilit√©, et de construire des topologies de r√©plication g√©ographiquement distribu√©es plus efficaces.

### Concept de base

**Architecture traditionnelle (sans cascade) :**
```
Primary
 ‚îú‚îÄ> Standby 1
 ‚îú‚îÄ> Standby 2
 ‚îî‚îÄ> Standby 3
```

**Architecture en cascade :**
```
Primary
 ‚îú‚îÄ> Standby 1
 ‚îî‚îÄ> Standby 2
      ‚îî‚îÄ> Standby 3
```

Dans le second cas, `Standby 3` re√ßoit les WAL de `Standby 2` au lieu du `Primary`. On dit que `Standby 2` est un **standby interm√©diaire** et `Standby 3` est un **standby en cascade**.

---

## Pourquoi Utiliser la R√©plication en Cascade ?

### 1. R√©duction de la charge sur le Primary

**Probl√®me :** Chaque standby connect√© au primary consomme des ressources :
- Un processus WAL sender par standby
- Bande passante r√©seau
- CPU pour l'encodage et l'envoi des WAL
- M√©moire pour maintenir les buffers de r√©plication

**Solution avec la cascade :**
Si vous avez 10 standbys, au lieu de les connecter tous au primary, vous pouvez avoir :
- Primary ‚Üí 2 standbys interm√©diaires
- Chaque standby interm√©diaire ‚Üí 4 standbys en cascade

**Avantages :**
- Le primary ne g√®re que 2 connexions WAL sender (au lieu de 10)
- R√©duction de 80% de la charge r√©seau sur le primary
- CPU et m√©moire lib√©r√©s pour traiter les requ√™tes applicatives

### 2. R√©plication g√©ographiquement distribu√©e

**Sc√©nario :** Vous avez des bureaux dans plusieurs r√©gions et souhaitez un standby dans chaque r√©gion.

**Architecture sans cascade :**
```
Primary (Paris)
 ‚îú‚îÄ> Standby (Londres)    [Latence: 10ms, 2000 km]
 ‚îú‚îÄ> Standby (New York)   [Latence: 80ms, 6000 km]
 ‚îú‚îÄ> Standby (Tokyo)      [Latence: 200ms, 10000 km]
 ‚îî‚îÄ> Standby (Sydney)     [Latence: 250ms, 17000 km]
```

**Probl√®mes :**
- Latence √©lev√©e pour les standbys distants
- Co√ªts de bande passante inter-continentale √©lev√©s
- Le primary doit envoyer les WAL 4 fois sur de longues distances

**Architecture avec cascade :**
```
Primary (Paris)
 ‚îú‚îÄ> Standby (Londres)    [Latence: 10ms]
 ‚îî‚îÄ> Standby (New York)   [Latence: 80ms]
      ‚îú‚îÄ> Standby (Tokyo)      [Latence depuis NY: 150ms]
      ‚îî‚îÄ> Standby (Sydney)     [Latence depuis NY: 180ms]
```

**Avantages :**
- Le primary n'envoie les WAL qu'√† 2 destinations (Europe et Am√©rique)
- Les standbys asiatiques re√ßoivent les WAL depuis New York (plus proche)
- R√©duction des co√ªts de bande passante intercontinentale
- Meilleure r√©partition de la charge r√©seau

### 3. Hi√©rarchie de criticit√©

**Sc√©nario :** Vous avez des standbys avec diff√©rents niveaux de criticit√©.

**Architecture :**
```
Primary (Production)
 ‚îú‚îÄ> Standby Hot (HA imm√©diate)     [Criticit√©: Haute]
 ‚îî‚îÄ> Standby Warm (HA secondaire)   [Criticit√©: Moyenne]
      ‚îú‚îÄ> Standby Reporting          [Criticit√©: Faible - Lectures]
      ‚îî‚îÄ> Standby Dev/Test            [Criticit√©: Faible - D√©veloppement]
```

**Avantages :**
- Les standbys critiques (HA) re√ßoivent les WAL directement du primary (latence minimale)
- Les standbys non critiques (reporting, dev) sont en cascade (ne surchargent pas le primary)
- En cas de panne du primary, seuls les standbys directs sont candidats √† la promotion

### 4. Limitation de bande passante

**Sc√©nario :** Le primary a une bande passante r√©seau limit√©e (ex: 100 Mbps).

**Calcul de bande passante :**
- Volume de WAL g√©n√©r√© : 50 MB/s
- Avec 5 standbys : 5 √ó 50 MB/s = 250 MB/s requis
- Bande passante disponible : 100 MB/s ‚Üí **Saturation !**

**Solution avec cascade :**
```
Primary (50 MB/s WAL)
 ‚îú‚îÄ> Standby 1 (50 MB/s) ‚Üí 2 standbys en cascade
 ‚îî‚îÄ> Standby 2 (50 MB/s) ‚Üí 2 standbys en cascade
```

Total utilis√© sur le primary : 100 MB/s (au lieu de 250 MB/s)

---

## Architecture et Fonctionnement

### Architecture de base en cascade

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Primary             ‚îÇ
‚îÇ   (Read/Write)        ‚îÇ
‚îÇ                       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ WAL Sender 1  ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îÇ Stream WAL (50 MB/s)
            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Standby Intermediate‚îÇ
‚îÇ   (Read-Only)         ‚îÇ
‚îÇ                       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ WAL Receiver  ‚îÇ   ‚îÇ  ‚Üê Re√ßoit du Primary
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ           ‚Üì           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ Startup Proc  ‚îÇ   ‚îÇ  ‚Üê Applique les WAL
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ           ‚Üì           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ WAL Sender 2  ‚îÇ   ‚îÇ  ‚Üê Renvoie aux cascades
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îÇ Stream WAL (50 MB/s)
            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Standby Cascade     ‚îÇ
‚îÇ   (Read-Only)         ‚îÇ
‚îÇ                       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ WAL Receiver  ‚îÇ   ‚îÇ  ‚Üê Re√ßoit du Standby
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ           ‚Üì           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ Startup Proc  ‚îÇ   ‚îÇ  ‚Üê Applique les WAL
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flux de donn√©es d√©taill√©

**√âtape par √©tape :**

1. **Primary g√©n√®re des WAL**
   - Les transactions sont valid√©es sur le primary
   - Les WAL sont √©crits dans `pg_wal/`

2. **Primary ‚Üí Standby Interm√©diaire**
   - Le WAL sender du primary envoie les WAL au standby interm√©diaire
   - Le standby interm√©diaire les re√ßoit via son WAL receiver
   - Le standby interm√©diaire √©crit ces WAL dans son propre `pg_wal/`

3. **Standby Interm√©diaire applique les WAL**
   - Le processus de startup rejoue les WAL
   - La base de donn√©es du standby interm√©diaire est mise √† jour

4. **Standby Interm√©diaire ‚Üí Standby Cascade**
   - **Important :** Le standby interm√©diaire agit comme un "mini-primary" pour les standbys en cascade
   - Son WAL sender envoie les WAL (qu'il a re√ßus du primary) aux standbys en cascade
   - Les standbys en cascade re√ßoivent et appliquent ces WAL

**Points cl√©s √† comprendre :**
- Le standby interm√©diaire **ne modifie pas** les WAL, il les retransmet
- Les standbys en cascade re√ßoivent exactement les m√™mes WAL que ceux g√©n√©r√©s par le primary
- Il peut y avoir un **d√©lai cumulatif** : le standby cascade est l√©g√®rement plus en retard que le standby interm√©diaire

### D√©lai de r√©plication (lag)

**Sans cascade :**
```
Primary ‚Üí Standby : Lag = 0.01s
```

**Avec cascade :**
```
Primary ‚Üí Standby Interm√©diaire : Lag 1 = 0.01s
Standby Interm√©diaire ‚Üí Standby Cascade : Lag 2 = 0.01s
Total : 0.02s
```

Le lag se **cumule** √† chaque niveau de cascade. Pour une architecture √† 3 niveaux :
```
Primary ‚Üí Standby1 ‚Üí Standby2 ‚Üí Standby3
Lag total = Lag1 + Lag2 + Lag3
```

**Recommandation :** Limiter √† 2-3 niveaux maximum pour √©viter un lag excessif.

---

## Configuration de la R√©plication en Cascade

### Pr√©requis

Avant de configurer une r√©plication en cascade, vous devez avoir :
- ‚úÖ Un serveur Primary fonctionnel
- ‚úÖ Au moins un Standby Interm√©diaire d√©j√† configur√© et op√©rationnel
- ‚úÖ Comprendre les bases de la r√©plication Primary/Standby (voir chapitre 17.2.1)

### √âtape 1 : Configurer le Standby Interm√©diaire

Le standby interm√©diaire doit √™tre configur√© pour **accepter des connexions de r√©plication** des standbys en cascade.

#### 1.1. Configuration de postgresql.conf sur le Standby Interm√©diaire

**√âditer `/var/lib/postgresql/18/main/postgresql.conf` :**

```ini
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Configuration pour accepter les cascades
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Nombre maximum de connexions WAL sender
# Doit √™tre > 0 pour permettre les cascades
max_wal_senders = 5

# Niveau WAL (h√©rit√© du primary, mais on le confirme)
wal_level = replica

# Conserver suffisamment de WAL pour les cascades
wal_keep_size = 1GB

# Slots de r√©plication pour les cascades (recommand√©)
max_replication_slots = 5

# Hot standby doit √™tre activ√©
hot_standby = on
```

**Explications :**

**`max_wal_senders = 5`**
- **Crucial !** Un standby avec `max_wal_senders = 0` ne peut pas servir de source de r√©plication
- Nombre de standbys en cascade qui peuvent se connecter √† ce standby interm√©diaire
- Valeur typique : nombre de cascades pr√©vues + 2-3 pour les outils de monitoring

**`wal_keep_size = 1GB`**
- Garantit que les WAL n√©cessaires aux cascades sont conserv√©s
- Important si un standby cascade se d√©connecte temporairement
- Dimensionner selon le volume d'√©critures et la tol√©rance aux d√©connexions

**`max_replication_slots = 5`**
- Permet de cr√©er des slots de r√©plication pour les standbys en cascade
- Garantit que les WAL ne seront pas supprim√©s tant que les cascades ne les ont pas re√ßus
- **Recommand√©** pour √©viter les erreurs "WAL segment has been removed"

#### 1.2. Configuration de pg_hba.conf sur le Standby Interm√©diaire

Le standby interm√©diaire doit autoriser les connexions de r√©plication des standbys en cascade.

**√âditer `/var/lib/postgresql/18/main/pg_hba.conf` :**

```
# TYPE  DATABASE        USER            ADDRESS                 METHOD

# Autoriser la r√©plication depuis les standbys en cascade
# Remplacer par les IPs r√©elles de vos standbys cascade
host    replication     replicator      192.168.1.30/32         scram-sha-256
host    replication     replicator      192.168.1.31/32         scram-sha-256

# Ou pour un sous-r√©seau entier (moins s√©curis√©)
host    replication     replicator      192.168.1.0/24          scram-sha-256
```

**Note :** C'est le m√™me utilisateur de r√©plication (`replicator`) que celui utilis√© par le primary.

#### 1.3. Cr√©er des slots de r√©plication sur le Standby Interm√©diaire (recommand√©)

**Se connecter au standby interm√©diaire :**

```sql
-- Cr√©er un slot pour chaque standby cascade
SELECT pg_create_physical_replication_slot('cascade_standby1_slot');
SELECT pg_create_physical_replication_slot('cascade_standby2_slot');
```

**V√©rification :**
```sql
SELECT slot_name, active, restart_lsn
FROM pg_replication_slots;
```

#### 1.4. Recharger/Red√©marrer le Standby Interm√©diaire

```bash
# Recharger la configuration (pour pg_hba.conf)
sudo systemctl reload postgresql

# Si max_wal_senders a √©t√© modifi√©, red√©marrage n√©cessaire
sudo systemctl restart postgresql
```

**V√©rification :**
```sql
-- Se connecter au standby interm√©diaire
SHOW max_wal_senders;  -- Doit √™tre > 0
```

### √âtape 2 : Cr√©er le Standby en Cascade

Le standby en cascade se configure exactement comme un standby normal, sauf qu'il se connecte au standby interm√©diaire au lieu du primary.

#### 2.1. Effectuer un Base Backup depuis le Standby Interm√©diaire

**Sur le serveur du futur standby cascade :**

```bash
# Arr√™ter PostgreSQL (si d√©j√† en cours)
sudo systemctl stop postgresql

# Sauvegarder l'ancien r√©pertoire (pr√©caution)
sudo mv /var/lib/postgresql/18/main /var/lib/postgresql/18/main.old

# Effectuer un basebackup DEPUIS le standby interm√©diaire
pg_basebackup \
    -h 192.168.1.20 \                      # IP du STANDBY INTERM√âDIAIRE (pas du primary)
    -D /var/lib/postgresql/18/main \       # R√©pertoire de destination
    -U replicator \                        # Utilisateur de r√©plication
    -P \                                   # Afficher la progression
    -v \                                   # Mode verbose
    -R \                                   # Cr√©er automatiquement standby.signal
    -X stream \                            # Inclure les WAL n√©cessaires
    -C \                                   # Cr√©er le slot automatiquement
    -S cascade_standby1_slot               # Nom du slot (doit exister sur le standby interm√©diaire)
```

**Point crucial :** L'option `-h` pointe vers le **standby interm√©diaire**, pas vers le primary.

#### 2.2. Configurer postgresql.conf sur le Standby Cascade

**√âditer `/var/lib/postgresql/18/main/postgresql.conf` :**

```ini
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Configuration Hot Standby
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

hot_standby = on
hot_standby_feedback = on

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Configuration de connexion au Standby Interm√©diaire
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Connexion au STANDBY INTERM√âDIAIRE (pas au primary !)
primary_conninfo = 'host=192.168.1.20 port=5432 user=replicator password=*** application_name=cascade_standby1'

# Utiliser le slot cr√©√© sur le standby interm√©diaire
primary_slot_name = 'cascade_standby1_slot'

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Options additionnelles
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Commande de restauration WAL (optionnel)
restore_command = 'cp /archive/pg_wal/%f %p'

# Fichier trigger pour promotion
promote_trigger_file = '/tmp/postgresql.trigger.5432'
```

**Points importants :**

**`primary_conninfo`**
- **host=192.168.1.20** : C'est l'IP du **standby interm√©diaire**, pas du primary
- **application_name=cascade_standby1** : Nom pour identifier ce standby dans les vues de monitoring

**`primary_slot_name`**
- Doit correspondre au slot cr√©√© sur le standby interm√©diaire
- Garantit que le standby interm√©diaire conservera les WAL n√©cessaires

#### 2.3. V√©rifier le fichier standby.signal

Si vous avez utilis√© l'option `-R` avec `pg_basebackup`, ce fichier est cr√©√© automatiquement :

```bash
ls -la /var/lib/postgresql/18/main/standby.signal
```

Sinon, cr√©ez-le :
```bash
touch /var/lib/postgresql/18/main/standby.signal
```

#### 2.4. Ajuster les permissions

```bash
sudo chown -R postgres:postgres /var/lib/postgresql/18/main
sudo chmod 700 /var/lib/postgresql/18/main
```

#### 2.5. D√©marrer le Standby Cascade

```bash
sudo systemctl start postgresql
sudo systemctl status postgresql
```

### √âtape 3 : V√©rification de la Cascade

#### 3.1. V√©rifier sur le Primary

```sql
-- Se connecter au primary
SELECT
    application_name,
    client_addr,
    state,
    sync_state
FROM pg_stat_replication;
```

**R√©sultat attendu :**
```
 application_name    | client_addr   | state     | sync_state
---------------------+---------------+-----------+------------
 standby_intermediate | 192.168.1.20  | streaming | async
```

Vous devriez voir uniquement le standby interm√©diaire connect√© (pas les cascades).

#### 3.2. V√©rifier sur le Standby Interm√©diaire

```sql
-- Se connecter au standby interm√©diaire
SELECT
    application_name,
    client_addr,
    state,
    sync_state
FROM pg_stat_replication;
```

**R√©sultat attendu :**
```
 application_name | client_addr   | state     | sync_state
------------------+---------------+-----------+------------
 cascade_standby1 | 192.168.1.30  | streaming | async
```

Vous devriez voir le(s) standby(s) en cascade connect√©(s).

#### 3.3. V√©rifier sur le Standby Cascade

```sql
-- Se connecter au standby cascade
SELECT pg_is_in_recovery();  -- Doit retourner 'true'

-- V√©rifier la source de r√©plication
SELECT conninfo
FROM pg_stat_wal_receiver;
```

**R√©sultat attendu :**
```
conninfo
----------------------------------------------------------------
host=192.168.1.20 port=5432 user=replicator application_name=...
```

L'adresse doit correspondre au **standby interm√©diaire** (192.168.1.20), pas au primary.

#### 3.4. Test de bout en bout

**Sur le Primary :**
```sql
CREATE TABLE test_cascade (
    id SERIAL PRIMARY KEY,
    message TEXT,
    created_at TIMESTAMP DEFAULT now()
);

INSERT INTO test_cascade (message) VALUES ('Test depuis le primary');
```

**Sur le Standby Interm√©diaire (attendre 1-2 secondes) :**
```sql
SELECT * FROM test_cascade;
-- La ligne doit √™tre visible
```

**Sur le Standby Cascade (attendre 2-3 secondes) :**
```sql
SELECT * FROM test_cascade;
-- La ligne doit √™tre visible avec un l√©ger d√©lai suppl√©mentaire
```

---

## Topologies Avanc√©es

### Cascade √† 2 niveaux

**Architecture :**
```
Primary
 ‚îú‚îÄ> Standby A
 ‚îÇ    ‚îú‚îÄ> Standby A1 (cascade)
 ‚îÇ    ‚îî‚îÄ> Standby A2 (cascade)
 ‚îî‚îÄ> Standby B
      ‚îú‚îÄ> Standby B1 (cascade)
      ‚îî‚îÄ> Standby B2 (cascade)
```

**Cas d'usage :**
- R√©partition g√©ographique : 2 r√©gions avec plusieurs standbys par r√©gion
- R√©partition de charge : Standby A pour les lectures OLTP, Standby B pour les lectures analytics

**Configuration :**
- Primary : `max_wal_senders = 5` (pour A et B + r√©serve)
- Standby A : `max_wal_senders = 5` (pour A1 et A2 + r√©serve)
- Standby B : `max_wal_senders = 5` (pour B1 et B2 + r√©serve)

### Cascade √† 3 niveaux

**Architecture :**
```
Primary (Paris)
 ‚îî‚îÄ> Standby 1 (Londres)
      ‚îî‚îÄ> Standby 2 (New York)
           ‚îî‚îÄ> Standby 3 (Tokyo)
```

**Avantages :**
- R√©duction maximale de la charge r√©seau sur le primary
- Adapt√© aux architectures vraiment globales

**Inconv√©nients :**
- Lag cumulatif √©lev√© (peut atteindre 500ms ou plus)
- Complexit√© de troubleshooting accrue
- Risque en cascade : si Standby 2 tombe, Standby 3 est d√©connect√©

**Recommandation :** √Ä utiliser uniquement si absolument n√©cessaire. Privil√©gier 2 niveaux maximum.

### Cascade mixte (synchrone + asynchrone)

**Architecture :**
```
Primary
 ‚îú‚îÄ> Standby Hot (synchrone)
 ‚îî‚îÄ> Standby Warm (asynchrone)
      ‚îú‚îÄ> Standby Reporting (cascade)
      ‚îî‚îÄ> Standby Dev/Test (cascade)
```

**Configuration sur le Primary :**
```ini
synchronous_standby_names = 'standby_hot'
```

**Avantages :**
- Zero data loss pour le standby Hot (synchrone)
- Standbys non critiques (reporting, dev) ne surchargent pas le primary
- Isolation des charges : lectures intensives sur les cascades

**Cas d'usage typique :** Production avec environnements de reporting et d√©veloppement.

### Multi-datacenter avec cascade

**Architecture :**
```
Datacenter Paris (Primary)
    ‚îú‚îÄ> Standby Local 1 (Paris)
    ‚îî‚îÄ> Standby Local 2 (Paris)

Datacenter Londres
    ‚îî‚îÄ> Standby Interm√©diaire (Londres)
         ‚îú‚îÄ> Standby Cascade 1 (Londres)
         ‚îî‚îÄ> Standby Cascade 2 (Londres)

Datacenter New York
    ‚îî‚îÄ> Standby Interm√©diaire (New York)
         ‚îú‚îÄ> Standby Cascade 1 (New York)
         ‚îî‚îÄ> Standby Cascade 2 (New York)
```

**Flux de donn√©es :**
- Primary (Paris) ‚Üí Standby Interm√©diaires (Londres, New York)
- Chaque standby interm√©diaire alimente ses cascades locales

**Avantages :**
- R√©duction drastique du trafic inter-datacenter
- Chaque r√©gion a plusieurs standbys pour la HA locale
- Co√ªts de bande passante optimis√©s

**Configuration du trafic :**
- Primary ‚Üí Londres : 50 MB/s
- Primary ‚Üí New York : 50 MB/s
- Total sur le primary : 100 MB/s (au lieu de 50 MB/s √ó 6 standbys = 300 MB/s)

---

## Monitoring et Observabilit√©

### Vue d'ensemble de la topologie

Pour visualiser la topologie compl√®te de r√©plication :

```sql
-- Sur n'importe quel serveur
WITH RECURSIVE replication_tree AS (
    -- Niveau 0 : Primary
    SELECT
        NULL::text AS upstream_host,
        inet_server_addr()::text AS host,
        current_database() AS database,
        0 AS level
    WHERE NOT pg_is_in_recovery()

    UNION ALL

    -- Niveaux suivants : Standbys
    SELECT
        regexp_replace(conninfo, '.*host=([^ ]+).*', '\1') AS upstream_host,
        application_name AS host,
        current_database() AS database,
        rt.level + 1 AS level
    FROM pg_stat_replication
    JOIN replication_tree rt ON true
)
SELECT
    level,
    REPEAT('  ', level) || host AS hierarchy,
    upstream_host
FROM replication_tree
ORDER BY level, host;
```

**Exemple de r√©sultat :**
```
 level | hierarchy               | upstream_host
-------+-------------------------+---------------
 0     | primary-db              | NULL
 1     |   standby-intermediate  | primary-db
 2     |     cascade-standby1    | standby-intermediate
 2     |     cascade-standby2    | standby-intermediate
```

### Monitoring du lag par niveau

**Sur le Primary :**
```sql
SELECT
    application_name,
    client_addr,
    state,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) / 1024 / 1024 AS lag_mb,
    write_lag,
    flush_lag,
    replay_lag
FROM pg_stat_replication
ORDER BY application_name;
```

**Sur les Standbys Interm√©diaires :**
```sql
-- Lag depuis le primary
SELECT
    now() - pg_last_xact_replay_timestamp() AS replication_lag_time
FROM pg_stat_database LIMIT 1;

-- Standbys en cascade connect√©s
SELECT
    application_name,
    client_addr,
    state,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) / 1024 / 1024 AS lag_mb
FROM pg_stat_replication;
```

### M√©triques √† surveiller

**1. Lag cumulatif total**

Pour calculer le lag total d'un standby cascade :
```
Lag Total = Lag(Primary ‚Üí Interm√©diaire) + Lag(Interm√©diaire ‚Üí Cascade)
```

**Seuils recommand√©s :**
- Lag niveau 1 (Primary ‚Üí Interm√©diaire) : < 10 secondes
- Lag niveau 2 (Interm√©diaire ‚Üí Cascade) : < 10 secondes
- **Lag total : < 20 secondes**

**2. √âtat des connexions**

```sql
-- Sur chaque niveau, v√©rifier que state = 'streaming'
SELECT application_name, state FROM pg_stat_replication;
```

**3. Saturation des WAL senders**

```sql
-- V√©rifier qu'on ne d√©passe pas max_wal_senders
SELECT
    count(*) AS active_senders,
    current_setting('max_wal_senders')::int AS max_senders
FROM pg_stat_replication;
```

Si `active_senders` approche `max_senders`, augmenter `max_wal_senders`.

**4. Accumulation de WAL**

```sql
-- Sur les standbys interm√©diaires
SELECT
    slot_name,
    active,
    pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) / 1024 / 1024 AS retained_mb
FROM pg_replication_slots;
```

Si `retained_mb` > 10 GB et `active = false`, un standby cascade est d√©connect√© depuis longtemps.

### Alertes recommand√©es

**Alertes critiques :**
- Aucun standby en cascade connect√© √† un standby interm√©diaire (alors que pr√©vu)
- Lag total > 60 secondes sur un standby cascade
- √âtat de r√©plication != 'streaming' pendant > 5 minutes
- Accumulation de WAL sur standby interm√©diaire > 20 GB

**Alertes warning :**
- Lag > 20 secondes sur n'importe quel niveau
- Utilisation de `max_wal_senders` > 80%

---

## Cas d'Usage D√©taill√©s

### Cas 1 : E-commerce avec Plusieurs Environnements

**Contexte :**
- Production : 1 primary + 1 standby HA
- Reporting : 2 standbys pour BI et analytics
- D√©veloppement : 2 standbys pour dev/test

**Architecture :**
```
Primary (Production)
 ‚îú‚îÄ> Standby HA (Production)
 ‚îî‚îÄ> Standby Reporting (Production)
      ‚îú‚îÄ> Standby BI (Cascade - lectures intensives)
      ‚îî‚îÄ> Standby Analytics (Cascade - requ√™tes lourdes)
```

**Configuration :**
```ini
# Primary
max_wal_senders = 5
synchronous_standby_names = 'standby_ha'

# Standby Reporting (interm√©diaire)
max_wal_senders = 5
```

**Avantages :**
- Standby HA prot√®ge la production (sync)
- Standbys reporting/analytics ne surchargent pas le primary
- Isolation des requ√™tes lourdes (analytics) qui n'impactent pas la production

### Cas 2 : SaaS Multi-r√©gion avec DR

**Contexte :**
- 3 r√©gions g√©ographiques (EU, US, ASIA)
- Chaque r√©gion n√©cessite 2 standbys (HA + lectures)
- Disaster Recovery cross-region

**Architecture :**
```
Primary (EU - Paris)
 ‚îú‚îÄ> Standby HA (EU - Paris)
 ‚îú‚îÄ> Standby Interm√©diaire (US - New York)
 ‚îÇ    ‚îú‚îÄ> Standby HA (US - New York)
 ‚îÇ    ‚îî‚îÄ> Standby Read (US - New York)
 ‚îî‚îÄ> Standby Interm√©diaire (ASIA - Singapore)
      ‚îú‚îÄ> Standby HA (ASIA - Singapore)
      ‚îî‚îÄ> Standby Read (ASIA - Singapore)
```

**Flux de donn√©es :**
- Primary ‚Üí 3 destinations : EU local, US, ASIA
- Chaque r√©gion a 2 standbys locaux (1 HA, 1 read)

**Avantages :**
- R√©plication g√©ographique pour DR
- Chaque r√©gion peut lire localement (latence faible)
- Trafic inter-r√©gion optimis√© (3 flux au lieu de 7)

**Co√ªts de bande passante :**
- Sans cascade : Primary ‚Üí 7 standbys
- Avec cascade : Primary ‚Üí 3 standbys interm√©diaires

**R√©duction de co√ªts :** ~60% sur la bande passante sortante du primary

### Cas 3 : Plateforme de Donn√©es avec Lacs de Donn√©es

**Contexte :**
- Base de donn√©es transactionnelle (OLTP)
- Besoin d'extraire les donn√©es vers un data lake
- Plusieurs pipelines ETL avec des charges diff√©rentes

**Architecture :**
```
Primary (OLTP)
 ‚îî‚îÄ> Standby ETL (Interm√©diaire)
      ‚îú‚îÄ> Standby Data Lake 1 (Cascade - export continu)
      ‚îú‚îÄ> Standby Data Lake 2 (Cascade - export batch)
      ‚îî‚îÄ> Standby CDC (Cascade - Change Data Capture)
```

**Configuration :**
- Standby ETL : D√©di√© √† l'extraction de donn√©es (pas de lectures utilisateur)
- Cascades : Chacune connect√©e via un outil sp√©cifique (Debezium, Airbyte, etc.)

**Avantages :**
- Le primary n'est pas impact√© par les pipelines ETL
- Isolation des diff√©rents processus d'extraction
- Scalabilit√© : ajout de nouveaux pipelines sans surcharge du primary

### Cas 4 : Banque avec Exigences R√©glementaires

**Contexte :**
- Donn√©es critiques (zero data loss)
- Obligation de garder 3 copies des donn√©es
- Audit trail complet

**Architecture :**
```
Primary (Datacenter A)
 ‚îú‚îÄ> Standby Sync 1 (Datacenter A) [SYNC]
 ‚îú‚îÄ> Standby Sync 2 (Datacenter B) [SYNC]
 ‚îî‚îÄ> Standby Intermediate (Datacenter C) [ASYNC]
      ‚îú‚îÄ> Standby Audit (Datacenter C) [CASCADE]
      ‚îî‚îÄ> Standby Compliance (Datacenter C) [CASCADE]
```

**Configuration :**
```ini
# Primary
synchronous_standby_names = 'FIRST 2 (standby_sync1, standby_sync2)'
```

**Explication :**
- 2 standbys synchrones (zero data loss)
- 1 standby asynchrone + 2 cascades pour audit/compliance
- Total : 5 copies des donn√©es (conforme aux exigences)

**Avantages :**
- Zero data loss garanti (r√©plication synchrone)
- Conformit√© r√©glementaire (3+ copies)
- Standbys audit/compliance ne ralentissent pas le primary

---

## Sc√©narios de Panne et Gestion

### Sc√©nario 1 : Le Standby Interm√©diaire tombe

**Situation :**
```
Primary
 ‚îî‚îÄ> Standby Intermediate [PANNE]
      ‚îú‚îÄ> Cascade 1 [D√âCONNECT√â]
      ‚îî‚îÄ> Cascade 2 [D√âCONNECT√â]
```

**Cons√©quences :**
- Les standbys en cascade perdent leur source de r√©plication
- Ils restent op√©rationnels en lecture seule avec les donn√©es actuelles
- Lag croissant tant que le standby interm√©diaire est down

**Actions √† entreprendre :**

**Option 1 : R√©parer le standby interm√©diaire**
```bash
# Diagnostiquer le probl√®me
sudo systemctl status postgresql
sudo tail -f /var/log/postgresql/postgresql-18-main.log

# Red√©marrer si n√©cessaire
sudo systemctl restart postgresql
```

Les cascades se reconnecteront automatiquement.

**Option 2 : Reconfigurer temporairement les cascades vers le primary**

```bash
# Sur chaque standby cascade
sudo -u postgres psql

# Modifier temporairement la source
ALTER SYSTEM SET primary_conninfo = 'host=<PRIMARY_IP> port=5432 user=replicator password=*** application_name=cascade_standby1';

# Recharger
SELECT pg_reload_conf();
```

**Option 3 : Promouvoir un standby cascade en interm√©diaire**

Si le standby interm√©diaire est d√©finitivement perdu, promouvoir un des standbys cascade :

```bash
# Sur Cascade 1 (futur interm√©diaire)
pg_ctl promote -D /var/lib/postgresql/18/main
```

Puis reconfigurer Cascade 2 pour se connecter √† Cascade 1.

### Sc√©nario 2 : Le Primary tombe

**Situation :**
```
Primary [PANNE]
 ‚îî‚îÄ> Standby Intermediate
      ‚îú‚îÄ> Cascade 1
      ‚îî‚îÄ> Cascade 2
```

**Questions critiques :**
1. Quel serveur doit devenir le nouveau primary ?
2. Les standbys en cascade doivent-ils √™tre reconfigur√©s ?

**R√©ponse :** Promouvoir le **standby interm√©diaire** en primary (pas un standby cascade).

**Raisons :**
- Le standby interm√©diaire a le moins de lag par rapport √† l'ancien primary
- Il est d√©j√† configur√© avec `max_wal_senders` et peut alimenter les cascades
- Transition transparente pour les standbys cascade (pas de reconfiguration n√©cessaire)

**Proc√©dure de promotion :**

**1. Promouvoir le standby interm√©diaire**
```bash
# Sur le standby interm√©diaire
pg_ctl promote -D /var/lib/postgresql/18/main
```

**2. V√©rifier qu'il accepte les √©critures**
```sql
SELECT pg_is_in_recovery();  -- Doit retourner 'false'

-- Tester une insertion
INSERT INTO test_table VALUES (1, 'test');
```

**3. Reconfigurer les applications**
Pointer les applications vers le nouveau primary (ancien standby interm√©diaire).

**4. V√©rifier que les cascades sont toujours connect√©es**
```sql
-- Sur le nouveau primary
SELECT application_name, state FROM pg_stat_replication;
```

Les standbys cascade devraient toujours √™tre connect√©s (aucune reconfiguration n√©cessaire).

**5. Reconfigurer l'ancien primary en standby (quand il revient)**

Si l'ancien primary revient, le transformer en standby :

```bash
# Sur l'ancien primary
sudo systemctl stop postgresql

# Sauvegarder les donn√©es actuelles
sudo mv /var/lib/postgresql/18/main /var/lib/postgresql/18/main.old

# Cr√©er un nouveau basebackup depuis le nouveau primary
pg_basebackup -h <NOUVEAU_PRIMARY_IP> -D /var/lib/postgresql/18/main -U replicator -P -v -R -X stream

# D√©marrer en tant que standby
sudo systemctl start postgresql
```

### Sc√©nario 3 : Lag excessif sur les cascades

**Sympt√¥me :** Le standby cascade a 5 minutes de retard.

**Diagnostic :**

**Sur le primary :**
```sql
SELECT
    application_name,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) / 1024 / 1024 AS lag_mb
FROM pg_stat_replication;
```

**Sur le standby interm√©diaire :**
```sql
SELECT
    application_name,
    pg_wal_lsn_diff(sent_lsn, replay_lsn) / 1024 / 1024 AS lag_mb
FROM pg_stat_replication;
```

**Causes possibles :**

**1. R√©seau lent entre interm√©diaire et cascade**
```bash
# Tester la latence
ping <CASCADE_IP>

# Tester la bande passante
iperf3 -s  # Sur le cascade
iperf3 -c <CASCADE_IP>  # Sur l'interm√©diaire
```

**Solutions :**
- Upgrade du lien r√©seau
- Compression des WAL (param√®tre `wal_compression`)
- Si impossible : accepter le lag ou supprimer la cascade

**2. Standby cascade sous-dimensionn√© (CPU, I/O)**

```sql
-- Sur le cascade, v√©rifier les I/O waits
SELECT
    wait_event_type,
    wait_event,
    count(*)
FROM pg_stat_activity
WHERE state = 'active'
GROUP BY wait_event_type, wait_event
ORDER BY count(*) DESC;
```

Si beaucoup de `wait_event_type = 'IO'`, le disque est satur√©.

**Solutions :**
- Am√©liorer les I/O (SSD, augmenter IOPS)
- Augmenter `maintenance_work_mem`
- R√©duire les lectures sur le cascade (redirections vers un autre standby)

**3. Volume de WAL tr√®s √©lev√© sur le primary**

Si le primary g√©n√®re 200 MB/s de WAL, les cascades peuvent avoir du mal √† suivre.

**Solutions :**
- Augmenter `max_wal_size` sur le primary
- Optimiser les requ√™tes pour r√©duire les √©critures
- Ajouter un standby interm√©diaire suppl√©mentaire pour r√©partir la charge

---

## Bonnes Pratiques

### 1. Limiter la profondeur de cascade √† 2-3 niveaux maximum

**‚ùå √Ä √©viter :**
```
Primary ‚Üí Standby1 ‚Üí Standby2 ‚Üí Standby3 ‚Üí Standby4 ‚Üí Standby5
```

**Probl√®mes :**
- Lag cumulatif tr√®s √©lev√©
- Complexit√© de debugging
- Effet domino en cas de panne interm√©diaire

**‚úÖ Recommand√© :**
```
Primary
 ‚îú‚îÄ> Standby1
 ‚îÇ    ‚îú‚îÄ> Standby1a
 ‚îÇ    ‚îî‚îÄ> Standby1b
 ‚îî‚îÄ> Standby2
      ‚îú‚îÄ> Standby2a
      ‚îî‚îÄ> Standby2b
```

### 2. Utiliser des slots de r√©plication pour les cascades

Les slots garantissent que les WAL ne sont pas supprim√©s pr√©matur√©ment.

**Cr√©er un slot pour chaque cascade :**
```sql
-- Sur le standby interm√©diaire
SELECT pg_create_physical_replication_slot('cascade1_slot');
SELECT pg_create_physical_replication_slot('cascade2_slot');
```

**Configurer les cascades pour utiliser ces slots :**
```ini
# Sur chaque cascade
primary_slot_name = 'cascade1_slot'
```

### 3. Monitorer chaque niveau de r√©plication

Ne pas se contenter de monitorer le primary. Chaque niveau doit √™tre surveill√© :
- Primary ‚Üí Interm√©diaires
- Interm√©diaires ‚Üí Cascades

**Dashboard recommand√© (Grafana) :**
- Graphique 1 : Lag du primary vers tous les standbys directs
- Graphique 2 : Lag de chaque interm√©diaire vers ses cascades
- Graphique 3 : Lag total (cumulatif) de chaque cascade

### 4. Documenter la topologie

Maintenir un sch√©ma √† jour de votre topologie de r√©plication :

**Documentation minimale :**
- Sch√©ma visuel (diagramme)
- Adresses IP de chaque serveur
- R√¥le de chaque serveur (primary, interm√©diaire, cascade)
- Configuration de `synchronous_standby_names`
- Proc√©dures de failover sp√©cifiques √† votre topologie

### 5. Tester les sc√©narios de panne

**Tests trimestriels recommand√©s :**
- Panne du standby interm√©diaire : v√©rifier que les cascades g√®rent la d√©connexion
- Panne du primary : promouvoir l'interm√©diaire et v√©rifier que les cascades continuent
- Panne d'un cascade : v√©rifier qu'il rattrape son retard apr√®s red√©marrage

### 6. Pr√©voir la bande passante

**Calcul de bande passante :**
```
Bande passante totale = Volume WAL/s √ó (Nombre de standbys directs + overhead)
```

**Exemple :**
- Primary g√©n√®re 50 MB/s de WAL
- 2 standbys directs du primary
- Bande passante n√©cessaire : 50 MB/s √ó 2 = 100 MB/s

Si vous avez 100 Mbps disponibles, vous √™tes limite. Avec la cascade, vous pouvez r√©partir :
- Primary ‚Üí Interm√©diaire 1 : 50 MB/s
- Primary ‚Üí Interm√©diaire 2 : 50 MB/s
- Interm√©diaire 1 ‚Üí 3 cascades : g√©r√©es localement
- Interm√©diaire 2 ‚Üí 3 cascades : g√©r√©es localement

### 7. Utiliser la cascade pour les environnements non-production

**Principe :** Isoler les environnements de dev/test/staging du primary de production.

**Architecture recommand√©e :**
```
Primary (Production)
 ‚îú‚îÄ> Standby HA (Production)
 ‚îî‚îÄ> Standby Staging (Interm√©diaire)
      ‚îú‚îÄ> Standby Dev
      ‚îú‚îÄ> Standby Test
      ‚îî‚îÄ> Standby QA
```

**Avantages :**
- Production isol√©e des charges non-production
- Facile d'ajouter/supprimer des environnements de dev sans impacter la prod
- Possibilit√© de tester des migrations sur les cascades avant la prod

---

## Configuration pour les Nouveaut√©s PostgreSQL 18

PostgreSQL 18 apporte des am√©liorations qui b√©n√©ficient √† la r√©plication en cascade :

### 1. I/O Asynchrone (AIO)

Activer l'I/O asynchrone am√©liore les performances de r√©plication :

```ini
# Sur tous les serveurs (primary, interm√©diaires, cascades)
io_method = async
```

**Gains attendus :**
- R√©duction du lag de 20-30%
- Meilleure utilisation du r√©seau
- Id√©al pour les cascades avec beaucoup de standbys

### 2. Statistiques de r√©plication am√©lior√©es

PostgreSQL 18 expose de nouvelles m√©triques :

```sql
-- M√©triques I/O par backend (incluant WAL sender)
SELECT
    backend_type,
    reads,
    writes,
    read_time,
    write_time
FROM pg_stat_io
WHERE backend_type = 'walsender';
```

**Utilit√© :**
- Identifier les goulots d'√©tranglement I/O sur les standbys interm√©diaires
- Optimiser la configuration des disques

### 3. SCRAM Passthrough

PostgreSQL 18 permet l'authentification SCRAM via FDW et dblink, ce qui simplifie la gestion des mots de passe en cascade :

```ini
# Les standbys peuvent utiliser l'authentification SCRAM de mani√®re transparente
# Pas de configuration suppl√©mentaire n√©cessaire
```

---

## R√©sum√© : Checklist de Configuration

### Configuration du Standby Interm√©diaire

- [ ] Configurer `postgresql.conf` :
  - [ ] `max_wal_senders = 5` (ou plus selon le nombre de cascades)
  - [ ] `wal_keep_size = 1GB`
  - [ ] `max_replication_slots = 5`
  - [ ] `hot_standby = on`
- [ ] Configurer `pg_hba.conf` : autoriser les connexions depuis les cascades
- [ ] Cr√©er des slots de r√©plication pour chaque cascade
- [ ] Red√©marrer PostgreSQL : `systemctl restart postgresql`
- [ ] V√©rifier : `SHOW max_wal_senders;` (doit √™tre > 0)

### Configuration du Standby Cascade

- [ ] Effectuer un `pg_basebackup` **depuis le standby interm√©diaire** (pas le primary)
- [ ] Configurer `postgresql.conf` :
  - [ ] `hot_standby = on`
  - [ ] `primary_conninfo` pointant vers le standby interm√©diaire
  - [ ] `primary_slot_name` (si slot cr√©√© sur l'interm√©diaire)
- [ ] V√©rifier que `standby.signal` existe
- [ ] Ajuster les permissions : `chown -R postgres:postgres ...`
- [ ] D√©marrer PostgreSQL : `systemctl start postgresql`
- [ ] V√©rifier : `SELECT pg_is_in_recovery();` (doit retourner `true`)

### V√©rifications Post-Configuration

- [ ] Sur le primary : Voir le standby interm√©diaire dans `pg_stat_replication`
- [ ] Sur l'interm√©diaire : Voir les cascades dans `pg_stat_replication`
- [ ] Sur les cascades : V√©rifier `pg_stat_wal_receiver` pointe vers l'interm√©diaire
- [ ] Test de bout en bout : INSERT sur primary ‚Üí visible sur cascades
- [ ] Configurer le monitoring : m√©triques de lag pour chaque niveau
- [ ] Documenter la topologie : sch√©ma + IPs + proc√©dures de failover

---

## Conclusion

La r√©plication en cascade est une fonctionnalit√© puissante qui permet de construire des architectures de r√©plication scalables et g√©ographiquement distribu√©es. Elle offre :

- ‚úÖ **R√©duction de la charge sur le primary** : Moins de connexions WAL sender, bande passante √©conomis√©e
- ‚úÖ **Scalabilit√©** : Possibilit√© d'avoir des dizaines de standbys sans surcharger le primary
- ‚úÖ **Flexibilit√© g√©ographique** : R√©plication multi-r√©gion optimis√©e
- ‚úÖ **Isolation** : S√©parer les environnements production/non-production

**Points cl√©s √† retenir :**

1. **Le standby interm√©diaire** doit avoir `max_wal_senders > 0` et √™tre configur√© comme un "mini-primary"
2. **Le lag se cumule** √† chaque niveau : limiter √† 2-3 niveaux maximum
3. **Utiliser des slots de r√©plication** pour √©viter la perte de WAL
4. **Monitorer chaque niveau** de r√©plication, pas seulement le primary
5. **Documenter la topologie** pour faciliter le troubleshooting et les failovers
6. **Tester les sc√©narios de panne** r√©guli√®rement

**Quand utiliser la cascade :**
- Vous avez > 5 standbys √† g√©rer
- Vous d√©ployez sur plusieurs r√©gions g√©ographiques
- Vous voulez isoler des environnements (prod vs non-prod)
- Vous avez des contraintes de bande passante sur le primary

**Quand ne pas utiliser la cascade :**
- Vous avez seulement 1-2 standbys (overhead inutile)
- Le lag cumulatif est inacceptable (< 1 seconde requis)
- Vous pr√©f√©rez la simplicit√© √† la scalabilit√©

La r√©plication en cascade est un outil d'architecture avanc√© qui n√©cessite une planification et un monitoring rigoureux, mais qui offre une scalabilit√© et une flexibilit√© in√©gal√©es pour les d√©ploiements PostgreSQL de grande envergure.

---

**Prochaines √©tapes :**
- Explorer le failover automatis√© avec Patroni (17.5)
- Mettre en place un monitoring complet (Prometheus + Grafana)
- √âtudier la r√©plication logique pour des cas d'usage compl√©mentaires (17.3)
- Planifier des architectures multi-datacenter haute disponibilit√©

---

**Ressources compl√©mentaires :**
- Documentation officielle : [Cascading Replication](https://www.postgresql.org/docs/18/warm-standby.html#CASCADING-REPLICATION)
- Blog Percona : [PostgreSQL Cascading Replication Best Practices](https://www.percona.com/blog/)
- 2ndQuadrant : [Building Multi-Region PostgreSQL Architectures](https://www.2ndquadrant.com/en/blog/)

---


‚è≠Ô∏è [R√©plication Logique (Logical Replication)](/17-haute-disponibilite-et-replication/03-replication-logique.md)
